[ { "title": "Documentation for cloudflare-ddns-edgeos", "url": "/posts/Documentation-for-cloudflare-ddns-edgeos/", "categories": "Linux, Management", "tags": "linux, shell, cloudflare-ddns-edgeos", "date": "2022-03-02 16:32:09 +0800", "snippet": "About this project Inspired by rehiy/dnspod-shell, I wrote this project. At the beginning, this is just a random script I wrote and configured it on my router with crontab, all because of my sudden switch of dns provider.There’re plenty of ddns(ddns-client) projects on Github. Some of them are written in shell-script and don’t require extra language or framework support. (I don’t want my poor router host a docker container everytime just for ddns update…) Some of them are specifically optimized, ( or let’s say ) , are only written for cloudflare ddns update, but in other language except shell-script. Thus, cloudflare-ddns-edgeos aims at For routers running EdgeOS (Ubiquiti EdgeRouter), light-weighted design. Written in shell-script, avoid complex language and framework dependencies. Specifically for Cloudflare DNS update, via Cloudflare v4 api. And…EdgeOS didn’t have official support for ddns on cloudflare, at least not an option in their web-ui. You need to configure it under cli, and it communicates with cloudflare through v1 api (currently v4 is actively supported &amp;amp; maintained by cloudflare)…Usage0 - PrerequisiteMake sure you have following dependencies installed. curl jq ip1 - Download Suppose you have already ssh into your EdgeOS based router, in my case, Ubiquiti ER-X(v1.10.11).1.1 Using Github Rawcd /config/scripts/ &amp;amp;&amp;amp; pwd# /config/scripts/curl -LS -o /config/scripts/cloudflare-ddns-edgeos.tar.gz https://github.com/MijazzChan/cloudflare-ddns-edgeos/raw/releases/cloudflare-ddns-edgeos.tar.gz1.2 Trouble with GFW? JsDelivr Github CDNcd /config/scripts/ &amp;amp;&amp;amp; pwd# /config/scripts/curl -LS -o /config/scripts/cloudflare-ddns-edgeos.tar.gz https://cdn.jsdelivr.net/gh/MijazzChan/cloudflare-ddns-edgeos@releases/cloudflare-ddns-edgeos.tar.gz2 - Deploy2.1 - Extractiontar -xvf /config/scripts/cloudflare-ddns-edgeos.tar.gz2.2 - Global Parameters Editing!!! This step is IMPORTANT !!!Alter variables in /config/scripts/cloudflare-ddns-edgeos/cfddns.shMeanings &amp;amp; where-can-be-found of global parameters is provided in cfddns.sh via code comments.2.2.1 - Cloudflare Access TokenVariable in cfddns.sh - CF_ACCESS_TOKENVisit Cloudflare Api-tokens, Click Create Token.Use Edit zone DNS template.Fill in the form.Continue to Summary, and Create Token.Copy following token string into CF_ACCESS_TOKEN, starting with Bearer .2.2.2 - Zone IdentifierVariable in cfddns.sh - ZONE_IDENTIFIER.2.2.3 - Record NameVariable in cfddns.sh - RECORD_NAMEFull domain name(With subdomain)eg. subdomain.mijazz.icu, example.subdomain.yourdomain.com2.3 - Extra Steps+x to files using chmod +x /config/scripts/cloudflare-ddns-edgeos/cfddns.sh3 - TestBefore you set it as a scheduled task, dryrun/execute it first./config/scripts/cloudflare-ddns-edgeos/cfddns.shIf it creates and updates your domain record both successfully and correctly, proceed to next step.Otherwise, feel free to open a issue4 - Using task-scheduler to cron itconfigureset system task-scheduler task cloudflareddns executable path /config/scripts/cloudflare-ddns-edgeos/cfddns.shset system task-scheduler task cloudflareddns interval 20mcommitsaveexitLicenseCopyright © 2022 MijazzChan mijazz@qq.comThis work is free. You can redistribute it and/or modify it under theterms of the Do What The Fuck You Want To Public License, Version 2,as published by Sam Hocevar. See http://www.wtfpl.net/ for more details." }, { "title": "Clean Pacman/Yay Package Cache on Manjaro or Arch-based Distros (Automation/Manual)", "url": "/posts/clean-pacman-yay-package-cache-on-manjaro-automation-manual/", "categories": "Linux, Management", "tags": "linux, manjaro, pacman, yay", "date": "2022-01-09 12:36:23 +0800", "snippet": "IntroAfter several months of using Manjaro Linux(Arch Based Distro), I feel like getting attracted to its package manager pacman and its Rolling Release pattern.Most of the time, Arch has vanilla packages (not as heavily modified as the Ubuntu/Debian based distros) that you can directly compile or install by just using pacman. It really save the day when you don’t have to worry about some OS specific workaround. Tweaking/Messing with dotfile while configuring a freshly installed non-functional package is a nightmare to me.Arch or AUR, often ships the newest/latest software with just-right-open-the-box experience.However, with all these upsides, we will also face some downsides.Reproduce the ScenarioI didn’t notice my disk usage until I have to meet a specific project requirements(machine learning) which needs installing cuda and tensorflow manually. Then I found out that my /var/cache/pacman/pkg ate up almost 20GB of my storage space, which results that my / barely have enough space to fit my new packages in. (Don’t buy Solid-State-Drive smaller than 512GB, and definitely don’t throw a Windows on it…)Because I am using pacman and yay to manage my system packages and never bother to clean them manually. It grows up in size. Browsing on ArchWiki, turns out that pacman will not automatically clean the package cache after installing/upgrading package in case users need to downgrade to a lower version of that.Although pacman do provide cli parameters to remove (un)installed package cache, pacman -Sc to clean cache of the uninstalled packages. pacman -Scc to clean all cache despite their state of installing. It’s still worth keeping 1 or 2 cached packages of each (un)installed software in case of emergency.Clean pacman cache Generally located in /var/cache/pacman/pkg. If altered, pass param -c or --cachedir to specify different cache directory.With the help of paccache, we can easily achieve the goal/purpose mentioned above. Note that paccache is now come from package called pacman-contrib, it originally ships with pacman till it became a tool component split into the ...-contrib package alongside with pacdiff, paclist, rankmirrors, etc. See also Community/pacman-contrib.For documentation/man of paccache, click here.TLDR?Check dependency first, find out whether pacman-contrib is installed or not.pacman -Q pacman-contribTo list out the cache that needs to be cleaned, follow steps below.paccache -dvk2# This line above list out the packages which are flagged# to delete. (but keep 2 versions of each of them.)# paccache --dryrun --verbose --keep 2paccache -dvuk1# This line above list out the already uninstalled packages # which are flagged to delete. (but keep 1 version of each of# them.)# paccache --dryrun --verbose --uninstalled --keep 1To clean up the cache, follow steps below.paccache -rvk2 &amp;amp;&amp;amp; paccache -rvuk1# paccache --remove --verbose --keep 2 # paccache --remove --verbose --uninstalled --keep 1# which does the same trick.Clean yay cache Generally located in /home/$(whoami)/.cache/yay. If altered, pass param -c or --cachedir to specify different cache directory.Using the same paccache approach mentioned above can also be useful when it comes to yay cache. Point the cache directory to /home/$(whoami)/.cache/yay/*/ by passing -c /home/mijazz/.cache/yay/*/(in my case).Also, to list’em outpaccache -c /home/$(whoami)/.cache/yay/*/ -dvk2# paccache -cachedir /home/$(whoami)/.cache/yay/*/ --dryrun --verbose --keep 2paccache -c /home/$(whoami)/.cache/yay/*/ -dvuk1# paccache -c /home/$(whoami)/.cache/yay/*/ --dryrun --verbose --uninstalled --keep 1to clean’empaccache -c /home/$(whoami)/.cache/yay/*/ -rvk2# paccache -cachedir /home/$(whoami)/.cache/yay/*/ --remove --verbose --keep 2paccache -c /home/$(whoami)/.cache/yay/*/ -rvuk1# paccache -c /home/$(whoami)/.cache/yay/*/ --remove --verbose --uninstalled --keep 1yay’s approach to clean Not recommended. (Interactive prompt in shell.)yay -SccCache directory: /var/cache/pacman/pkg/:: Do you want to remove ALL files from cache? [y/N] NDatabase directory: /var/lib/pacman/:: Do you want to remove unused repositories? [Y/n] NBuild directory: /home/mijazz/.cache/yay==&amp;gt; Do you want to remove ALL AUR packages from cache? [Y/n] yremoving AUR packages from cache...Achieve Automation via HooksAutomatically clean cache after each package transaction can be achieved given you are familiar with alpm-hooks.It provides the ability to run hooks before or after package transaction based on the packages and/or files being modified.You can custom the trigger of the hook whether it will run [ before | after ] the [ specific | * ]package [ Install | Upgrade | Remove ] transaction.In this specific scenario this post focusing on, a cleaning-hook triggered after package wildcard *** being **upgraded or uninstalled will be perfectly suitable. Hooks are written in alpm hook file format, ArchWiki - alpm-hooks Default hook files location is /usr/share/libalpm/hooks, and /etc/pacman.d/hooks, additionally it can be customized via /etc/pacman.conf - HookDir. ## /etc/pacman.conf## See the pacman.conf(5) manpage for option and repository directives## GENERAL OPTIONS#[options]# The following paths are commented out with their default values listed.# If you wish to use different paths, uncomment and update the paths.#RootDir = /#DBPath = /var/lib/pacman/CacheDir = /var/cache/pacman/pkg/#LogFile = /var/log/pacman.log#GPGDir = /etc/pacman.d/gnupg/#HookDir = /etc/pacman.d/hooks/ More details can be found at ArchWiki - Pacman #HooksStart with cleaning script. Name it /home/$(whoami)/.local/bin/cleaning-automation(whatever), just don’t forget to chmod +x.#!/usr/bin/env bash# Clean cache upgrade/uninstall at PostTransaction# /home/mijazz/.local/bin/cleaning-automation# Hook starts here# Suppose you run yay with your user account id 1000yay_running_on_behalf=$(id -nu 1000)yay_cache_dir=&quot;/home/$yay_running_on_behalf/.cache/yay/&quot;pkg_cache_dir=&quot;$(find $yay_cache_dir -mindepth 1 -maxdepth 1 -type d | xargs -r printf &quot;-c %s &quot;)&quot;echo &quot;Removing cached packages, keeping latest 2 versions of each one.&quot;/usr/bin/paccache -rk2/usr/bin/paccache -rk2 $pkg_cache_direcho &quot;Removing cached uninstalled packages, keeping latest 1 versions of each one.&quot;/usr/bin/paccache -ruk1/usr/bin/paccache -ruk1 $pkg_cache_dirthen the hook. Place it under /etc/pacman.d/hooks(if this specific directory doesn’t exist, just mkdir it). Do remember that hook’s filename must look like *.hook. File SYNOPSIS: [Trigger] (Required, Repeatable)Operation = Install|Upgrade|Remove (Required, Repeatable)Type = Path|Package (Required)Target = &amp;lt;Path|PkgName&amp;gt; (Required, Repeatable)[Action] (Required)Description = ... (Optional)When = PreTransaction|PostTransaction (Required)Exec = &amp;lt;Command&amp;gt; (Required)Depends = &amp;lt;PkgName&amp;gt; (Optional)AbortOnFail (Optional, PreTransaction only)NeedsTargets (Optional) # Pacman Cache Cleaning Hook# /etc/pacman.d/hooks/cleaning-automation.hook[Trigger]Operation = UpgradeOperation = RemoveType = PackageTarget = *[Action]Description = Pacman Cache Cleaning HookWhen = PostTransactionExec = /home/mijazz/.local/bin/cleaning-automationDepends = pacman-contribConclusionUsing PostTransaction hook to achieve this automation is perfectly suitable for those folks who don’t have big / disk space but daily drive Arch based Linux. However, keeping extra versions of cache packages also benefits Linux user to some extend, in case you need to roll back some packages to previous version in order to prevent crashing or mis-behavior of the latest one." }, { "title": "Functional Interface and its Underlying Pattern - Effective Java reading notes", "url": "/posts/Functional-Interface-and-its-Underlying-Pattern/", "categories": "Java, Design", "tags": "java", "date": "2021-06-15 20:47:41 +0800", "snippet": " Just like I said in this post, These patterns are pretty easy/simple, but it really helps me a lot especially when managing to understanding Java’s underlying design pattern through reading Java source code. Following these patterns also helps producing code which is developer-friendly.Keyword: Functional Interface, Map::computeIfPresent, PECS mnemonic, Consumer, Predicate, Supplier, BinaryOperator, UnaryOperator.Functional Interface IntroductionAnnotated widely across java.util.function, Functional Interface provide a way to represent a function that accept one/multiple argument(s) by creating a interface then implementing it with lambda expressions, method references, or constructors.However, before I started writing this post, crawling over blogs and posts, I still can’t find a vivid example that can explain or express how flexible it can be. Thus, here is a example I came up with. @Test void functionalInterface_usage_Stream() { LongStream longStream = LongStream.range(1L, 200L); // LongToIntFunction LongToIntFunction mapPositiveLongToInt = ( longNumber -&amp;gt; (longNumber &amp;gt; Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int) longNumber ); IntStream intStream = longStream.mapToInt(mapPositiveLongToInt); // IntPredicate IntPredicate isPowerOf2 = ( num -&amp;gt; (num != 0) &amp;amp;&amp;amp; ((num &amp;amp; (num - 1)) == 0) ); List&amp;lt;Integer&amp;gt; powerOf2Under200 = intStream .filter(isPowerOf2) .boxed() .collect(Collectors.toUnmodifiableList()); Assertions.assertEquals(powerOf2Under200, List.of(1, 2, 4, 8, 16, 32, 64, 128)); class Circle { double radius; double area; public Circle(double radius, double area) { this.radius = radius; this.area = area; } } // takes Integer -&amp;gt; returns Circle Function&amp;lt;Integer, Circle&amp;gt; radiusToCircle = ( radius -&amp;gt; new Circle(radius, StrictMath.PI * radius * radius) ); List&amp;lt;Circle&amp;gt; circleList = powerOf2Under200 .stream() .map(radiusToCircle) .collect(Collectors.toList()); // Takes Circle -&amp;gt; returns boolean Predicate&amp;lt;Circle&amp;gt; areaBetween800and5000 = ( circle -&amp;gt; circle.area &amp;gt; 800 &amp;amp;&amp;amp; circle.area &amp;lt; 5000 ); // Takes Circle -&amp;gt; void, consumes it. Consumer&amp;lt;Circle&amp;gt; circleAreaPrinter = ( circle -&amp;gt; System.out.printf(&quot;%-10.5f&quot;, circle.area) ); circleList.stream() .filter(areaBetween800and5000) .forEach(circleAreaPrinter); // console: 804.24772 3216.99088 }Functional Interfaces in java.util.function/** * Represents a function that accepts one argument and produces a result. * * &amp;lt;p&amp;gt;This is a &amp;lt;a href=&quot;package-summary.html&quot;&amp;gt;functional interface&amp;lt;/a&amp;gt; * whose functional method is {@link #apply(Object)}. * * @param &amp;lt;T&amp;gt; the type of the input to the function * @param &amp;lt;R&amp;gt; the type of the result of the function * * @since 1.8 */@FunctionalInterfacepublic interface Function&amp;lt;T, R&amp;gt; { /** * Applies this function to the given argument. * * @param t the function argument * @return the function result */ R apply(T t); // ... compose(), andThen(), identity()}Dig deeper into said package, you will find 43 functional interfaces. There are interfaces with specific type declaration, such as IntConsumer, LongToDoubleFunction. Let’s set aside those with type declaration, with simple classification, we can derive 6 basic functional interfaces. Interface Function Signature How it perform Example Function&amp;lt;T, R&amp;gt; R apply(T t) Functions which take T but return R Arrays::asList Supplier&amp;lt;T&amp;gt; T get() … take no arg and return T LocalDate::now Comsumer&amp;lt;T&amp;gt; void accept(T t) … take T as arg but return nothing System.out::println Predicate&amp;lt;T&amp;gt; boolean test(T t) … take T as arg and return a condition bool Collection::isEmpty UnaryOperator&amp;lt;T&amp;gt; T apply(T t) … take 1 T as arg and also return T String::toLowerCase BinaryOperator&amp;lt;T&amp;gt; T apply(T t1, T t2) … take 2 T as arg and also return T BigInteger::add With all this method only accepting certain type or returning certain type, despite 8 primitive types also have corresponding boxed primitives which fits the design pattern, additional variants of Function interfaces are provided, for use when the argument/result type is primitive.Mentioned in Effective Java, Do NOT use basic functional interface with boxed primitives instead of primitive functional interface. Although with the auto-boxing and auto-unboxing mechanisms, it will still work but with the consequences of bad performance.Pattern Usage in Java’s Design Map::compute, Map::computeIfabsent, Map::computeIfpresent have similar design pattern. We will discuss computeIfPresent here.Map::computeIfPresentThis method has been added since 1.8. You can have a glance at the source code(Map.java:1074). Its code is really straight-forward. The basic idea of this method is to Accept a key, and a BiFunction If the key exists in the Map use BiFunction with key and its oldValue as arguments to derive a new Value If the new Value is not null Update the value by map.put(key, newValue); If the new Value is null Remove the entry of the key. If the key not exists in the Map Do nothing. // java.util.Map.java : 1074 default V computeIfPresent(K key, BiFunction&amp;lt;? super K, ? super V, ? extends V&amp;gt; remappingFunction) { Objects.requireNonNull(remappingFunction); V oldValue; if ((oldValue = get(key)) != null) { V newValue = remappingFunction.apply(key, oldValue); if (newValue != null) { put(key, newValue); return newValue; } else { remove(key); return null; } } else { return null; } }learn from above, a BiFunction is simply a Function Interface which takes 3 type parameters, first and second are the types of function argument, the third one is the type of returning obj.// java.util.function.BiFunction.java : 44@FunctionalInterfacepublic interface BiFunction&amp;lt;T, U, R&amp;gt; { /** * Applies this function to the given arguments. * * @param t the first function argument * @param u the second function argument * @return the function result */ R apply(T t, U u); // .......}Focusing on its remappingFunction argument type BiFunction&amp;lt;? super K, ? super V, ? extends V&amp;gt;, it is also obviously a PECS pattern. Didn’t heard of them? Check out my last post about PECS Mnemonic.From the aspect of PECS Mnemonic, in the scope of BiFunction &amp;lt;? super K&amp;gt; is a consumer, it consumes K key from this argument for apply() ‘s remapping usage. &amp;lt;? super V&amp;gt; is a consumer, it consumes V oldValue from this argument for apply() ‘s remapping usage. &amp;lt;? extends V&amp;gt; is a producer, it produces a newly generated V newValue and return it.From the aspect of Functional Interface, first type argument is Map’s key type (as arg being passed in) second type argument is Map’s value type (as arg being passed in) third type argument should be Map’s value type V or V’s sub-type. (as obj being returned) Otherwise, returning type cannot be put inside the Map because of type mis-matching. Theory and explanation without practice or example are always hard to swallow(follow).// Map.compute// Map.computeIfPresent @Test void computeIfPresent_FunctionalInterface() { // Suppose you have a piecewise-defined function /* { x*2, 0&amp;lt;x&amp;lt;3 } * y = { x*3, 3&amp;lt;=x&amp;lt;5 } x is N{0, 1, 2, 3, 4...} * { 0 , others } */ // This map is to store function value from 0~10 Map&amp;lt;Integer, Double&amp;gt; yValMap = new HashMap&amp;lt;&amp;gt;(); // Create a functional interface to calculate the value of y Function&amp;lt;Integer, Double&amp;gt; calY = ( x -&amp;gt; { if (x &amp;gt; 0 &amp;amp;&amp;amp; x &amp;lt; 3) return (double) (x * 2); else if (x &amp;gt;= 3 &amp;amp;&amp;amp; x &amp;lt; 5) return (double) (x * 3); else return null; } ); // for x in range(1, 5) // because yValMap has no k-v, initialize it with x from 1 to 10 // calY fits =&amp;gt; Function&amp;lt;? super Integer, ? extends Number&amp;gt; IntStream.range(1, 5).forEach( x -&amp;gt; yValMap.computeIfAbsent(x, calY) ); printMyFunctionMap(yValMap); // Suppose a z, where z = x + y*1.6 // z&#39;s equation contains both x and y. inside yValMap, you have // both x and y as K and V. use x as key, y as oldValue compute z as newValue // You just alter yValMap to fit zValMap&#39;s logic. BiFunction&amp;lt;Integer, Double, Double&amp;gt; updateZFromY = ( (x, y) -&amp;gt; { return x + 1.6 * y; } ); // for x in range(1, 5) // x from 1, 10 is present in the map, invoke computeIfPresent // will pass (key, oldValue) which is (x, y) as argument // to updateZFromY to perform compute z&#39;s value as newValue. IntStream.range(1, 5).forEach( x -&amp;gt; yValMap.computeIfPresent(x, updateZFromY) ); // After computing, yValMap is zValMap now. Map&amp;lt;Integer, Double&amp;gt; zValMap = yValMap; printMyFunctionMap(zValMap); } // parameter PECS Mnemonic void printMyFunctionMap(Map&amp;lt;? extends Integer, ? extends Double&amp;gt; map) { for (Map.Entry&amp;lt;? extends Integer, ? extends Double&amp;gt; entry : map.entrySet()) { int x = entry.getKey(); double y = entry.getValue(); System.out.printf(&quot;%d -&amp;gt; %.2f\\n&quot;, x, y); } System.out.println(&quot;*****&quot;); }// Console1 -&amp;gt; 2.002 -&amp;gt; 4.003 -&amp;gt; 9.004 -&amp;gt; 12.00*****1 -&amp;gt; 4.202 -&amp;gt; 8.403 -&amp;gt; 17.404 -&amp;gt; 23.20*****Similar pattern can be seen wildly across java.util.Stream. Especially on map() and flatMap() method, it help abstraction on the data/object flow, generalize it like a data/object pipe. &amp;lt;R&amp;gt; Stream&amp;lt;R&amp;gt; map(Function&amp;lt;? super T, ? extends R&amp;gt; mapper), IntStream flatMapToInt(Function&amp;lt;? super T, ? extends IntStream&amp;gt; mapper)…." }, { "title": "Notes of PECS Mnemonic, Bounded wildcard type - Effective Java reading notes", "url": "/posts/PECS-Mnemonic-Bounded-wildcard-Type-Effective-Java-reading-notes/", "categories": "Java, Design", "tags": "java", "date": "2021-06-10 20:37:19 +0800", "snippet": "IntroPECS stands for Producer extend Consumer super. The PECS mnemonic captures the fundamental principle that guides the use of wildcard types.Although the way it works may seem a little bit confusing. An answer from Stack Overflow by Julien perfectly illustrate how that actually affects the way of adding object to List and List assigning.Bounded Wildcard Type with super and extendsSuppose you have had following Classes, and their hierarchies =&amp;gt;class A {}class B extends A {}class C extends B {} A wildcard of List&amp;lt;? extends T&amp;gt;|-------------------------|-------------------|---------------------------------|| wildcard | get | assign ||-------------------------|-------------------|---------------------------------|| List&amp;lt;? extends C&amp;gt; | A B C | List&amp;lt;C&amp;gt; ||-------------------------|-------------------|---------------------------------|| List&amp;lt;? extends B&amp;gt; | A B | List&amp;lt;B&amp;gt; List&amp;lt;C&amp;gt; ||-------------------------|-------------------|---------------------------------|| List&amp;lt;? extends A&amp;gt; | A | List&amp;lt;A&amp;gt; List&amp;lt;B&amp;gt; List&amp;lt;C&amp;gt; ||-------------------------|-------------------|---------------------------------| A wildcard of List&amp;lt;? super T&amp;gt;|-------------------------|-------------------|-------------------------------------------|| wildcard | add | assign ||-------------------------|-------------------|-------------------------------------------|| List&amp;lt;? super C&amp;gt; | C | List&amp;lt;Object&amp;gt; List&amp;lt;A&amp;gt; List&amp;lt;B&amp;gt; List&amp;lt;C&amp;gt; ||-------------------------|-------------------|-------------------------------------------|| List&amp;lt;? super B&amp;gt; | B C | List&amp;lt;Object&amp;gt; List&amp;lt;A&amp;gt; List&amp;lt;B&amp;gt; ||-------------------------|-------------------|-------------------------------------------|| List&amp;lt;? super A&amp;gt; | A B C | List&amp;lt;Object&amp;gt; List&amp;lt;A&amp;gt; ||-------------------------|-------------------|-------------------------------------------|In all of the cases: you can always get Object from a list regardless of the wildcard. you can always add null to a mutable list regardless of the wildcard.Producer or Consumer in PECS MnemonicDon’t be scared by the table above. Producer and Consumer pattern is often/normally used in function param.Code example below is mentioned in Effective Java Chapter5-ITEM31. (with minor modifications)// suppose you have a Stack Classpublic class Stack&amp;lt;E&amp;gt; { // Constructor // other methods...size(), isEmpty()..etc. public void push(E e) { // push a element } public void pop(E e) { // pop a element }}ProducerThen you would have a pushAll() method which handles a incoming instance of Iterable and push it all into the stack.public void pushAll(Iterable&amp;lt;E&amp;gt; from) { for (E element: from){ this.push(element); }}However, without wildcard support, pushAll(Iterable&amp;lt;E&amp;gt; from) can only support the exact class E of Iterable&amp;lt;&amp;gt; being passed to the method. Normally you would expect pushAll(Iterable&amp;lt;E&amp;gt; from) can handle a Iterable containing not only E but also E’s sub-classes. Just like the example provided in Effective Java.Stack&amp;lt;Number&amp;gt; numberStack = new Stack&amp;lt;&amp;gt;(); // &amp;lt;&amp;gt; is called Diamond operator.Iterable&amp;lt;Number&amp;gt; numbers = ...;Iterable&amp;lt;Integer&amp;gt; integers = ...;// Works, because E =&amp;gt; Number is a exact matchnumberStack.pushAll(numbers);// Error, but normally you would expect this to work.// Because Integer is a sub-class of E=Number.// Stack&amp;lt;Number&amp;gt; can definitely contain Number&#39;s sub-classes.numberStack.pushAll(integers);But with the help of bounded wildcard, pushAll can accept Iterable of E and E’s subtype instead of E itself. Subtype is a little bit mis-leading, since subtype =&amp;gt; every type is a subtyope of itself, even though it does not explicitly extend itself. But for readability, I will mention X and X’s subtype.public void pushAll(Iterable&amp;lt;? extends E&amp;gt; from) { for (E element: from){ this.push(element); }}You don’t have to worry about whether from’s element can be casted to E, because the bounded wildcard ensures from‘s elements are and only are E’s type or E’s subtype.In the scope of Stack&amp;lt;E&amp;gt;, the fromobject from pushAll is considered/regarded as the producer of E. Since it produces E, and its products are being added into the stack.### ConsumerWith the pushAll method implemented above, you may think of adding a popAll method.Let’s start without that bounded wildcard support.public void popAll(Collection&amp;lt;E&amp;gt; to) { while (this.size() != 0) { to.add(this.pop()); }}Again, that to from popAll is considered/regarded as the consumer of E inside this scope. Since it consumes E from the Stack&amp;lt;E&amp;gt; then passing them out.However, If you pass a Collection&amp;lt;Object&amp;gt; object as to into this method, it will not compile. But normally you would expect, a Collection&amp;lt;Object&amp;gt; should have no trouble being added with Object and Object’s subtype. In other words, Collection&amp;lt;Object&amp;gt; should be able to add E, since E is a subtype of Object.Once again, Bounded Wildcard types provide a way out.We want to ensure that Collection&amp;lt;E&amp;gt; to is a Collection of E and E’s super-type. That’s how it comes into handy.public void popAll(Collection&amp;lt;? super E&amp;gt; to){ while (this.size() != 0) { to.add(this.pop()); }}ConclusionProper usage of bounded wildcard type is nearly invisible to the users of a class.Similar pattern can be seen in Java’s source code. It’s wildly used.For instance, in java.util.Collections /** * Copies all of the elements from one list into another. After the * operation, the index of each copied element in the destination list * will be identical to its index in the source list. The destination * list&#39;s size must be greater than or equal to the source list&#39;s size. * If it is greater, the remaining elements in the destination list are * unaffected. &amp;lt;p&amp;gt; * * This method runs in linear time. * * @param &amp;lt;T&amp;gt; the class of the objects in the lists * @param dest The destination list. * @param src The source list. * @throws IndexOutOfBoundsException if the destination list is too small * to contain the entire source List. * @throws UnsupportedOperationException if the destination list&#39;s * list-iterator does not support the {@code set} operation. */ public static &amp;lt;T&amp;gt; void copy(List&amp;lt;? super T&amp;gt; dest, List&amp;lt;? extends T&amp;gt; src) { int srcSize = src.size(); if (srcSize &amp;gt; dest.size()) throw new IndexOutOfBoundsException(&quot;Source does not fit in dest&quot;); if (srcSize &amp;lt; COPY_THRESHOLD || (src instanceof RandomAccess &amp;amp;&amp;amp; dest instanceof RandomAccess)) { for (int i=0; i&amp;lt;srcSize; i++) dest.set(i, src.get(i)); } else { ListIterator&amp;lt;? super T&amp;gt; di=dest.listIterator(); ListIterator&amp;lt;? extends T&amp;gt; si=src.listIterator(); for (int i=0; i&amp;lt;srcSize; i++) { di.next(); di.set(si.next()); } } }In the view of T in Collections.copy(List&amp;lt;? super T&amp;gt; dest, List&amp;lt;? extends T&amp;gt; src). src produces T, dest consumes T. Hence the bounded wildcard is fit in this case.At the same time, suggestions from Effective Java. If a type parammeter appears only once in a method declaration, replace it with a wildcard. All comparables and comparators are consumers. DO NOT use bounded wildcard types as return types. // Not recommended but readability is nice for me. public static &amp;lt;T extends Comparable&amp;lt;T&amp;gt;&amp;gt; T someMethod(List&amp;lt;T&amp;gt; list)// Replace it withpublic static &amp;lt;T extends Comparable&amp;lt;? super T&amp;gt;&amp;gt; someMethod(List&amp;lt;? extend T&amp;gt; list)" }, { "title": "Some Neat Design Patterns - Effective Java reading notes", "url": "/posts/Some-Neat-Design-Patterns-Effective-Java-reading-notes/", "categories": "Java, Design", "tags": "java", "date": "2021-06-10 00:22:19 +0800", "snippet": " Builder Pattern Simple Builder Pattern without considering Hierarchy Generic Type Builder Pattern (suits to class hierarchies) Singleton property Non-instantiability while dealing with Utility Class These patterns are pretty easy/simple, but it really helps me a lot especially when managing to understanding Java’s underlying design pattern through reading Java source code. Following these patterns also helps producing code which is developer-friendly. Code with Simplicity, Unifinity and Tidiness.Builder Pattern Just seen a post on V2EX not long before. It’s about which pattern you will choose when developing, via properties methods or getters/setters. properties methods =&amp;gt; obj.name() to retrieve name, obj.name(String n) to set name. getters/setters =&amp;gt; obj.getName() retrieve name, obj.setName() to set name.Since Java is discussed in this scope, you won’t feel unfamiliar with getters/setters pattern. A typical Java Bean pattern is:BeanObject beanObject = new BeanObject(); // No arg ConstructorbeanObject.setFirstProperty(arg1);beanObject.setSecondProperty(arg2);beanObject.setThirdProperty(arg3);Normally it will not be a problem, until you have a Java Bean consisting of a whole bunch of attributes/properties. Wherever this Bean is instantiated, you will have a ton of setter code piled up there.That is the perfect scenario where Builder Pattern comes in handy. It targets objects with a significant amount of properties and consisting of required properties. Following this pattern will make code tidy to some degree.Simple Builder Pattern without considering Hierarchypublic class ComputerComponents { // final properties &amp;lt;=&amp;gt; required properties. private final String cpu; private final String motherBoard; private final String ramStick; // private final String psu; // Optional properties private String hardDrive; private String solidStateDrive; private String graphicsCard; // make it private to dis-allow instantiability outside this class. private ComputerComponents(Builder builder) { this.cpu = builder.cpu; this.motherBoard = builder.motherBoard; this.ramStick = builder.ramStick; this.hardDrive = builder.hardDrive; this.solidStateDrive = builder.solidStateDrive; this.graphicsCard = builder.graphicsCard; } // Builder Starts // make it public to allow outer access. public static class Builder { // required properties. private final String cpu; private final String motherBoard; private final String ramStick; // Optional properties private String hardDrive; private String solidStateDrive; private String graphicsCard; // Ensure Builder comes with required properties public Builder(String cpu, String motherBoard, String ramStick) { this.cpu = builder.cpu; this.motherBoard = builder.motherBoard; this.ramStick = builder.ramStick; } // Optional properties public Builder hardDrive(String hardDrive) { this.hardDrive = hardDrive; } public Builder solidStateDrive(String solidStateDrive) { this.solidStateDrive = solidStateDrive; } public Builder graphicsCard(String graphicsCard) { this.graphicsCard = graphicsCard; } // call the private constructor inside this scope. // provide builder as init param. public ComputerComponents build() { return new ComputerComponents(this); } }} Class Constructor should be private. Avoid being invoked and instantiate outside this Class. Builder class Constructor should be public.The pattern may seem more complicated than the conventional getters/setter pattern.(especially Lombok meh…), though it makes somebody else who uses your code life a bit easier.new ComputerComponents .Builder(&quot;Ryzen_5900X&quot;, &quot;Asus Prime X570-Pro&quot;, &quot;SAMSUNG 16Gx2&quot;) .solidStateDrive(&quot;SAMSUMG 980 PRO&quot;) .build() Neat. Easy to write, easy to read. You can also implement validity check in Builder. Throws IllegalArgumentException if necessary.Builder Pattern suits to HierarchiesTop-level as abstract classpublic abstract class Cellphone { // properties that are common in sub-class. public enum Feature { HEADPHONE_JACK, QUICK_CHARGE, DEPTH_CAMERA, OLED_SCREEN } final Set&amp;lt;Feature&amp;gt; featureSet; abstract static class Builder&amp;lt;T extends Builder&amp;lt;T&amp;gt;&amp;gt; { EnumSet&amp;lt;Feature&amp;gt; featureSet = EnumSet.noneOf(Feature.class); // self() method =&amp;gt; simulated self-type idiom. // Subclass must override. protected abstract T self(); public T addFeature(Feature feature) { featureSet.add(Objects.requireNonNull(feature)); return self(); } abstract Cellphone build(); } Cellphone(Builder&amp;lt;?&amp;gt; builder) { // Deep clone. this.featureSet = builder.featureSet.clone(); }} Note that at Row 8, Builder uses a recursive type parameter, with the T self(), it will allow method in subclass work flawlessly without the type problem. Remember to override T self() in subclass’s Builder.Sub-classespublic class IPhone6S extends Cellphone { // properties that are identical to sub-class public enum Size {NORMAL, PLUS} private final Size size; public static class Builder extends Cellphone.Builder&amp;lt;Builder&amp;gt; { private final Size size; // Also, builder starts with required properties. public Builder(Size size) { this.size = Objects.requireNonNull(size); } @Override protected Builder self() { return this; } @Override public IPhone6S build() { return new IPhone6S(this); } } // Also uses private constructor to limit access. private IPhone6S(Builder builder) { super(builder); this.size = builder.size; }}public class IPhone12 extends Cellphone { // properties that are identical to sub-class public enum Size { MINI, NORMAL, PRO, PROMAX} private final Size size; public static class Builder extends Cellphone.Builder&amp;lt;Builder&amp;gt; { private final Size size; // Also, builder starts with required properties. public Builder(Size size) { this.size = Objects.requireNonNull(size); } @Override protected Builder self() { return this; } @Override public IPhone12 build() { return new IPhone12(this); } } // Also uses private constructor to limit access. private IPhone12(Builder builder) { super(builder); this.size = builder.size; }}The client code will be like:IPhone6S iPhone6S = new IPhone6S.Builder(IPhone6S.Size.PLUS) .addFeature(HEADPHONE_JACK).build();IPhone12 iPhone12 = new IPhone12.Builder(IPhone12.Size.PROMAX) .addFeature(QUICK_CHARGE).addFeature(DEPTH_CAMERA) .addFeature(OLED_SCREEN).build();Enforce SingletonA Singleton is a class that is instantiated exactly and only once. Usually enforcing a class as singleton is typically because that class is a state-less object or intrinsically unique.Singleton with public field and private constructorpublic class SystemKernel { public static final SystemKernel INSTANCE = new SystemKernel(); // private constructor private SystemKernel() { // init method } public XX retrieveStuff(){}}// Calling it likeSystemKernel.INSTANCE.retrieveStuff();### Singleton with private field, private constructor, but public getInstance() This is probably the most common way to do so???public class SystemKernel { private static final SystemKernel INSTANCE = new SystemKernel(); // private constructor private SystemKernel() { // init method } public SystemKernel getInstance() { return INSTANCE; } public XX retrieveStuff(){}}// Calling it likeSystemKernel.getInstance().retrieveStuff();Using either approach with Serializable, keep in mind that - to maintain singleton guarantee, declare all instance fields transient and provide a readResolve() method. Otherwise De-Serialization will break that guarantee.private Object readResolve() { // Effective Java Third Edition - Page 360. return INSTANCE;}### Enum Approach You may encounter problems when Class has to extend a super-class.public enum SystemKernel { INSTANCE; public XX retrieveStuff(){}}Non-instantiability while dealing with Utility Class Singleton =&amp;gt; instantiated exactly and only once.When dealing with utility class, especially one filled with static methods inside the class, you normally wouldn’t expect this class to be instantiated. But keep in mind that: A default constructor is generated if a class contains no explicit constructor. You may avoid adding constructor inside a utility class to prevent it from being instantiated. DO NO SUCH THING. Making a class abstract does not work. It does prevent a class from being instantiated by making it abstract. But its subclasses can be INSTANTIATED. Doing so will make user think you may want this class to have hierarchy usage. The solution is quite simple this time, just explicitly declare a private constructor inside your class that filled with static methods.public class MyStringUtils() { // explictly declare a private constructor private MyStringUtils() { } // filled with static methods. public static XX XXX() {} public static XX XXXX() {} //...}Both default constructor and abstract for hierarchy usage can be solved at the same time while keeping it developer-friendly." }, { "title": "Validate Object Using Reflection Inspired by Spring", "url": "/posts/Validate-Object-Using-Reflection-Inspired-by-Spring/", "categories": "Java, Design", "tags": "java, reflection, annotation", "date": "2021-06-06 15:15:27 +0800", "snippet": "Inspired by Spring’s excellent dependency injection way of handling bean validation, after reading some official java documentations and Spring Docs, I decided to find a way of achieving the purpose of validating object, but this time, without spring.Since no Spring will be involved this time, this post will mainly focusing on Java’s own Reflection API. And also I will re-use some great pattern inside Spring. Haven’t seen my last post? Check out HERE. Effective Java - Third Edition Java Documentation - Reflection Spring Documentation - 5.3.7IntroRemember how I design my own annotation @HexColorValue in my last post?@Target(ElementType.FIELD)@Retention(RetentionPolicy.RUNTIME)@Constraint(validatedBy = HexColorValueValidator.class)public @interface HexColorValue { String message() default &quot;{HexColorValue.inValid}&quot;; Class&amp;lt;?&amp;gt;[] groups() default {}; Class&amp;lt;? extends Payload&amp;gt;[] payload() default {};}It is the @Constraint annotation that annotated above this exact annotation that gives it magic. It allows Spring to locate the class of our custom validator and inject it on the fly.Check out out last validator class itself.public class HexColorValueValidator implements ConstraintValidator&amp;lt;HexColorValue, String&amp;gt; { private static final Pattern hexColorValuePattern = Pattern.compile(&quot;^#?([a-fA-F0-9]{6}|[a-fA-F0-9]{3})$&quot;); @Override public boolean isValid(String value, ConstraintValidatorContext context) { return hexColorValuePattern.matcher(value).matches(); }}It implements interface ConstraintValidator&amp;lt;A extends Annotation, T&amp;gt; to equip this interface with our custom validation capability. This technique significantly improves the code extendibility horizontally. (Which means you can add a bunch of custom validator without polluting you code with shit ton of utility classes.) For how to turn this into a spring service, I have illustrated it in the last post, again, check out here.这样写可以使代码在水平方向上的延展能力极大增强. 只需要在需要的地方打上注解, 需要校验的地方用自动注入加入一个验证器, 一行解决且无侵入. 接下来就是在无Spring的情况, 仅用Reflection API来复现逻辑, 摆脱框架.Custom AnnotationStart by creating out own Annotation. I will use @Ipv4Address as a starter this time.@Ipv4Address@Target(ElementType.FIELD)@Retention(RetentionPolicy.RUNTIME)@ValidateBy(clazz = Ipv4AddressValidatorImpl.class)public @interface Ipv4Address {}This annotation requires nothing passing in when it is being annotated. Although pay attention to @Target. It means this annotation is annotated above FIELD (字段). RetentionPolicy.RUNTIME means this annotation will still be remained in RUNTIME. @ValidateBy is also a custom annotation. Inspired by Spring’s @Constraint(validateBy=?)@ValidateByWhen we design our validation logic inside some utils class, it is essential to know what exact Validator Class or Method we will be using and invoking. This piece of information must be grabbed from the constraint annotation. And inspired by Spring. the best place for storing this information is the annotation’s annotation. Because you will never want to add Validator Class everytime you use that annotation.Here is the simple example of illustrating the difference. No @ValidateBy on @Ipv4Adress// If @Target(ElementType.FIELD)@Retention(RetentionPolicy.RUNTIME)public @interface Ipv4Address{ Class&amp;lt;? extend Validator&amp;gt; validateBy();}// You will need to assign validateBy everytime you use this annotation.public class QO{ @Ipv4Adrress(validateBy=Ipv4AddressValidator.class) String firstIp; @Ipv4Adrress(validateBy=Ipv4AddressValidator.class) String secondIp; @Ipv4Adrress(validateBy=Ipv4AddressValidator.class) String thirdIp;}// You don&#39;t want that. @ValidateBy on @Ipv4Adress@Target(ElementType.FIELD)@Retention(RetentionPolicy.RUNTIME)@ValidateBy(clazz = Ipv4AddressValidatorImpl.class)public @interface Ipv4Address {}// You NEED NOT TO assign validateBy everytime you use this annotation.public class QO{ @Ipv4Adrress String firstIp; @Ipv4Adrress String secondIp; @Ipv4Adrress String thirdIp;}This piece of information should be stored as annotation inside the annotation itself. So, let’s see@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.ANNOTATION_TYPE) // it means this annotation should be annotated above annotation.public @interface ValidateBy { Class&amp;lt;? extends Validator&amp;lt;?&amp;gt;&amp;gt; clazz();}Validator ImplementThis interface itself only has one method, which is isValid(T value)Validator.javapublic interface Validator&amp;lt;T&amp;gt; { boolean isValid(T value);}We will implement the capability of validation here. Staring with Ipv4AdressValidatorImplpublic class Ipv4AddressValidatorImpl implements Validator&amp;lt;String&amp;gt; { private static final Pattern v4Pattern = Pattern.compile(&quot;^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?).){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$&quot;); public Ipv4AddressValidatorImpl() { // Reflection&#39;s getDeclaredConstructor() } // Implementation is here. public boolean isValid(String value) { return isValidIpv4Address(value); } private static boolean isValidIpv4Address(String value) { return v4Pattern.matcher(value).matches(); }}Now that we have given the capability to interface Validator.Service-ify the ValidationSpring uses ValidationFactory to provide validator. For me, a ValidationProvider is more than enough. Singleton Pattern is introduced here.ValidationProvider - frameworkpublic class ValidationProvider { private static final ValidationProvider INSTANCE = new ValidationProvider(); // NOTE: SINGLETON PATTERN private ValidationProvider() { // Non-instantiable outside this scope because of this private constructor. } public static ValidationProvider getInstance() { return INSTANCE; } public &amp;lt;T&amp;gt; Collection&amp;lt;ViolationException&amp;gt; validate(T object) { // validation will start here. }ValidationProvider - logicBefore I paste up all the code from validate(T object), couple insights of how the validation service are as follows. ValidationProvider.getInstance() will allow access from any scope of the project, and ready for validating by calling validationProvider.validate(object). Validation Logic can be simply described in the following steps. object is passed into validate(object) Fields that declared inside the object will all be collected using getDeclaredFields() provided by java.lang.reflect. If the field is still not accessible after field.trySetAccessible(), a ViolationException will be added to the violationList with corresponding reason. For each field, its annotations will be gathered using getDeclaredAnnotations(). If the annotation is related to validation, it will try to dig out the @ValidateBy by following up the annotation “Inheritance link”. If @ValidateBy can be found, which means the corresponding annotation is a Constraint-Related annotation. Because annotation stores specific Class by using @ValidateBy(clazz=XXX), XXX will be instantiate using getDeclaredConstructor().newInstance(). &quot;isValid&quot; method will be invoked by reflection API to gather the validation result. You may wonder why XXX will always have the same “isValid” method that can be invoked via reflection, that’s exactly the reason why we need Validator as a interface and then implement the isValid method afterwards. For Reflection API usage, Here is the documentation you can look into. ViolationException will be included in the end of this post. public &amp;lt;T&amp;gt; Collection&amp;lt;ViolationException&amp;gt; validate(T object) { Field[] fields; Annotation[] annotations; List&amp;lt;ViolationException&amp;gt; violationList = new LinkedList&amp;lt;&amp;gt;(); fields = object.getClass().getDeclaredFields(); // For each field. for (Field field : fields) { if (!field.trySetAccessible()) { violationList.add( new ViolationException.Builder() .object(object) .field(field) .reason(&quot;Fails to access object&#39;s field.&quot;) ); continue; } annotations = field.getDeclaredAnnotations(); // For each field&#39;s annotation for (Annotation annotation : annotations) { try { // Annotation&#39;s annotation. Locate the @ValidateBy annotation declared // on the corresponding annotation. This will lead us to the // Validator Class we assigned. ValidateBy validateBy = annotation.annotationType().getDeclaredAnnotation(ValidateBy.class); if (validateBy == null) { // No Validator Class is assigned. continue; } // Retrieve the validator class assigned from the corresponding annotation by @ValidateBy(clazz=?) Class&amp;lt;? extends Validator&amp;lt;?&amp;gt;&amp;gt; assignedValidator = validateBy.clazz(); // Prepare the Method for validation of the exact validator class provided at @ValidateBy Method validationMethod = assignedValidator.getMethod(&quot;isValid&quot;, field.getType()); validationMethod.setAccessible(true); // Construct a instance of the exact Validator using Reflection for method invoke usage. // e.g: Ipv4AdressValidatorImpl::new Validator validator = assignedValidator.getDeclaredConstructor().newInstance(); // Invoke method and get the validation result boolean result = (boolean) validationMethod.invoke(validator, field.get(object)); // Validation return false if (!result) { violationList.add( new ViolationException.Builder() .object(object) .field(field) .constraintAnnotation(annotation.annotationType()) .build() ); } } catch (InvocationTargetException | IllegalAccessException | NoSuchMethodException | InstantiationException e) { violationList.add( new ViolationException.Builder() .object(object).field(field).reason(e.getLocalizedMessage()) ); } } } return violationList; }Simple Test-Drive Horizontal extendibility will be described in next chapter.Let’s write up a simple test case.public class MyQueryObject { @Ipv4Address private String ipRangeStart; @Ipv4Address private String ipRangeStop; @Ipv4Address private String proxyPoolAddress; // getter and setter}MyQueryObject validObj = new MyQueryObject();validObj.setIpRangeStart(&quot;10.0.0.1&quot;);validObj.setIpRangeStop(&quot;10.7.255.254&quot;);validObj.setProxyPoolAddress(&quot;172.17.0.1&quot;);MyQueryObject invalidObj = new MyQueryObject();invalidObj.setIpRangeStart(&quot;255.256.258.999&quot;);invalidObj.setIpRangeStop(&quot;172.18.1.1&quot;);invalidObj.setProxyPoolAddress(&quot;where 1=1&quot;);ValidationProvider validationProvider = ValidationProvider.getInstance(); // Violation Count System.out.println(validationProvider.validate(validObj).size());// Violation OutputSystem.err.println(validationProvider.validate(invalidObj));0[===&amp;gt; Violation &amp;gt; xyz.mijazz.springfreevalidation.objects.MyQueryObject.ipRangeStart | @Ipv4Address, ===&amp;gt; Violation &amp;gt; xyz.mijazz.springfreevalidation.objects.MyQueryObject.proxyPoolAddress | @Ipv4Address]Smoothly.Horizontal Extendibility Horizontal Extendibility in the post, specifically in plain words, means you can create your custom validation annotations as many as you want. For instance, I want to create @Ipv6Adress, @FutureDatetime, @PastDatetime, and then annotated them all inside one object.ValidationProvider is designed to be applicable to various situation. In this case, we only need to create our annotation class annotated by @ValidateBy(clazz=XXX), then a XXXValidationImpl implements Validator. and boom you are good to go. No need to alter code elsewhere.@Ipv6Adress - Step 1@Target(ElementType.FIELD)@Retention(RetentionPolicy.RUNTIME)@ValidateBy(clazz = Ipv6AddressValidatorImpl.class)public @interface Ipv6Address {}Ipv6AddressValidatorImpl - Step 2public class Ipv6AddressValidatorImpl implements Validator&amp;lt;String&amp;gt; { private static final Pattern v6Pattern = Pattern.compile(&quot;^(([0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,7}:|([0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,5}(:[0-9a-fA-F]{1,4}){1,2}|([0-9a-fA-F]{1,4}:){1,4}(:[0-9a-fA-F]{1,4}){1,3}|([0-9a-fA-F]{1,4}:){1,3}(:[0-9a-fA-F]{1,4}){1,4}|([0-9a-fA-F]{1,4}:){1,2}(:[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:((:[0-9a-fA-F]{1,4}){1,6})|:((:[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(:[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(ffff(:0{1,4}){0,1}:){0,1}((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\\\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])|([0-9a-fA-F]{1,4}:){1,4}:((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\\\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9]))$&quot;); public Ipv6AddressValidatorImpl() { // Reflection&#39;s getDeclaredConstructor() } // Implement Validator&#39;s method. public boolean isValid(String value) { return isValidIpv6Address(value); } private static boolean isValidIpv6Address(String value) { return v6Pattern.matcher(value).matches(); }}MyV6QueryObject - Step 3// Mixed with Ipv4, Ipv6.public class MyV6QueryObject { @Ipv6Address private String ipRangeStart; @Ipv6Address private String ipRangeStop; @Ipv4Address private String proxyPoolIp; // getters and setters}Let’s write up a test case shall we.MyV6QueryObject myV6QueryObject = new MyV6QueryObject();myV6QueryObject.setIpRangeStart(&quot;fd0a:e481:6bf9:d049:0000:0000:0000:0000&quot;);myV6QueryObject.setIpRangeStop(&quot;fd0a:e481:6bf9:d049:?!*#:ff=ff:!*@3:ffff&quot;);myV6QueryObject.setProxyPoolIp(&quot;172.17.0.1&quot;);use that same ValidationProviderValidationProvider validationProvider = ValidationProvider.getInstance();System.out.println(validationProvider.validate(myV6QueryObject));[===&amp;gt; Violation &amp;gt; xyz.mijazz.springfreevalidation.objects.MyV6QueryObject.ipRangeStop | @Ipv6Address]So the main idea of this chapter is that, you don’t have to bother yourself altering code inside that ValidationProvider. For horizontal development in the future, you just have to make your own annotation and implement corresponding Validator, and boom, you are good to go.Extended ResultI went ahead then finish the logic of @FutureDatetime, @PastDatetime. The result came back fine.public class MixedQueryObject { @Ipv4Address String validIpv4; @Ipv4Address String invalidIpv4; @Ipv6Address String validIpv6; @Ipv6Address String inValidIpv6; @FutureDatetime LocalDateTime validFutureDt; @FutureDatetime LocalDateTime invalidFutureDt; @PastDatetime LocalDateTime validPastDt; @PastDatetime LocalDateTime invalidPastDt;}[===&amp;gt; Violation &amp;gt; xyz.mijazz.springfreevalidation.objects.MixedQueryObject.invalidIpv4 | @Ipv4Address, ===&amp;gt; Violation &amp;gt; xyz.mijazz.springfreevalidation.objects.MixedQueryObject.inValidIpv6 | @Ipv6Address, ===&amp;gt; Violation &amp;gt; xyz.mijazz.springfreevalidation.objects.MixedQueryObject.invalidFutureDt | @FutureDatetime, ===&amp;gt; Violation &amp;gt; xyz.mijazz.springfreevalidation.objects.MixedQueryObject.invalidPastDt | @PastDatetime]ConclusionAt the time I wrote up this post, I was thinking that maybe adding a nullable feature will be helpful. After all, the whole idea of this post is to offer a way/possibility to validate object without using any Spring or Hibernate feature.Although those frameworks do great or even excellent jobs at creating the things we need or use in our dev career, it’s still worth having a look inside to figure out how that actually works and manage to achieve it without using them. At the end of the day, I PREFER NOT use framework only to do the FARMWORK.MASSIVE shout out to Effective Java - Third Edition by Joshua Bloch." }, { "title": "From Spring Boot Bean Validation to Python Wrapper", "url": "/posts/From-Spring-Boot-Bean-Validation-to-Python-Wrapper/", "categories": "Java, Design", "tags": "python, java, annotation, wrapper", "date": "2021-06-03 12:59:54 +0800", "snippet": "Keywords in this post are as follows. Spring Boot Bean Validation JSR-303/349/380: Bean Validation Related Topic JSR Java Annotation Processing Custom Annotation Java Reflection API (java.lang.reflect) Python Function WrapperPrerequisite. Basic Spring Boot 2 knowledge(IoC)Bean Validation ScenarioSpring boot code exampleIf you have encountered with Java Bean Validation or Field Validation, then you may be familiar with the following pattern.pom.xml(Maven3)&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-validation&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;This is our Controller’s incoming query payload class. (Query Object)// Lombok enabled.@Datapublic class MyQueryObject{ @Min(0) @Max(100) private int queryNumberInstance; @Pattern(regexp = &quot;^#?([a-fA-F0-9]{6}|[a-fA-F0-9]{3})$&quot;) private String hexColorValue; // getter, setter, ... handled by Lombok.}Now the controller itself.@RestControllerpublic class MyControllerDemo{ @RequestMapping(value = &quot;/some/path&quot;, method = RequestMethod.POST, produces = &quot;application/json&quot;) public ResponseEntity&amp;lt;String&amp;gt; someQuery(@Valid @RequestBody MyQueryObject queryObj){ // ... return ResponseEntity.ok(&quot;Here u go.&quot;); }}If the query object itself fails the validation check, a MethodArgumentNotValidException will be generated, and by default, spring will translate that exception into HTTP STATUS - 400(BAD REQUEST).Spring boot test exampleProvided by spring-test itself, MockMvc can be instanized to perform mock POST request easily. Let’s write up a simple spring boot test, shall we?@SpringBootTest@AutoConfigureMockMvc@Slf4jpublic class QueryObjectValidationTest { @Autowired private MockMvc mockMvc; @Test void passValidQueryObject_to_MyControllerDemo() throws Exception { MediaType mediaType = MediaType.APPLICATION_JSON; // This payload will not cause any exception. String validPayload = &quot;{\\&quot;queryNumberInstance\\&quot;: 69, \\&quot;hexColorValue\\&quot; : \\&quot;#282a36\\&quot;}&quot;; mockMvc.perform(MockMvcRequestBuilders.post(&quot;/some/path&quot;) .content(validPayload) .contentType(mediaType)) .andExpect(MockMvcResultMatchers.status().isOk()) .andExpect(MockMvcResultMatchers.content().contentType(mediaType)) .andDo(mvcResult -&amp;gt; log.info(mvcResult.getResponse().getContentAsString())); // This payload will definitely trigger exception, // and spring will handle it with BAD_REQUEST. String invalidPayload = &quot;{\\&quot;queryNumberInstance\\&quot;: 96, \\&quot;hexColorValue\\&quot; : \\&quot;#Z10RYP\\&quot;}&quot;; mockMvc.perform(MockMvcRequestBuilders.post(&quot;/some/path&quot;) .content(invalidPayload) .contentType(mediaType)) .andExpect(MockMvcResultMatchers.status().isBadRequest()) .andDo(mvcResult -&amp;gt; log.info(String.valueOf(HttpStatus.valueOf(mvcResult.getResponse().getStatus())))); } Test provided above passed without exception.2021-06-03 15:35:56.324 INFO 15821 --- [ main] i.m.t.QueryObjectValidationTest : Here u go.2021-06-03 15:35:56.344 WARN 15821 --- [ main] .w.s.m.s.DefaultHandlerExceptionResolver : Resolved [org.springframework.web.bind.MethodArgumentNotValidException: Validation failed for argument [0] in public org.springframework.http.ResponseEntity&amp;lt;java.lang.String&amp;gt; icu.mijazz.temporaryspringbootdemo.controller.Demo2Controller.someQuery(icu.mijazz.temporaryspringbootdemo.qo.MyQueryObject): [Field error in object &#39;myQueryObject&#39; on field &#39;hexColorValue&#39;: rejected value [#Z10RYP]; codes [Pattern.myQueryObject.hexColorValue,Pattern.hexColorValue,Pattern.java.lang.String,Pattern]; arguments [org.springframework.context.support.DefaultMessageSourceResolvable: codes [myQueryObject.hexColorValue,hexColorValue]; arguments []; default message [hexColorValue],[Ljavax.validation.constraints.Pattern$Flag;@7ac058a0,^#?([a-fA-F0-9]{6}|[a-fA-F0-9]{3})$]; default message [must match &quot;^#?([a-fA-F0-9]{6}|[a-fA-F0-9]{3})$&quot;]] ]2021-06-03 15:35:56.347 INFO 15821 --- [ main] i.m.t.QueryObjectValidationTest : 400 BAD_REQUESTException Handling(Additional) A digression into irrelevant details.@ExceptionHandler annotation@ExceptionHandler(value = MethodArgumentNotValidException.class)public ResponseEntity&amp;lt;String&amp;gt; invalidQueryHandler() { // Do Whatever. return ResponseEntity.status(HttpStatus.I_AM_A_TEAPOT).body(&quot;Handled.&quot;);}Other ScenarioYou may also find it common for developers to have validation-annotation in Persistence Layer Entity. By default, Spring Data uses Hibernate underneath, which supports Bean Validation. However, Validations done in persistence layer act only as a anti data-corruption method. They can effectively stop invalid data from being written to DB, yet have little effect on the protection of parameter Injection based attack.Custom Validator**You may wonder why I wrote a complete example just to illustrate how the spring boot validator’ s approach on parameter validation. **Because besides that, spring boot also offers a approach to customize and realize your data validator just by minor implementation.Construct a Custom AnnotationRemember how we validate the MyQueryObject.hexColorValue above? We use a @Pattern with a regex . What if we can create our own annotation @HexColorValue to validate every occurrence of Hex Color Value inside our project. It will improve our code readability significantly.@Target(ElementType.FIELD)@Retention(RetentionPolicy.RUNTIME)@Constraint(validatedBy = HexColorValueValidator.class)public @interface HexColorValue { String message() default &quot;{HexColorValue.inValid}&quot;; Class&amp;lt;?&amp;gt;[] groups() default {}; Class&amp;lt;? extends Payload&amp;gt;[] payload() default {}; }Note that @Constraint is provided by javax.validation.Constraint. **Unlike the common scenario of creating custom annotation from stretch, this annotation with corresponding validator class will save us some trouble creating our own Annotation Processor using Java Reflection API. **We only need to use @Constraint to explicitly point out which class is our validation handler, spring will automatically instantiate a instace. Validator discussed in this scope should have an implementation of interface ConstraintValidator&amp;lt;A extends Annotation, T&amp;gt;, in this case, which is ConstraintValidator&amp;lt;HexColorValue, String&amp;gt;. message() =&amp;gt; ValidationMessages.properties The ValidationMessages resource bundle and the locale variants of this resource bundle contain strings that override the default validation messages. The ValidationMessages resource bundle is typically a properties file, ValidationMessages.properties, in the default package of an application.### Implement ConstraintValidatorAfter creating our own @HexColorValue and pointing validator class using @Constraint. We will implement HexColorValueValidator as follows.public class HexColorValueValidator implements ConstraintValidator&amp;lt;HexColorValue, String&amp;gt; { private static final Pattern hexColorValuePattern = Pattern.compile(&quot;^#?([a-fA-F0-9]{6}|[a-fA-F0-9]{3})$&quot;); @Override public boolean isValid(String value, ConstraintValidatorContext context) { return hexColorValuePattern.matcher(value).matches(); }}Substitute @Pattern with @HexColorValue@Datapublic class MyQueryObject{ // ...... // No longer need @Pattern // @Pattern(regexp = &quot;^#?([a-fA-F0-9]{6}|[a-fA-F0-9]{3})$&quot;) @HexColorValue private String hexColorValue;}Everything will work smoothly as it used to be.Invoke Validation ManuallySince we have already created a custom validator class for HexColorValue, what if we need to re-use that validator code to validate String on the fly.You may think of using HexColorValueValidator.isValid(String hexValue) -&amp;gt; boolean. However, Raw usage of this method is not recommended. (I just cannot find the way to explicitly invoke that exact method without providing a self ConstraintValidatorContext context…). Spring got us covered by using it perfect dependency injection technique.Bean Tweak implements Serializable(Optional).public class MyQueryObject implements SerializableService-ify the manual validator with Generic Class Support Validation, ValidatorFactory, Validator … validator.validate() will automatically locate the field that needs to be validated.(Annotated with registered with @Constraint), then invoke the corresponding method inside the registered validation handler class to proceed.@Servicepublic class ValidationService&amp;lt;A extends Serializable&amp;gt; { private final ValidatorFactory validatorFactory = Validation.buildDefaultValidatorFactory(); private final Validator validator = validatorFactory.getValidator(); public boolean isValid(A objectPendingValidate) { Set&amp;lt;ConstraintViolation&amp;lt;A&amp;gt;&amp;gt; violations = validator.validate(objectPendingValidate); return violations.isEmpty(); }}with the validation class being @Service-ify, now we can use @Autowired to inject ValidationService project-wisely.Test-Drive our ValidationService @Autowired ValidationService&amp;lt;MyQueryObject&amp;gt; qoValidationService; @Test void customValidationService_on_MyQueryObject() { MyQueryObject validQueryObject = new MyQueryObject(); validQueryObject.setQueryNumberInstance(69); validQueryObject.setHexColorValue(&quot;#2B2C2D&quot;); MyQueryObject invalidQueryObject = new MyQueryObject(); invalidQueryObject.setQueryNumberInstance(96); invalidQueryObject.setHexColorValue(&quot;#ZSDZXF&quot;); assertTrue(qoValidationService.isValid(validQueryObject)); assertFalse(qoValidationService.isValid(invalidQueryObject)); // Test passed. }Strict ValidationHere’s an interesting point. When we call the constructor of MyQueryObject to initialize an instance, instead of letting spring handle the bean via dependency injection, the validation strategy will not kick in.Look at invalidQueryObject.setHexColorValue(&quot;#ZSDZXF&quot;);. We just throw a invalid value to a setter. What if we are in a situation when all the bean existence should be “strictly valid”?Well, this perhaps goes a little bit off the track of this post original purpose. You can always make some validation implementations in the setter or getter.Python Wrapper ApproachIntroductionIf you are not familiar with python wrapper pattern or how does it work in python, it will not matter. If you ever coded in python, it wouldn’t be hard for you to understanding the following pattern. Remember how %timeit works in Jupyter Notebook?import timedef timeit(function_instance): def wrapper(*args, **kargs): time_start = time.time() result = function_instance(*args, **kargs) time_stop = time.time() print(&#39;{} second(s) taken up to execute function {}&#39; \\ .format(time_stop - time_start, function_instance.__name__)) return result return wrapper@timeitdef yell_out_shit(slogan: str) -&amp;gt; None: time.sleep(1.5) print(slogan)if __name__ == &quot;__main__&quot;: yell_out_shit(&quot;PHP is the best language!&quot;)❯ python /home/mijazz/Dev/pyworkspace/temp.pyPHP is the best language!1.5018470287322998 second(s) taken up to execute function yell_out_shitBut how come this ever get related in Java Bean Validation?Scene Re-appearancedef math_func_need_positiveNumber(input: int): if input &amp;lt;= 0: raise AssertionError(&#39;Invalid input in {} with {}&#39;\\ .format(math_func_need_positiveNumber.__name__, input)) # bla, bla, bla passdef math_func_need_negativeNumber(input: float): if input &amp;gt; 0: raise AssertionError(&#39;Invalid input in {} with {}&#39;\\ .format(math_func_need_negativeNumber.__name__, input)) passdef func_need_num_in_range(input: float): if input &amp;lt; 69 or input &amp;gt; 96: raise AssertionError(&#39;Invalid input in {} with {}&#39;\\ .format(func_need_num_in_range.__name__, input)) passFor coder just need python for some simple math calculation, it is not uncommon to see this type of code…However, to state my point, I am not saying this code style is bad or something, a quick and dirty fix can simply be realized by python wrapper.Implement our own @Range() annotation In our MyQueryObject.queryNumberInstance, I use @Min(1), @Max(100) annotations to specify a range. But just so you know, org.hibernate.validator.constraints.Range is also out of the box. @Range(min=1, max=100) will be like the exact same.Let’s start with the Range method first.a little heads up. *annotion_args is positional args given in decorator. They are decorator/wrapper params. **annotation_kargs is keyword args given in decorator. They are decorator/wrapper params. *func_args is the positional args given into the function itself. They are function params. **func_kargs is the keyword args given into the function itself. They are function params.def Range(*annotation_args, **annotation_kargs): def wrapper(func): def on_invoke(*func_args, **func_kargs): # Keyword args processing for (arg_key, (low, high)) in annotation_kargs.items(): if low is not None and func_kargs[arg_key] &amp;lt; low: raise AssertionError(&#39;{} - Limit exceeded.&#39;.format(func_kargs[arg_position])) if high is not None and func_kargs[arg_key] &amp;gt;= high: raise AssertionError(&#39;{} + Bound exceeded.&#39;.format(func_kargs[arg_position])) # Positional args processing for (arg_position, low, high) in annotation_args: if low is not None and func_args[arg_position] &amp;lt; low: raise AssertionError(&#39;{} - Limit exceeded.&#39;.format(func_args[arg_position])) if high is not None and func_args[arg_position] &amp;gt;= high: raise AssertionError(&#39;{} + Bound exceeded.&#39;.format(func_args[arg_position])) return func(*func_args, **func_kargs) return on_invoke return wrapperUsageLet’s see how this puppy works. Derive a function of common usage.@Range((0, None, 0), (1, 0, 1), positive=(1, None))def some_func(negative, ZERO, **kwargs): pass (0, None, 0) and (1, 0, 1) are *annotion_args, which represents that function_arg[0] &amp;lt;=&amp;gt; negative should have None as low limit, 0 as upper bound; Function_args[1] &amp;lt;=&amp;gt; ZERO should have 0 as low limit, 1 as upper bound.In general. (0, None, 0) =&amp;gt; 0th positional arg should be in [-inf, 0). (1, 0, 1) =&amp;gt; 1st positional arg should be in [0, 1). positive=(1, None) =&amp;gt; keyword arg positive should be in [1, +inf).some_func(-9, 0, positive=9) # Smooth.some_func(-9, 0, 9) # positive not given.some_func(10, 0, positive=9) # 10 + Upper bound exceeded.ThoughtsYou can even use the same technique to do a argument Type Check, instead of using a regular one. (Back in python2 style ??? or pythonic???).def ForceType(**annotation_kargs): def wrapper(func): def on_invoke(**function_kargs): for (argname, type) in annotation_kargs.items(): if not isinstance(function_kargs[argname], type): raise TypeError() return func(**function_kargs) return on_invoke return wrapper@ForceType(Integer=int, String=str)def another_func(*, Integer, String): passanother_func(Integer=1, String=&quot;2&quot;) # Smoothanother_func(Integer=1, String=2) # TypeErrorOnce you figure out the underlying pattern of the example provided above, it’s easy to implement other feature like str length check." }, { "title": "Decision Tree Wine Classification", "url": "/posts/Decision-Tree-Wine-Classification/", "categories": "Data, Python", "tags": "pandas, data, visualization, decisiontree", "date": "2021-05-11 04:18:12 +0800", "snippet": " Go to this gist for original files.# -*- coding: utf-8 -*-# @Author: MijazzChan, 2017326603075# @ =&amp;gt; https://mijazz.icu# Python Version == 3.8import osimport pandas as pdimport numpy as npfrom matplotlib import pyplot as pltimport seaborn as snsfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_scorefrom sklearn.tree import DecisionTreeClassifier, export_textfrom sklearn.utils import resamplefrom sklearn.metrics import confusion_matrix, classification_reportimport warnings%matplotlib inlineplt.rcParams[&#39;figure.dpi&#39;] = 150plt.rcParams[&#39;savefig.dpi&#39;] = 150sns.set(rc={&quot;figure.dpi&quot;: 150, &#39;savefig.dpi&#39;: 150})from jupyterthemes import jtplotjtplot.style(theme=&#39;monokai&#39;, context=&#39;notebook&#39;, ticks=True, grid=False)from IPython.core.display import HTMLHTML(&quot;&quot;&quot;&amp;lt;style&amp;gt;.output_png { display: table-cell; text-align: center; vertical-align: middle;}&amp;lt;/style&amp;gt;&quot;&quot;&quot;);Data Preprocessing Reading from csv. Replace space( ) to underscore(_) in column names check whether data containing nan/null. check if data gets any duplicated entries.original_data = pd.read_csv(&#39;./winequality-red.csv&#39;, encoding=&#39;utf-8&#39;)# Tweaks header. GET RID OF THOSE DAMN SPACE!new_columns = list(map(lambda col: str(col).replace(&#39; &#39;, &#39;_&#39;), original_data.columns.to_list()))original_data.columns = new_columnsoriginal_data.head(6) fixed_acidity volatile_acidity citric_acid residual_sugar chlorides free_sulfur_dioxide total_sulfur_dioxide density pH sulphates alcohol quality 0 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5 1 7.8 0.88 0.00 2.6 0.098 25.0 67.0 0.9968 3.20 0.68 9.8 5 2 7.8 0.76 0.04 2.3 0.092 15.0 54.0 0.9970 3.26 0.65 9.8 5 3 11.2 0.28 0.56 1.9 0.075 17.0 60.0 0.9980 3.16 0.58 9.8 6 4 7.4 0.66 0.00 1.8 0.075 13.0 40.0 0.9978 3.51 0.56 9.4 5 5 7.9 0.60 0.06 1.6 0.069 15.0 59.0 0.9964 3.30 0.46 9.4 5 # Check whether data got null/na mixed inside.original_data.isna().sum()fixed_acidity 0volatile_acidity 0citric_acid 0residual_sugar 0chlorides 0free_sulfur_dioxide 0total_sulfur_dioxide 0density 0pH 0sulphates 0alcohol 0quality 0dtype: int64original_data.describe().T count mean std min 25% 50% 75% max fixed_acidity 1359.0 8.310596 1.736990 4.60000 7.1000 7.9000 9.20000 15.90000 volatile_acidity 1359.0 0.529478 0.183031 0.12000 0.3900 0.5200 0.64000 1.58000 citric_acid 1359.0 0.272333 0.195537 0.00000 0.0900 0.2600 0.43000 1.00000 residual_sugar 1359.0 2.523400 1.352314 0.90000 1.9000 2.2000 2.60000 15.50000 chlorides 1359.0 0.088124 0.049377 0.01200 0.0700 0.0790 0.09100 0.61100 free_sulfur_dioxide 1359.0 15.893304 10.447270 1.00000 7.0000 14.0000 21.00000 72.00000 total_sulfur_dioxide 1359.0 46.825975 33.408946 6.00000 22.0000 38.0000 63.00000 289.00000 density 1359.0 0.996709 0.001869 0.99007 0.9956 0.9967 0.99782 1.00369 pH 1359.0 3.309787 0.155036 2.74000 3.2100 3.3100 3.40000 4.01000 sulphates 1359.0 0.658705 0.170667 0.33000 0.5500 0.6200 0.73000 2.00000 alcohol 1359.0 10.432315 1.082065 8.40000 9.5000 10.2000 11.10000 14.90000 quality 1359.0 5.623252 0.823578 3.00000 5.0000 6.0000 6.00000 8.00000 original_data.info()&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;RangeIndex: 1359 entries, 0 to 1358Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 fixed_acidity 1359 non-null float64 1 volatile_acidity 1359 non-null float64 2 citric_acid 1359 non-null float64 3 residual_sugar 1359 non-null float64 4 chlorides 1359 non-null float64 5 free_sulfur_dioxide 1359 non-null float64 6 total_sulfur_dioxide 1359 non-null float64 7 density 1359 non-null float64 8 pH 1359 non-null float64 9 sulphates 1359 non-null float64 10 alcohol 1359 non-null float64 11 quality 1359 non-null int64 dtypes: float64(11), int64(1)memory usage: 127.5 KB# Look for duplicatesoriginal_data.duplicated().sum()0On quality(target) columnit seems that quality column only contains a few unique values. Go check that out shall we.original_data[&#39;quality&#39;].unique().tolist()[5, 6, 7, 4, 8, 3]#original_data.groupby([&#39;quality&#39;]).size().to_dict()quality_plot_data = original_data.groupby([&#39;quality&#39;]).size().reset_index()quality_plot_data.columns = [&#39;quality&#39;, &#39;count&#39;]quality_plot_data.plot(kind=&#39;bar&#39;, x=&#39;quality&#39;, y=&#39;count&#39;, rot=0)Data visualizationBox plotsHave a look at how each column data affects others.# tempX appended after each var is sort of a anti-corruption method? For me...warnings.filterwarnings(&quot;ignore&quot;)fig_temp1, ax_temp1 = plt.subplots(4, 3, figsize=(32, 24))rolling_index = 0columns_temp1 = list(original_data.columns)columns_temp1.remove(&#39;quality&#39;)for fig_row in range(4): for fig_col in range(3): sns.boxplot(x=&#39;quality&#39;, y=columns_temp1[rolling_index], data=original_data, ax=ax_temp1[fig_row][fig_col]) rolling_index += 1 if (rolling_index &amp;gt;= len(columns_temp1)): breakplt.show()Heatmap on corr()# Need Square plot here temporailyplt.figure(figsize=(16, 16))sns.heatmap(original_data.corr(), annot=True, cmap=&#39;vlag&#39;)Distributions(distplot)# Anti-variable-corrupt kicks in.warnings.filterwarnings(&quot;ignore&quot;)fig_temp2, ax_temp2 = plt.subplots(4, 3, figsize=(32, 24))rolling_index = 0columns_temp2 = list(original_data.columns)columns_temp2.remove(&#39;quality&#39;)for fig_row in range(4): for fig_col in range(3): sns.distplot(original_data[columns_temp2[rolling_index]], ax=ax_temp2[fig_row][fig_col]) rolling_index += 1 if (rolling_index &amp;gt;= len(columns_temp2)): breakMinor tweaks based on the visualization plotWhy?It is noticeable that some data distributing in a skew way. No good for training. TODO: log trans? or sigmoid trans? MijazzChan @ 20210510220732Let’s add some transformation shall we. residual sugar chlorides free SO2 total SO2 sulphates alcoholdef trans_tweaks(col): return np.log(col[0])tweaked_data = original_data.copy()cols_need_tweaks = [&#39;residual_sugar&#39;, &#39;chlorides&#39;, &#39;free_sulfur_dioxide&#39;, &#39;total_sulfur_dioxide&#39;, &#39;sulphates&#39;, &#39;alcohol&#39;]for each_col in cols_need_tweaks: tweaked_data[each_col] = original_data[[each_col]].apply(trans_tweaks, axis=1)# Tweaks completed.Tweak resultdistribution after transform.fig_temp3, ax_temp3 = plt.subplots(4, 3, figsize=(32, 24))rolling_index = 0columns_temp3 = list(tweaked_data.columns)columns_temp3.remove(&#39;quality&#39;)for fig_row in range(4): for fig_col in range(3): sns.distplot(tweaked_data[columns_temp3[rolling_index]], ax=ax_temp3[fig_row][fig_col]) rolling_index += 1 if (rolling_index &amp;gt;= len(columns_temp3)): breakData LearningOnly with np.log transformUse sklearn build-in DecisionTree to make a approach.# use sklearn.model_selection.train_test_split to split train and test data.X_temp1 = tweaked_data.drop([&#39;quality&#39;], axis=1)Y_temp1 = tweaked_data[&#39;quality&#39;]x_train, x_test, y_train, y_test = train_test_split(X_temp1, Y_temp1, random_state=99)print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)print(&#39;which is {0} rows in training set, consisting of {1} features.&#39;.format(x_train.shape[0], x_train.shape[1]))print(&#39;Corespondingly, {} rows are included in testing set.&#39;.format(x_test.shape[0]))(1019, 11) (340, 11) (1019,) (340,)which is 1019 rows in training set, consisting of 11 features.Corespondingly, 340 rows are included in testing set.# ID3 is entropy based DecisionTree.entropy_decision_tree = DecisionTreeClassifier(criterion=&#39;entropy&#39;, max_depth=6)# CART is gini based DecisionTree.gini_decision_tree = DecisionTreeClassifier(criterion=&#39;gini&#39;, max_depth=6)trees_apoch1 = [entropy_decision_tree, gini_decision_tree]# Fit/Trainfor tree in trees_apoch1: tree.fit(x_train, y_train)# Predictentropy_predition = entropy_decision_tree.predict(x_test)gini_predition = gini_decision_tree.predict(x_test)# scoreentropy_score = accuracy_score(y_test, entropy_predition)gini_score = accuracy_score(y_test, gini_predition)# print the damn resultprint(&#39;ID3 - Entropy Decision Tree (prediction score) ==&amp;gt; {}%&#39;.format(np.round(entropy_score*100, decimals=3)))print(&#39;CART - Gini Decision Tree (prediction score) ==&amp;gt; {}%&#39;.format(np.round(gini_score*100, decimals=3)))ID3 - Entropy Decision Tree (prediction score) ==&amp;gt; 58.235%CART - Gini Decision Tree (prediction score) ==&amp;gt; 54.118%print(&#39; ID3 - Entropy Decision Tree \\n Confusion Matrix 或 判断矩阵 或 真假阴阳性矩阵 ==&amp;gt;\\n{}&#39; .format(confusion_matrix(y_test, entropy_predition)))print(&#39; Classification Report 或 分类预测详细 ==&amp;gt;\\n&#39;)print(classification_report(y_test, entropy_predition))print(&#39;*&#39;*50 + &#39;\\n&#39;)print(&#39; CART - Gini Decision Tree \\n Confusion Matrix 或 判断矩阵 或 真假阴阳性矩阵 ==&amp;gt; \\n{}&#39; .format(confusion_matrix(y_test, gini_predition)))print(&#39; Classification Report 或 分类预测详细 ==&amp;gt;\\n&#39;)print(classification_report(y_test, gini_predition)) ID3 - Entropy Decision Tree Confusion Matrix 或 判断矩阵 或 真假阴阳性矩阵 ==&amp;gt;[[ 0 1 2 0 0 0] [ 0 3 8 0 0 0] [ 0 0 109 28 2 0] [ 0 3 56 67 7 0] [ 0 1 10 22 19 0] [ 0 0 0 1 1 0]] Classification Report 或 分类预测详细 ==&amp;gt; precision recall f1-score support 3 0.00 0.00 0.00 3 4 0.38 0.27 0.32 11 5 0.59 0.78 0.67 139 6 0.57 0.50 0.53 133 7 0.66 0.37 0.47 52 8 0.00 0.00 0.00 2 accuracy 0.58 340 macro avg 0.36 0.32 0.33 340weighted avg 0.58 0.58 0.57 340************************************************** CART - Gini Decision Tree Confusion Matrix 或 判断矩阵 或 真假阴阳性矩阵 ==&amp;gt; [[ 0 0 3 0 0 0] [ 1 0 7 3 0 0] [ 0 0 94 42 3 0] [ 0 0 48 74 10 1] [ 0 0 8 28 16 0] [ 0 0 0 1 1 0]] Classification Report 或 分类预测详细 ==&amp;gt; precision recall f1-score support 3 0.00 0.00 0.00 3 4 0.00 0.00 0.00 11 5 0.59 0.68 0.63 139 6 0.50 0.56 0.53 133 7 0.53 0.31 0.39 52 8 0.00 0.00 0.00 2 accuracy 0.54 340 macro avg 0.27 0.26 0.26 340weighted avg 0.52 0.54 0.52 340Train after grouped qualityIt’s worth noticing that this prediction outcome falls into 6 zones.Including quality in [3, 4, 5, 6, 7, 8].Perhaps a little grouping may help.DEFINE {quality == 3 or quality == 4} AS 1 (LOW quality)DEFINE {quality == 5 or quality == 6} AS 2 (MID quality)DEFINE {quality == 7 or quality == 8} AS 3 (HIGH quality) Note that data prediction in this sector will only falls into [1, 2, 3] =&amp;gt; [(3, 4), (5, 6), (7, 8)].def rate_transform(col): if col[0] &amp;lt; 5: return 1 elif col[0] &amp;lt; 7: return 2 return 3tweaked_data[&#39;rate&#39;] = tweaked_data[[&#39;quality&#39;]].apply(rate_transform, axis=1)rate_plot_data = tweaked_data.groupby([&#39;rate&#39;]).size().reset_index()rate_plot_data.columns = [&#39;rate&#39;, &#39;count&#39;]rate_plot_data.plot(kind=&#39;bar&#39;, x=&#39;rate&#39;, y=&#39;count&#39;, rot=0)This time, prediction will falls into 3 zones. As define above, RATE =&amp;gt; [1, 2, 3]Try re-fit.This time, drop quality and rate. rate will be y.X_temp2 = tweaked_data.drop([&#39;quality&#39;, &#39;rate&#39;], axis=1)Y_temp2 = tweaked_data[&#39;rate&#39;]x_train, x_test, y_train, y_test = train_test_split(X_temp2, Y_temp2, random_state=99)print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)print(&#39;which is {0} rows in training set, consisting of {1} features.&#39;.format(x_train.shape[0], x_train.shape[1]))print(&#39;Corespondingly, {} rows are included in testing set.\\n&#39;.format(x_test.shape[0]))entropy_decision_tree = DecisionTreeClassifier(criterion=&#39;entropy&#39;, max_depth=5)gini_decision_tree = DecisionTreeClassifier(criterion=&#39;gini&#39;, max_depth=5)trees_apoch2 = [entropy_decision_tree, gini_decision_tree]for tree in trees_apoch2: tree.fit(x_train, y_train)# Predictentropy_predition = entropy_decision_tree.predict(x_test)gini_predition = gini_decision_tree.predict(x_test)# scoreentropy_score = accuracy_score(y_test, entropy_predition)gini_score = accuracy_score(y_test, gini_predition)# print the result again.print(&#39;ID3 - Entropy Decision Tree (prediction score) ==&amp;gt; {}%&#39;.format(np.round(entropy_score*100, decimals=3)))print(&#39;CART - Gini Decision Tree (prediction score) ==&amp;gt; {}%&#39;.format(np.round(gini_score*100, decimals=3)))(1019, 11) (340, 11) (1019,) (340,)which is 1019 rows in training set, consisting of 11 features.Corespondingly, 340 rows are included in testing set.ID3 - Entropy Decision Tree (prediction score) ==&amp;gt; 82.059%CART - Gini Decision Tree (prediction score) ==&amp;gt; 78.824%print(&#39; ID3 - Entropy Decision Tree \\n Confusion Matrix 或 判断矩阵 或 真假阴阳性矩阵 ==&amp;gt;\\n{}&#39; .format(confusion_matrix(y_test, entropy_predition)))print(&#39; Classification Report 或 分类预测详细 ==&amp;gt;\\n&#39;)print(classification_report(y_test, entropy_predition))print(&#39;*&#39;*50 + &#39;\\n&#39;)print(&#39; CART - Gini Decision Tree \\n Confusion Matrix 或 判断矩阵 或 真假阴阳性矩阵 ==&amp;gt; \\n{}&#39; .format(confusion_matrix(y_test, gini_predition)))print(&#39; Classification Report 或 分类预测详细 ==&amp;gt;\\n&#39;)print(classification_report(y_test, gini_predition)) ID3 - Entropy Decision Tree Confusion Matrix 或 判断矩阵 或 真假阴阳性矩阵 ==&amp;gt;[[ 0 14 0] [ 1 260 11] [ 0 35 19]] Classification Report 或 分类预测详细 ==&amp;gt; precision recall f1-score support 1 0.00 0.00 0.00 14 2 0.84 0.96 0.90 272 3 0.63 0.35 0.45 54 accuracy 0.82 340 macro avg 0.49 0.44 0.45 340weighted avg 0.77 0.82 0.79 340************************************************** CART - Gini Decision Tree Confusion Matrix 或 判断矩阵 或 真假阴阳性矩阵 ==&amp;gt; [[ 2 12 0] [ 3 250 19] [ 0 38 16]] Classification Report 或 分类预测详细 ==&amp;gt; precision recall f1-score support 1 0.40 0.14 0.21 14 2 0.83 0.92 0.87 272 3 0.46 0.30 0.36 54 accuracy 0.79 340 macro avg 0.56 0.45 0.48 340weighted avg 0.76 0.79 0.77 340Learning on tweaked dataHow to resampleY-data seems a little bit inbalanced?Okay then, add some resample might work.Going back to quality column, which contains 6 different values.Hence drop the rate column we added before.# Drop the rate data we added before.tweaked_data.drop([&#39;rate&#39;], inplace=True, axis=1)df_3 = tweaked_data[tweaked_data[&#39;quality&#39;] == 3]df_4 = tweaked_data[tweaked_data[&#39;quality&#39;] == 4]df_5 = tweaked_data[tweaked_data[&#39;quality&#39;] == 5]df_6 = tweaked_data[tweaked_data[&#39;quality&#39;] == 6]df_7 = tweaked_data[tweaked_data[&#39;quality&#39;] == 7]df_8 = tweaked_data[tweaked_data[&#39;quality&#39;] == 8]i = 3for each_df in [df_3, df_4, df_5, df_6, df_7, df_8]: print(&#39;quality == {0} has {1} entries&#39;.format(i, each_df.shape[0])) i += 1quality == 3 has 10 entriesquality == 4 has 53 entriesquality == 5 has 577 entriesquality == 6 has 535 entriesquality == 7 has 167 entriesquality == 8 has 17 entriesIt’s quite obvious that 3, 4, 7, 8 are minorities. 5, 6 are majorities.Hence we up-sample 3, 4, 6, 7, 8 to 550 entries.down-sample 5 to 550 entriesdf_3_upsampled = resample(df_3, replace=True, n_samples=550, random_state=9) df_4_upsampled = resample(df_4, replace=True, n_samples=550, random_state=9) df_7_upsampled = resample(df_7, replace=True, n_samples=550, random_state=9) df_8_upsampled = resample(df_8, replace=True, n_samples=550, random_state=9) df_5_downsampled = df_5.sample(n=550).reset_index(drop=True)df_6_upsampled = resample(df_6, replace=True, n_samples=550, random_state=9) After Re-sample(upsample &amp;amp; downsample). Value count looks like this.resampled_dfs = [df_3_upsampled, df_4_upsampled, df_5_downsampled, df_6_upsampled, df_7_upsampled, df_8_upsampled]resampled_data = pd.concat(resampled_dfs, axis=0)i = 3for each_df in resampled_dfs: print(&#39;quality == {0} has {1} entries&#39;.format(i, each_df.shape[0])) i += 1quality == 3 has 550 entriesquality == 4 has 550 entriesquality == 5 has 550 entriesquality == 6 has 550 entriesquality == 7 has 550 entriesquality == 8 has 550 entriesGet rid of some “unrelated” dataBefore data is sent to training. Minor tweak is advised here. TODO: pre-train data process. MijazzChan@20210511-010817TODO-FINISH: MijazzChan@20210511-032652Try drop some unrelated columns/features?resampled_data.corr()[&#39;quality&#39;]fixed_acidity 0.125105volatile_acidity -0.600034citric_acid 0.384477residual_sugar 0.009648chlorides -0.377277free_sulfur_dioxide 0.081144total_sulfur_dioxide 0.057946density -0.321477pH -0.267204sulphates 0.490420alcohol 0.597137quality 1.000000Name: quality, dtype: float64little note here: corr() values is 皮尔逊积矩相关系数 或correlation coefficient. It stays within [-1, 1].abs(corr()) 越逼近1, 即相关度越高.取abs后, 再看这些因素与quality的相关性.After mapping with abs, we can review how much these columns are co-related with quality.def map_abs(col): return np.abs(col[0])corr_df = resampled_data.corr()[&#39;quality&#39;].reset_index()corr_df.columns = [&#39;factors&#39;, &#39;correlation&#39;]# quality is always related to quality, which means quality.corr == 1# Hence the drop.corr_df = corr_df[corr_df[&#39;factors&#39;] != &#39;quality&#39;]# Mapping with abs functioncorr_df[&#39;correlation(abs)&#39;] = corr_df[[&#39;correlation&#39;]].apply(map_abs, axis=1)corr_df.sort_values(by=&#39;correlation(abs)&#39;, ascending=False, inplace=True)corr_df.drop([&#39;correlation&#39;], inplace=True, axis=1)print(&#39;TOP 5 quality-related factors\\n&#39;, corr_df.head(5))sns.barplot(x=&#39;correlation(abs)&#39;, y=&#39;factors&#39;, data=corr_df, orient=&#39;h&#39;, palette=&#39;flare&#39;)TOP 5 quality-related factors factors correlation(abs)1 volatile_acidity 0.60003410 alcohol 0.5971379 sulphates 0.4904202 citric_acid 0.3844774 chlorides 0.377277Note that only following factors is worth putting into training. They are volatile acidity alcohol sulphates citric acid chlorides density pH free sulfur dioxide fixed acidity total sulfur dioxideDropping factors as follows: residual sugarfactors_to_drop = [&#39;residual_sugar&#39;]simplified_data = resampled_data[resampled_data.columns[~resampled_data.columns.isin(factors_to_drop)]]simplified_data.head(6) fixed_acidity volatile_acidity citric_acid chlorides free_sulfur_dioxide total_sulfur_dioxide density pH sulphates alcohol quality 1106 7.6 1.580 0.00 -1.987774 1.609438 2.197225 0.99476 3.50 -0.916291 2.388763 3 1165 6.8 0.815 0.00 -1.320507 2.772589 3.367296 0.99471 3.32 -0.673345 2.282382 3 1253 7.1 0.875 0.05 -2.501036 1.098612 2.639057 0.99808 3.40 -0.653926 2.322388 3 1165 6.8 0.815 0.00 -1.320507 2.772589 3.367296 0.99471 3.32 -0.673345 2.282382 3 450 10.4 0.610 0.49 -1.609438 1.609438 2.772589 0.99940 3.16 -0.462035 2.128232 3 1165 6.8 0.815 0.00 -1.320507 2.772589 3.367296 0.99471 3.32 -0.673345 2.282382 3 Put the simplified df into training. See how it performs.X_temp3 = simplified_data.drop([&#39;quality&#39;], axis=1)Y_temp3 = simplified_data[&#39;quality&#39;]x_train, x_test, y_train, y_test = train_test_split(X_temp3, Y_temp3, random_state=99, test_size=0.3)# Note that you don&#39;t use resampled data to test!!!!!!# Drop duplicated row you added when resample.# test_df = pd.concat([x_test, y_test], axis=1)# print(&#39;test data containing duplicated entries(added during resample) =&amp;gt; {}, drop them.&#39;.format(test_df.duplicated().sum()))# test_df.drop_duplicates(inplace=True)# x_test = test_df.drop([&#39;quality&#39;], axis=1)# y_test = test_df[&#39;quality&#39;]print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)print(&#39;which is {0} rows in training set, consisting of {1} features.&#39;.format(x_train.shape[0], x_train.shape[1]))print(&#39;Corespondingly, {} rows are included in testing set.\\n&#39;.format(x_test.shape[0]))entropy_decision_tree = DecisionTreeClassifier(criterion=&#39;entropy&#39;, splitter=&#39;random&#39;)gini_decision_tree = DecisionTreeClassifier(criterion=&#39;gini&#39;, splitter=&#39;random&#39;)trees_apoch3 = [entropy_decision_tree, gini_decision_tree]for tree in trees_apoch3: tree.fit(x_train, y_train)# Predictentropy_predition = entropy_decision_tree.predict(x_test)gini_predition = gini_decision_tree.predict(x_test)# scoreentropy_score = accuracy_score(y_test, entropy_predition)gini_score = accuracy_score(y_test, gini_predition)# print the result.print(&#39;ID3 - Entropy Decision Tree (prediction score) ==&amp;gt; {}%&#39;.format(np.round(entropy_score*100, decimals=3)))print(&#39;CART - Gini Decision Tree (prediction score) ==&amp;gt; {}%&#39;.format(np.round(gini_score*100, decimals=3)))(2310, 10) (990, 10) (2310,) (990,)which is 2310 rows in training set, consisting of 10 features.Corespondingly, 990 rows are included in testing set.ID3 - Entropy Decision Tree (prediction score) ==&amp;gt; 89.293%CART - Gini Decision Tree (prediction score) ==&amp;gt; 89.697%print(&#39; ID3 - Entropy Decision Tree \\n Confusion Matrix 或 判断矩阵 或 真假阴阳性矩阵 ==&amp;gt;\\n{}&#39; .format(confusion_matrix(y_test, entropy_predition)))print(&#39; Classification Report 或 分类预测详细 ==&amp;gt;\\n&#39;)print(classification_report(y_test, entropy_predition))print(&#39;*&#39;*50 + &#39;\\n&#39;)print(&#39; CART - Gini Decision Tree \\n Confusion Matrix 或 判断矩阵 或 真假阴阳性矩阵 ==&amp;gt; \\n{}&#39; .format(confusion_matrix(y_test, gini_predition)))print(&#39; Classification Report 或 分类预测详细 ==&amp;gt;\\n&#39;)print(classification_report(y_test, gini_predition)) ID3 - Entropy Decision Tree Confusion Matrix 或 判断矩阵 或 真假阴阳性矩阵 ==&amp;gt;[[180 0 0 0 0 0] [ 0 172 0 0 0 0] [ 0 10 88 32 9 0] [ 0 5 31 119 9 2] [ 0 0 3 3 149 2] [ 0 0 0 0 0 176]] Classification Report 或 分类预测详细 ==&amp;gt; precision recall f1-score support 3 1.00 1.00 1.00 180 4 0.92 1.00 0.96 172 5 0.72 0.63 0.67 139 6 0.77 0.72 0.74 166 7 0.89 0.95 0.92 157 8 0.98 1.00 0.99 176 accuracy 0.89 990 macro avg 0.88 0.88 0.88 990weighted avg 0.89 0.89 0.89 990************************************************** CART - Gini Decision Tree Confusion Matrix 或 判断矩阵 或 真假阴阳性矩阵 ==&amp;gt; [[180 0 0 0 0 0] [ 0 172 0 0 0 0] [ 3 10 84 31 9 2] [ 0 4 22 125 11 4] [ 0 0 0 4 151 2] [ 0 0 0 0 0 176]] Classification Report 或 分类预测详细 ==&amp;gt; precision recall f1-score support 3 0.98 1.00 0.99 180 4 0.92 1.00 0.96 172 5 0.79 0.60 0.69 139 6 0.78 0.75 0.77 166 7 0.88 0.96 0.92 157 8 0.96 1.00 0.98 176 accuracy 0.90 990 macro avg 0.89 0.89 0.88 990weighted avg 0.89 0.90 0.89 990The outcome is quite satisfying given that the predition this time falls into 6 zones.这次预测的结果已经比较让人满意了, 因为预测值并非像先前加入rate一样, 只预测3个结果(LOW MID HIGH).本次的预测结果是直接落入准确的[3,4,5,6,7,8]里的.make a simple text visualization on the DecisionTreeprint(export_text(entropy_decision_tree, feature_names=x_train.columns.to_list()))Content too long to display.# Goto Gist for original file: # https://gist.github.com/MijazzChan/00f39ed1f17e6026ab3b1be94fc8e636" }, { "title": "Store Multiple Booleans Using Single Variable", "url": "/posts/Store-Multiple-Booleans-Using-Byte/", "categories": "Java, Design", "tags": "java, tricks, code", "date": "2021-04-18 17:14:49 +0800", "snippet": "IntroInspired by a stackoverflow answer which is more than a decade old. StackOverflow-Answer1, StackOverflow - Answer2@Jon Skeet there provide a way of measuring how much memory do byte, int and boolean variables takes.To simply clarify my point here, I modified some of his code in order to reproduce the situation. He pointed out that the actual size of those variables was VM-dependent, and his VM was Sun&#39;s JDK build 1.6.0_11. My testcase VM is AdoptOpenJDK (build 11.0.10+9) - hotspot VM.Also, in this Oracle Java Documentation about Primitive Data Types. boolean: The boolean data type has only two possible values: true and false. Use this data type for simple flags that track true/false conditions. This data type represents one bit of information, but its “size” isn’t something that’s precisely defined.Reproduce Code class LotsOfBytes { byte a0, a1, a2, a3, a4, a5, a6, a7, a8, a9, aa, ab, ac, ad, ae, af; byte b0, b1, b2, b3, b4, b5, b6, b7, b8, b9, ba, bb, bc, bd, be, bf; byte c0, c1, c2, c3, c4, c5, c6, c7, c8, c9, ca, cb, cc, cd, ce, cf; byte d0, d1, d2, d3, d4, d5, d6, d7, d8, d9, da, db, dc, dd, de, df; byte e0, e1, e2, e3, e4, e5, e6, e7, e8, e9, ea, eb, ec, ed, ee, ef; } class LotsOfInts { int a0, a1, a2, a3, a4, a5, a6, a7, a8, a9, aa, ab, ac, ad, ae, af; int b0, b1, b2, b3, b4, b5, b6, b7, b8, b9, ba, bb, bc, bd, be, bf; int c0, c1, c2, c3, c4, c5, c6, c7, c8, c9, ca, cb, cc, cd, ce, cf; int d0, d1, d2, d3, d4, d5, d6, d7, d8, d9, da, db, dc, dd, de, df; int e0, e1, e2, e3, e4, e5, e6, e7, e8, e9, ea, eb, ec, ed, ee, ef; } class LotsOfBooleans { boolean a0, a1, a2, a3, a4, a5, a6, a7, a8, a9, aa, ab, ac, ad, ae, af; boolean b0, b1, b2, b3, b4, b5, b6, b7, b8, b9, ba, bb, bc, bd, be, bf; boolean c0, c1, c2, c3, c4, c5, c6, c7, c8, c9, ca, cb, cc, cd, ce, cf; boolean d0, d1, d2, d3, d4, d5, d6, d7, d8, d9, da, db, dc, dd, de, df; boolean e0, e1, e2, e3, e4, e5, e6, e7, e8, e9, ea, eb, ec, ed, ee, ef; }each of LotsOfXXX has 80 variables.private static final int SIZE = 1000000; @Test void test() { LotsOfBytes[] first = new LotsOfBytes[SIZE]; LotsOfInts[] second = new LotsOfInts[SIZE]; LotsOfBooleans[] third = new LotsOfBooleans[SIZE]; System.gc(); long startMem = getMemory(); for (int i = 0; i &amp;lt; SIZE; i++) { first[i] = new LotsOfBytes(); } System.gc(); long endMem = getMemory(); System.out.println(&quot;Size for LotsOfBytes: &quot; + (endMem - startMem)); System.out.println(&quot;Average size: &quot; + ((endMem - startMem) / ((double) SIZE))); System.out.println(&quot;Appro. size for each Byte: &quot; + ((endMem - startMem) / ((double) SIZE)) / 80); System.out.println(); System.gc(); startMem = getMemory(); for (int i = 0; i &amp;lt; SIZE; i++) { second[i] = new LotsOfInts(); } System.gc(); endMem = getMemory(); System.out.println(&quot;Size for LotsOfInts: &quot; + (endMem - startMem)); System.out.println(&quot;Average size: &quot; + ((endMem - startMem) / ((double) SIZE))); System.out.println(&quot;Appro. size for each Int: &quot; + ((endMem - startMem) / ((double) SIZE)) / 80); System.out.println(); System.gc(); startMem = getMemory(); for (int i = 0; i &amp;lt; SIZE; i++) { third[i] = new LotsOfBooleans(); } System.gc(); endMem = getMemory(); System.out.println(&quot;Size for LotsOfBooleans: &quot; + (endMem - startMem)); System.out.println(&quot;Average size: &quot; + ((endMem - startMem) / ((double) SIZE))); System.out.println(&quot;Appro. size for each Boolean: &quot; + ((endMem - startMem) / ((double) SIZE)) / 80); System.out.println(); // Make sure nothing gets collected long total = 0; for (int i = 0; i &amp;lt; SIZE; i++) { total += first[i].a0 + second[i].a0 + ((!third[i].a0) ? 0 : 1); } System.out.println(total); } private static long getMemory() { Runtime runtime = Runtime.getRuntime(); return runtime.totalMemory() - runtime.freeMemory(); }ResultSize for LotsOfBytes: 95671792Average size: 95.671792Appro. size for each Byte: 1.1958974Size for LotsOfInts: 336440976Average size: 336.440976Appro. size for each Int: 4.205512199999999Size for LotsOfBooleans: 95999768Average size: 95.999768Appro. size for each Boolean: 1.1999971Note that Appro. size for each XXX is not precisely the actual size of each XXX. There was memory reserved for class/obj definition or other kind of stuff.As it generates, byte and boolean , each of them, takes 1 Byte memory. int takes 4 Byte each.What if you can store 8 boolean inside a single byte. In that way, 1 boolean takes nearly 1 bit.MethodHow this works?Suppose that you need to store a boolean array just likeboolean[] someFlags = new boolean[8];you will have a array of 8 boolean, all in false state, which is[false, false, false, false, false, false, false, false]Given the situation discussed above, 8 Byte here taken.What if you can using a byte ‘s bit to represent them.127(dec) -&amp;gt; 0111 1111 (binary) 0(dec) -&amp;gt; 0000 0000 (binary)Each bit holds a positional boolean status inside one Byte.Here is an example:[false, false, true, false, false, false, true, true] boolean[] &amp;lt;=&amp;gt; binary[ 0 , 0 , 1 , 0 , 0 , 0 , 1 , 1 ] binary &amp;lt;=&amp;gt; dec35(dec)Set positional boolean flag to true byte flags = 0;flags |= 1 &amp;lt;&amp;lt; position;// which is equivalent to // flag = flag | 1 &amp;lt;&amp;lt; positionTake a deep look in how that works. Suppose we need to someFlags[2] = true flags = 0000 0000 | (1 &amp;lt;&amp;lt; 2);flags = 0000 0000 | 0000 0100;flags = 0000 0100; ``` ThensomeFlags[4] = true: flags = 0000 0100 | (1 &amp;lt;&amp;lt; 4);flags = 0000 0100 | 0001 0000;flags = 0001 0100; as long as the position arg not exceeding 7, flags will be well preserved in this variable. Set positional boolean flag to false flags will remain untouched.(0001 0100) Additional info: ~operator is Complement Operator. Flipping bits in binary will do the work.flags &amp;amp;= ~(1 &amp;lt;&amp;lt; position);// which is equivalent to // flag = flag &amp;amp; ~(1 &amp;lt;&amp;lt; position) Suppose we need to toggle someFlags[2] = false flags = 0001 0100 &amp;amp; ~(1 &amp;lt;&amp;lt; 2);flags = 0001 0100 &amp;amp; ~(0000 0100);// &quot;~&quot; operator =&amp;gt; flip bitsflags = 0001 0100 &amp;amp; 1111 1011;flags = 0001 0000; Now the someFlags[2] is set to false; Retrieve flag flags will remain untouched.(0001 0000)return (flags &amp;amp; (1 &amp;lt;&amp;lt; position)) == (1 &amp;lt;&amp;lt; position); Suppose we need to check whether someFlags[4] == true return (0001 0000 &amp;amp; (1 &amp;lt;&amp;lt; 4)) == (1 &amp;lt;&amp;lt; 4);return (0001 0000 &amp;amp; 0001 0000) == 0001 0000;return 0001 0000 == 0001 0000;return true; Furthermoreint, which consists of 4 Byte(32 bit), is totally viable theoretically, and it can hold 32 boolean variables, but consumes memory just as 4 boolean do.In modern computer we may not need this kind of twisted way to saving memory. Just consider it a way of leetcode technique. However, working on some embedded devices with little tiny memory size available, you may find it useful.Keyword Leetcode save memory memory saving tweaks multiple booleans in 1 variable" }, { "title": "Some Utility Functions While Leetcoding", "url": "/posts/Utility-Functions-While-Leetcoding/", "categories": "Java, Code", "tags": "java, leetcode, tools, code", "date": "2021-04-13 14:52:54 +0800", "snippet": "Array Generator Secure.Random / ThreadLocalRandompublic static final ThreadLocalRandom randStream = ThreadLocalRandom.current(); public static int randomInt(int from, int to) { return randStream.nextInt(Math.min(from, to), Math.max(from, to)); } public static double randomDouble(double from, double to) { return randStream.nextDouble(Math.min(from, to), Math.max(from, to)); } public static int[] randomIntArray(int size, int from, int to) { return randStream.ints(size, from, to).toArray(); } public static int[] randomIntArray(int size){ return randStream.ints(size, 0, 10_000).toArray(); } public static double[] randomDoubleArray(int size, double from, double to) { return randStream.doubles(size, from, to).toArray(); } public static double[] randomDoubleArray(int size){ return randStream.doubles(size, 0, 100).toArray(); }ListNode with toString() method implement toString() method is really essential and useful during unit/problem testing/testcase. Build ListNode from an array: The combination of buildListNodeFrom() method and randomIntArray() mentioned above can save a shit-load of time while building ListNode with plain code.public class ListNode { public int val; public ListNode next; // Constructor public ListNode() { } public ListNode(int val) { this.val = val; } public ListNode(int val, ListNode next) { this.val = val; this.next = next; } // ListNode toString method // ListNode Print Method @Override public String toString() { StringBuilder stringBuilder = new StringBuilder(); stringBuilder.append(this.val).append(&quot;=&amp;gt;&quot;); while (this.next != null) { stringBuilder.append(this.next.val).append(&quot;=&amp;gt;&quot;); this.next = this.next.next; } return stringBuilder.append(&quot;NULL&quot;).toString(); } // Build ListNode from an array public static ListNode buildListNodeFrom(int[] values) { ListNode head = new ListNode(0, new ListNode()); ListNode listNode = head.next; if (null == values || values.length == 0) { throw new IllegalArgumentException(&quot;Values[] cannot be null&quot;); } for (int i = 1; i &amp;lt; values.length; i++) { listNode.val = values[i - 1]; listNode.next = new ListNode(values[i]); listNode = listNode.next; } return head.next; }}To be updated." }, { "title": "Dockerize My Network", "url": "/posts/Dockerize-My-Network/", "categories": "Linux, Proxy", "tags": "linux, docker, networking", "date": "2021-03-29 16:51:49 +0800", "snippet": "Current network architectureVisual diagram Just a sketch.So-called safe LAN, in which there will be no unexpected public traffic inbound under normal circumstances. In local LAN, use socks5 and http for next_hop in v2ray. (2 ports needed) In openvpn nat LAN, use http with proxy automation script(pac) to proxy gfw network flow to v2ray .(1 port needed) Manual Proxy certain traffic to docker container unblocknetease.(1 port needed)Public network(In my case, i have a public ip addr and pre-configured ddns) Use vmess inbound with tls. (1 port needed) Inside v2ray, use routing to pass traffic to unblocknetease.(docker network port binding needed)In this situation, I have to utilize/listen/bind at lease 4 to 5 ports to get a fully functional “gateway”.ModificationsCombine native-runtime v2ray and docker container unblocknetease into one single docker container. Server as a “gateway” and Bind only 2 ports. Inbound 1: http Inbound 2: vmessAll traffic redirected to unblocknetease will be handled by v2ray’ s routing feature.DockerfileARG APLINE_VERSION=3FROM alpine:${APLINE_VERSION}LABEL maintainer=&quot;Mijazz_Chan&quot;LABEL version=&quot;1.0&quot;ARG V2RAY_VER=&quot;4.34.0&quot;ARG UNBLOCK_NETEASE_VER=&quot;0.25.3&quot;ARG SYSTEM_ARCH=&quot;linux-64&quot;# PRODUCTION ENVENV V2RAY_BINARY=&quot;https://github.com/v2fly/v2ray-core/releases/download/v&quot;${V2RAY_VER}&quot;/v2ray-&quot;${SYSTEM_ARCH}&quot;.zip&quot;ENV UNBLOCK_NETEASE_SRC=&quot;https://github.com/nondanee/UnblockNeteaseMusic/archive/refs/tags/v&quot;${UNBLOCK_NETEASE_VER}&quot;.zip&quot;ENV NODE_ENV=productionENV NPM_REG=&quot;https://registry.npmjs.org&quot;# DEVELOPMENT ENV#ENV V2RAY_BINARY=&quot;http://download.mijazz.xyz/v2ray-linux-64.zip&quot;#ENV UNBLOCK_NETEASE_SRC=&quot;http://download.mijazz.xyz/v0.25.3.zip&quot;#ENV NPM_REG=&quot;https://registry.npm.taobao.org&quot;# Due to known network issue, you may need to uncomment this line or change mirrors#RUN sed -i &#39;s/dl-cdn.alpinelinux.org/opentuna.cn/g&#39; /etc/apk/repositoriesRUN apk add --update nodejs npm supervisor &amp;amp;&amp;amp; \\ wget -O /tmp/v2ray.zip $V2RAY_BINARY &amp;amp;&amp;amp; \\ wget -O /tmp/v2ray.zip.dgst $V2RAY_BINARY.dgst &amp;amp;&amp;amp; \\ expect_md5=`awk &#39;/MD5/{print $2}&#39; /tmp/v2ray.zip.dgst` &amp;amp;&amp;amp; \\ current_md5=`md5sum /tmp/v2ray.zip | cut -d&#39; &#39; -f1` &amp;amp;&amp;amp; \\ if test &quot;$expect_md5&quot; != &quot;$current_md5&quot;; then echo &quot;Corrupt File&quot;; exit -2; fi &amp;amp;&amp;amp; \\ echo -e &quot;\\n=====V2ray Fetched=====\\n=====MD5 CHECKSUM MATCH=====\\nExpecting: $expect_md5\\nCurrent: $current_md5\\n&quot; &amp;amp;&amp;amp; \\ unzip -q -d /opt/v2ray /tmp/v2ray.zip &amp;amp;&amp;amp; \\ wget -O /tmp/unblock_netease.zip $UNBLOCK_NETEASE_SRC &amp;amp;&amp;amp; \\ unzip -q -d /opt/unblocknetease /tmp/unblock_netease.zip &amp;amp;&amp;amp; \\ mv $(find /opt/unblocknetease -mindepth 1 -maxdepth 2) /opt/unblocknetease/ &amp;amp;&amp;amp; \\ npm --registry $NPM_REG install --production /opt/unblocknetease &amp;amp;&amp;amp; \\ rm -rf /tmp/* /var/cache/apk/*COPY ./server.crt /opt/unblocknetease/server.crtCOPY ./server.key /opt/unblocknetease/server.keyVOLUME /confENTRYPOINT [&quot;supervisord&quot;,&quot;--nodaemon&quot;, &quot;--configuration&quot;, &quot;/conf/supervisord.conf&quot;] volume for /conf should contain a valid v2ray json config, supervisord config. Upstream project unblocknetease does not provide a configurable certificate path, you must have to follow this github issue to sign/generate your own certificates. Certificates are being COPY into docker image during docker build procedure.Files ~/Dev/otherproject/net-breach  tree . [Mon Mar 29,06:36:40 PM].├── Dockerfile├── server.crt└── server.key0 directories, 3 files ~/Dev/otherproject/netbreach-conf  tree . [Mon Mar 29,06:37:35 PM].├── supervisord.conf└── v2ray.conf0 directories, 2 filessupervisord.conf[supervisord]nodaemon=truelogfile=/dev/stdoutlogfile_maxbytes=0loglevel=info[supervisorctl]serverurl=unix:///tmp/supervisor.sock[program:v2ray]priority=2command=/opt/v2ray/v2ray -c /conf/v2ray.confstdout_logfile=/dev/stdoutstdout_logfile_maxbytes=0autorestart=unexpectedstartsecs=10exitcodes=23startretries=3[program:unblocknetease]priority=1directory=/opt/unblockneteasecommand=node app.js -p 50000:50001 -o kuwo qq joox kugou -e https://music.163.comstdout_logfile=/dev/stdoutstdout_logfile_maxbytes=0startsecs=10startretries=3v2ray.conf project related config parts#OUTBOUND{ &quot;tag&quot;: &quot;netease&quot;, &quot;protocol&quot;: &quot;http&quot;, &quot;settings&quot;:{ &quot;servers&quot;: [ { &quot;address&quot;: &quot;127.0.0.1&quot;, &quot;port&quot;: 50000 } ] }, &quot;streamSettings&quot;: null, &quot;mux&quot;: null}# ROUTING{ &quot;type&quot;: &quot;field&quot;, &quot;outboundTag&quot;: &quot;netease&quot;, &quot;domain&quot;: [ &quot;music.163.com&quot;, &quot;music.126.net&quot; ]},Clisudo docker build -t net-breach:0.1 $NET_BREACH_PATHsudo docker run $ASSIGN_RUNTIME(rm/it/restart policy) -p $PORT_BINDING -v /home/mijazz/Dev/otherproject/netbreach-conf:/conf net-breach:0.1 Port 50002 for http inbound. Port 50003 for vmess inbound.No extra port needed. Traffic routing is done by v2ray inside the container.2021-03-29 18:55:38,043 INFO success: unblocknetease entered RUNNING state, process has stayed up for &amp;gt; than 10 seconds (startsecs)2021-03-29 18:55:38,044 INFO success: v2ray entered RUNNING state, process has stayed up for &amp;gt; than 10 seconds (startsecs)Few extra notes Container timezone inside alpine image is not set, or not correct. Normally this will not cause any issue, but since vmess protocol is being used, it is recommended that you should bind/configure a read-only host timezone file to container. Proxy next hop can be reached via inbound traffic, unblocknetease is also fully functional during test. The way I handle params is not recommended with docker build procedure. (Whatever…It works…) Supervisord conf probably needs a little tweaking, combination with http server and DOCKER HEALTHCHECK is more elegant. (Although this primitive way works…….In case of network issues, just restart docker container via docker restart policy)Result Redirect ALL traffic by vmess protocol via public network inbound. iPhone - iOS 13, (jailbroken-not related), with certificate trust.References v2fly/v2ray-core nondanee/UnblockNeteaseMusic Docker DocsDisclaimer This post or project does NOT endorse, claim or imply that you could use this as a tool of bypassing government network censorship or anonymizing your network activities. If any unit or individual that the project may be suspected of violation of their rights, you should promptly notify and provide proof of identity and proof of ownership, I will remove the relevant content after receiving certification documents." }, { "title": "SpringBoot Scala Spark Co-Develop Dependency Problem", "url": "/posts/SpringBoot-Scala-Spark-Dependency-Problem/", "categories": "Java, Tracing", "tags": "trace, error, scala, spark", "date": "2020-12-22 14:33:12 +0800", "snippet": "Key Wordjava.lang.NoClassDefFoundError: org/codehaus/janino/InternalCompilerExceptionProblemDescriptionVersions Overview Java - OpenJDK8 Spark - 2.4.7 Spring Boot - 2.4.1 (Dependency Handled By pom.xml gen by start.spring.io) Scala - 2.11.12 Spark is on Hadoop YARN (Not really related)Problem occurred when I tried to call scala module in Spring Boot Project to execute spark related stuff. In my case, I invoke SparkSession to read csv from my local-storage.Main Exception Seems to be:java.lang.NoClassDefFoundError: org/codehaus/janino/InternalCompilerExceptionTracingFound in Maven project-wise pom.xml&amp;lt;!-- Spark Dependencies Start Here --&amp;gt; &amp;lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core --&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spark-core_2.11&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;2.4.7&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-sql --&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spark-sql_2.11&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;2.4.7&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt;Artifact: spark-sql bring the wrong version(higher version) of janino and commons-compiler, which seems to be not compatible with spark 2.4.7 &amp;amp; scala 2.11.12.SolutionAlter pom.xml, exclude the wrong version brought by spark-sql artifact. Append version-wise pom of janino and common-compiler.Version 3.0.8 works. If you are using the same version of spark &amp;amp; scala.Attempt to change versions to any- lower than 3.0.10 if you have different stack of versions.&amp;lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-sql --&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spark-sql_2.11&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;2.4.7&amp;lt;/version&amp;gt; &amp;lt;exclusions&amp;gt; &amp;lt;exclusion&amp;gt; &amp;lt;artifactId&amp;gt;janino&amp;lt;/artifactId&amp;gt; &amp;lt;groupId&amp;gt;org.codehaus.janino&amp;lt;/groupId&amp;gt; &amp;lt;/exclusion&amp;gt; &amp;lt;exclusion&amp;gt; &amp;lt;artifactId&amp;gt;commons-compiler&amp;lt;/artifactId&amp;gt; &amp;lt;groupId&amp;gt;org.codehaus.janino&amp;lt;/groupId&amp;gt; &amp;lt;/exclusion&amp;gt; &amp;lt;/exclusions&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;artifactId&amp;gt;janino&amp;lt;/artifactId&amp;gt; &amp;lt;groupId&amp;gt;org.codehaus.janino&amp;lt;/groupId&amp;gt; &amp;lt;version&amp;gt;3.0.8&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;artifactId&amp;gt;commons-compiler&amp;lt;/artifactId&amp;gt; &amp;lt;groupId&amp;gt;org.codehaus.janino&amp;lt;/groupId&amp;gt; &amp;lt;version&amp;gt;3.0.8&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;!-- Spark Dependencies End Here --&amp;gt;mvn-repo link : Maven Repository" }, { "title": "Try Understanding Lombok", "url": "/posts/Try-Understanding-Lombok/", "categories": "Java, Design", "tags": "java, lombok, ast, reflection", "date": "2020-12-09 16:49:56 +0800", "snippet": "Try Understanding LombokWhat is Lombok Project Lombok MVN Repo大概一年多以前我接触Sping Boot的设计模式时, 了解到Entity, Service, Repository, 等层次设计的时候, POJO什么的.当时的项目用的是Spring Data JPA做的持久层, 中间经过几层的数据, 对象传递. 同时也为了方便调试, 产生了很多无用的field.getter(), field.setter(), Object.toString(), Object.Constructer函数或字段.Lombok简单来说就是使用注释引入或者说注入所需的生成的字节码.但是如果知道Annotation的运行原理的话, 也比较难理解其实现方式. 因为Annotation说白了也只是一个接口. 其下几个相关的 Target - 规定修饰的类型 Retention - 规定策略的类型, 见RetentionPolicy也不直接具有修改注入的能力.Go Deep?查阅了一些资料后 Java Annotation - runoob Lombok Github Repo Compilation-Overview - OpenJDK JSR 269 Pluggable Annotation Processing API所以说到Annotation就是一个接口的话, 那么回想被注释的类其实是被Implement了. 并且被这些注解注释的类, 在javac对其进行编译过程时, 会拉起他们对应的注解 解释?/执行? 器Annotation Processor. 在这些注解执行类中, 通过重写@OverWrite几个关键的执行方法从而达到编译注入的目的./** * An abstract annotation processor designed to be a convenient * superclass for most concrete annotation processors. This class * examines annotation values to compute the {@linkplain * #getSupportedOptions options}, {@linkplain * #getSupportedAnnotationTypes annotation types}, and {@linkplain * #getSupportedSourceVersion source version} supported by its * subtypes. * * &amp;lt;p&amp;gt;The getter methods may {@linkplain Messager#printMessage issue * warnings} about noteworthy conditions using the facilities available * after the processor has been {@linkplain #isInitialized * initialized}. * * &amp;lt;p&amp;gt;Subclasses are free to override the implementation and * specification of any of the methods in this class as long as the * general {@link javax.annotation.processing.Processor Processor} * contract for that method is obeyed. * * @author Joseph D. Darcy * @author Scott Seligman * @author Peter von der Ah&amp;amp;eacute; * @since 1.6 */public abstract class AbstractProcessor implements Processor在Lombok的源码中, 可见其几个Processor的类都是继承自上述这个在package javax.annotation.processing;中的类的. core/lombok/javac/handlers/HandleData.java core/lombok/javac/handlers/HandleSetter.java …并且采用了一些常用的反射和修改AST树来实现功能的.这里不细说反射原理, 同时javac处理RetentionPolicy的策略属Java Compilation范畴.所以回到代码举个例子 本次工程是Maven同步的, 所以javac -cp引入lombok的时候要找到本地maven repo的位置并定位lombok-xxx.jar, 如果你没修改过默认应该是~/.m2下的Contact.java@Datapublic class Contact { private Long contactId; private String familyName; private String givenName; private String mobileNum; private Date birthDay;}@Data 部分注释, 即是@Data在被处理时同时引入下列几个 * @see Getter * @see Setter * @see RequiredArgsConstructor * @see ToString * @see EqualsAndHashCode * @see lombok.Value并且观察Lombok的源码也可以发现其拉起了几个Handlercore/lombok/javac/handlers/HandleData.javahandleConstructor.generateRequiredArgsConstructor(typeNode, AccessLevel.PUBLIC, staticConstructorName, SkipIfConstructorExists.YES, annotationNode); handleConstructor.generateExtraNoArgsConstructor(typeNode, annotationNode); handleGetter.generateGetterForType(typeNode, annotationNode, AccessLevel.PUBLIC, true, List.&amp;lt;JCAnnotation&amp;gt;nil()); handleSetter.generateSetterForType(typeNode, annotationNode, AccessLevel.PUBLIC, true, List.&amp;lt;JCAnnotation&amp;gt;nil(), List.&amp;lt;JCAnnotation&amp;gt;nil()); handleEqualsAndHashCode.generateEqualsAndHashCodeForType(typeNode, annotationNode); handleToString.generateToStringForType(typeNode, annotationNode);这里只去看HandleSetter - HandleSetter - Lombok, 不贴出通过MijazzChan@R720 MINGW64 /e/JWorkSpace/JSourceCodeLearn/src/main/java/edu/zstu/mijazz/lomboklearn/entity (master)$ javac -cp /c/Dev/Env/m2Repo/org/projectlombok/lombok/1.18.16/lombok-1.18.16.jar ./Contact.javaMijazzChan@R720 MINGW64 /e/JWorkSpace/JSourceCodeLearn/src/main/java/edu/zstu/mijazz/lomboklearn/entity (master)$ lsContact.class Contact.javaMijazzChan@R720 MINGW64 /e/JWorkSpace/JSourceCodeLearn/src/main/java/edu/zstu/mijazz/lomboklearn/entity (master)$ javap ./Contact.classCompiled from &quot;Contact.java&quot;public class edu.zstu.mijazz.lomboklearn.entity.Contact { public edu.zstu.mijazz.lomboklearn.entity.Contact(); / # 不是Lombok的 public java.lang.Long getContactId(); public java.lang.String getFamilyName(); public java.lang.String getGivenName(); public java.lang.String getMobileNum(); public java.util.Date getBirthDay(); public void setContactId(java.lang.Long); public void setFamilyName(java.lang.String); public void setGivenName(java.lang.String); public void setMobileNum(java.lang.String); public void setBirthDay(java.util.Date); public boolean equals(java.lang.Object); protected boolean canEqual(java.lang.Object); public int hashCode(); public java.lang.String toString();}可以看到lombok注入的方法是可以通过反编译类看到的, 如果你想看到更详细的类内步骤, 可以用IntelliJ IDEA打开反编译. 这里额外提一下, @Data不会注入无参Constructor, 这里之所以有无参构造器, 是因为JDK自动会根据其超类-Object自动创建, 这个在Java文档Constructor中有提到. 如果需要自定义一个属于自己的类的无参构造器, 你需要@NoArgsConstructor.You don&#39;t have to provide any constructors for your class, but you must be careful when doing this. The compiler automatically provides a no-argument, default constructor for any class without constructors. This default constructor will call the no-argument constructor of the superclass. In this situation, the compiler will complain if the superclass doesn&#39;t have a no-argument constructor so you must verify that it does. If your class has no explicit superclass, then it has an implicit superclass of Object, which does have a no-argument constructorRaising Another Question既然解决了javac的问题, 那另一个问题, javac是编译时是有AST的, 也就是Abstract Syntax Tree.反编译出来的文件中存在Lombok加入的方法, 那么在编译时, 这些方法就会有对应的AST节点.如果能够把注入前的AST通过静态代码抽象出来, 若静态代码的AST树不包含这些新加入方法的AST节点, 那么就可以判定Lombok的确是在编译时通过修改并补全AST来实现字节码更改的.引入javaparser的Maven依赖&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;com.github.javaparser&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;javaparser-core&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;3.18.0&amp;lt;/version&amp;gt;&amp;lt;/dependency&amp;gt;写一个简单的工具类public class JavacCompileUtil { @SneakyThrows public static void main(String[] args) { CompilationUnit compilationUnit = StaticJavaParser.parse(new File(&quot;$$CHANGE HERE TO CLASS FILE PATH&quot;)); YamlPrinter yamlPrinter = new YamlPrinter(true); System.out.println(yamlPrinter.output(compilationUnit)); }}---root(Type=CompilationUnit): packageDeclaration(Type=PackageDeclaration): name(Type=Name): identifier: &quot;entity&quot; qualifier(Type=Name): identifier: &quot;lomboklearn&quot; qualifier(Type=Name): identifier: &quot;mijazz&quot; qualifier(Type=Name): identifier: &quot;zstu&quot; qualifier(Type=Name): identifier: &quot;edu&quot; imports: - import(Type=ImportDeclaration): isAsterisk: &quot;false&quot; isStatic: &quot;false&quot; name(Type=Name): identifier: &quot;Data&quot; qualifier(Type=Name): identifier: &quot;lombok&quot; - import(Type=ImportDeclaration): isAsterisk: &quot;false&quot; isStatic: &quot;false&quot; name(Type=Name): identifier: &quot;NoArgsConstructor&quot; qualifier(Type=Name): identifier: &quot;lombok&quot; - import(Type=ImportDeclaration): isAsterisk: &quot;false&quot; isStatic: &quot;false&quot; name(Type=Name): identifier: &quot;Date&quot; qualifier(Type=Name): identifier: &quot;util&quot; qualifier(Type=Name): identifier: &quot;java&quot; types: - type(Type=ClassOrInterfaceDeclaration): isInterface: &quot;false&quot; name(Type=SimpleName): identifier: &quot;Contact&quot; comment(Type=JavadocComment): content: &quot;\\r\\n * @Time 2020-12-09 3:54 PM\\r\\n * @Author MijazzChan\\r\\n * Lombok Learn package, ENTITY class, class{Contact} as a person\\r\\n &quot; members: - member(Type=FieldDeclaration): modifiers: - modifier(Type=Modifier): keyword: &quot;PRIVATE&quot; variables: - variable(Type=VariableDeclarator): name(Type=SimpleName): identifier: &quot;contactId&quot; type(Type=ClassOrInterfaceType): name(Type=SimpleName): identifier: &quot;Long&quot; - member(Type=FieldDeclaration): modifiers: - modifier(Type=Modifier): keyword: &quot;PRIVATE&quot; variables: - variable(Type=VariableDeclarator): name(Type=SimpleName): identifier: &quot;familyName&quot; type(Type=ClassOrInterfaceType): name(Type=SimpleName): identifier: &quot;String&quot; - member(Type=FieldDeclaration): modifiers: - modifier(Type=Modifier): keyword: &quot;PRIVATE&quot; variables: - variable(Type=VariableDeclarator): name(Type=SimpleName): identifier: &quot;givenName&quot; type(Type=ClassOrInterfaceType): name(Type=SimpleName): identifier: &quot;String&quot; - member(Type=FieldDeclaration): modifiers: - modifier(Type=Modifier): keyword: &quot;PRIVATE&quot; variables: - variable(Type=VariableDeclarator): name(Type=SimpleName): identifier: &quot;mobileNum&quot; type(Type=ClassOrInterfaceType): name(Type=SimpleName): identifier: &quot;String&quot; - member(Type=FieldDeclaration): modifiers: - modifier(Type=Modifier): keyword: &quot;PRIVATE&quot; variables: - variable(Type=VariableDeclarator): name(Type=SimpleName): identifier: &quot;birthDay&quot; type(Type=ClassOrInterfaceType): name(Type=SimpleName): identifier: &quot;Date&quot; modifiers: - modifier(Type=Modifier): keyword: &quot;PUBLIC&quot; annotations: - annotation(Type=MarkerAnnotationExpr): name(Type=Name): identifier: &quot;Data&quot; - annotation(Type=MarkerAnnotationExpr): name(Type=Name): identifier: &quot;NoArgsConstructor&quot;...可以很清楚的看到, 在针对Contact.java的AST树中, 并不存在有关Lombok注入方法的节点.Back to Documentation通过了解AST在javac里的作用后, 几乎可以确定Lombok是编译时通过修改AST树并补全相应节点, 来实现方法的注入的.即 Source ClassFile -&amp;gt; Parse -&amp;gt; AST -&amp;gt; Handle Annotation -&amp;gt; Call/Find Annotation Handler -&amp;gt; Lombok Annotation Processor - handle/modify AST -&amp;gt; Analyze/Fill AST node -&amp;gt; New/Modified AST -&amp;gt; Byte Code上述编译过程中, Lombok对应的即是Annotation Processing这一步. 详细可以参考JSR-269同时也在src/utils/lombok/javac/JavacTreeMaker.java找到了相应对AST进行操作的代码.至于为什么在Lombok下搜索Processor会出现多个Annotation Processor相关的类呢, 官网也给出了解释. src/core/lombok/core/AnnotationProcessor.java src/core/lombok/javac/apt/Processor.java src/core/lombok/javac/apt/LombokProcessor.java src/launch/lombok/launch/AnnotationProcessor.javalombok.launch.AnnotationProcessorHider$AnnotationProcessor作为入口, 被javac在执行Annotation Processing这一步拉起. 它将被实例化并且执行init(). 它会开始寻找lombok的Jar File, 注: 这里的Jar包并不是.jar结尾的, 而是.SCL.lombok, 并且通过ClassLoader开始加载lombok的core包.lombok.core.AnnotationProcessor会是接下来core中先执行的类, 它也是一个入口类. 它根据运行环境是否是javac或者是eclipse ecj, 来选择对应的Annotation Processor来进一步处理.最终lombok.javac.apt.LombokProcessor才是操作并处理注入的Annotation Processor.同时你在Jar File处看不到Lombok的源码也有其原因的书写代码时, IDE会根据Jar包中索引到的类对你进行代码提示, 但是由于Lombok工程的特殊性, 你只需要在编译时需要其Jar包的依赖.对于未编译层面的Java语句来说, 如果包在这个层级可见, 会在代码提示中或索引里增加很多你可能不需要的类.所以Lombok类在Java-the-language是不可见的, 但在Java-the-JVM是可见的.同时上面也说到lombok.launch作为入口处, 其寻找.SCL.lombok结尾的包, 使用ClassLoader运行时才加载, 这种反常规甚至奇妙的方式(官方用的convoluted trick)也可以避免其被索引所带来的麻烦. https://projectlombok.org/contributing/lombok-execution-path With javac (and netbeans, maven, gradle, and most other build systems), lombok runs as an annotation processor. Lombok is on the classpath, and javac will load every META-INF/services/javax.annotation.processing.Processor file on the classpath it can find, reading each line and loading that class, then executing it as an annotation processor. lombok.jar has this file, it lists lombok.launch.AnnotationProcessorHider$AnnotationProcessor as entry. This class is not actually visible (it is public, but its outer class (AnnotationProcessorHider) is package private, making it invisible to java-the-language), however, it is considered visible for the purposes of java-the-VM and therefore it will run. This convoluted trick is used to ensure that anybody who develops with lombok on the classpath doesn’t get lombok’s classes or lombok’s dependencies injected into their ‘namespace’ (for example, if you add lombok to your project, your IDE will not start suggesting lombok classes for auto-complete dialogs). The lombok.launch.AnnotationProcessorHider$AnnotationProcessor class is loaded by javac, instantiated, and init() is called on it. This class starts lombok’s ShadowClassLoader; it finds the jar file it is in, then will start loading classes from this jar file. It looks not for files ending in .class like normal loaders, it looks for files ending in .SCL.lombok instead (this too is for the purpose of hiding lombok’s classes from IDEs and such). Via this classloader, the real annotation processor is launched, which is class lombok.core.AnnotationProcessor. The lombok.core.AnnotationProcessor is also a delegating processor. It can delegate to one of 2 sub-processors based on the environment lombok finds itself in: If it’s javac, class lombok.javac.apt.LombokProcessor is used (and if the plexus compiler framework is used, which can be the case when compiling with javac, some extra code runs to patch lombok into its modular classloading architecture). If it’s ecj (eclipse’s compiler, which means we’re either running inside eclipse itself, or being invoked as annotation processor for ecj, the standalone eclipse compiler), errors/warnings are injected into the compilation process to tell the user they should use different parameters to use lombok in eclipse/ecj. lombok.javac.apt.LombokProcessor is the ‘real’ annotation processor that does the work of transforming your code." }, { "title": "Code Scoping in Data Practicing", "url": "/posts/Code-Scoping-in-Data-Practicing/", "categories": "Data, Spark", "tags": "notes", "date": "2020-12-04 01:48:17 +0800", "snippet": "Code Scoping in Data Practicing Personal Notescast() in spark scalaFound useful when I tried to import or operate on a dataFrame. Source code is as follows /** * Casts the column to a different data type. * * // Casts colA to IntegerType. * import org.apache.spark.sql.types.IntegerType * df.select(df(&quot;colA&quot;).cast(IntegerType)) * * // equivalent to * df.select(df(&quot;colA&quot;).cast(&quot;int&quot;)) * * * @group expr_ops * @since 1.3.0 **/ def cast(to: DataType): Column = withExpr { Cast(expr, to) } /** * Casts the column to a different data type, using the canonical string representation * of the type. The supported types are: `string`, `boolean`, `byte`, `short`, `int`, `long`, * `float`, `double`, `decimal`, `date`, `timestamp`. * * // Casts colA to integer. * df.select(df(&quot;colA&quot;).cast(&quot;int&quot;)) * * * @group expr_ops * @since 1.3.0 **/ def cast(to: String): Column = cast(CatalystSqlParser.parseDataType(to))cast() method can receive String of type, supporting string, boolean, byte, short, int, long,float, double, decimal, date, timestamp.it uses CatalystSqlParser.parseDataType() -&amp;gt; abstract class -&amp;gt; AstBuilder.visitPrimitiveDataType()there are several primitive data types written in case clause./** * Create a Spark DataType. */ private def visitSparkDataType(ctx: DataTypeContext): DataType = { HiveStringType.replaceCharType(typedVisit(ctx)) } /** * Resolve/create a primitive type. */ override def visitPrimitiveDataType(ctx: PrimitiveDataTypeContext): DataType = withOrigin(ctx) { val dataType = ctx.identifier.getText.toLowerCase(Locale.ROOT) (dataType, ctx.INTEGER_VALUE().asScala.toList) match { case (&quot;boolean&quot;, Nil) =&amp;gt; BooleanType case (&quot;tinyint&quot; | &quot;byte&quot;, Nil) =&amp;gt; ByteType case (&quot;smallint&quot; | &quot;short&quot;, Nil) =&amp;gt; ShortType case (&quot;int&quot; | &quot;integer&quot;, Nil) =&amp;gt; IntegerType case (&quot;bigint&quot; | &quot;long&quot;, Nil) =&amp;gt; LongType case (&quot;float&quot;, Nil) =&amp;gt; FloatType case (&quot;double&quot;, Nil) =&amp;gt; DoubleType case (&quot;date&quot;, Nil) =&amp;gt; DateType case (&quot;timestamp&quot;, Nil) =&amp;gt; TimestampType case (&quot;string&quot;, Nil) =&amp;gt; StringType case (&quot;char&quot;, length :: Nil) =&amp;gt; CharType(length.getText.toInt) case (&quot;varchar&quot;, length :: Nil) =&amp;gt; VarcharType(length.getText.toInt) case (&quot;binary&quot;, Nil) =&amp;gt; BinaryType case (&quot;decimal&quot;, Nil) =&amp;gt; DecimalType.USER_DEFAULT case (&quot;decimal&quot;, precision :: Nil) =&amp;gt; DecimalType(precision.getText.toInt, 0) case (&quot;decimal&quot;, precision :: scale :: Nil) =&amp;gt; DecimalType(precision.getText.toInt, scale.getText.toInt) case (dt, params) =&amp;gt; val dtStr = if (params.nonEmpty) s&quot;$dt(${params.mkString(&quot;,&quot;)})&quot; else dt throw new ParseException(s&quot;DataType $dtStr is not supported.&quot;, ctx) } }Going deep, this package is located right under the spark.sql core component =&amp;gt; Catalyst, which provides spark.sql with Parser Analyzer Optimizerextends App in Scala/** * &#39;&#39;&#39;&#39;&#39;It should be noted that this trait is implemented using the [[DelayedInit]] * functionality, which means that fields of the object will not have been initialized * before the main method has been executed.&#39;&#39;&#39;&#39;&#39;**//** The main method. * This stores all arguments so that they can be retrieved with `args` * and then executes all initialization code segments in the order in which * they were passed to `delayedInit`. * @param args the arguments passed to the main method */ @deprecatedOverriding(&quot;main should not be overridden&quot;, &quot;2.11.0&quot;) def main(args: Array[String]) = { this._args = args for (proc &amp;lt;- initCode) proc() if (util.Properties.propIsSet(&quot;scala.time&quot;)) { val total = currentTime - executionStart Console.println(&quot;[total &quot; + total + &quot;ms]&quot;) } }Object which extends App will get a extend-chain with DelayInit, and a time-consumption output.The main() method will also be overwritten, thus the whole class may become a main method.(Kind of like a lang-trick)Plus, the DelayedInit is flagged outdated.Temp View of sparkspark.sql(&#39;SQL STATEMENT&#39;) can easily call sql executed in spark session. Make sure you create a tempview before you try to sql your dataframe. TempView createOrReplaceTempView() GlobalTempView createOrReplaceGlobalTempView() @throws[AnalysisException] def createGlobalTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = false, global = true) }/** * Creates or replaces a global temporary view using the given name. The lifetime of this * temporary view is tied to this Spark application. * * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application, * i.e. it will be automatically dropped when the application terminates. It&#39;s tied to a system * preserved database `global_temp`, and we must use the qualified name to refer a global temp * view, e.g. `SELECT * FROM global_temp.view1`. * * @group basic * @since 2.2.0 */ def createOrReplaceGlobalTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = true, global = true) }Try not use create...(). It throws AnalysisException when the view name has already been taken.Also, mind that global_temp.viewname is required when accessing GlobalTemp.Column to List in Pysparkeg = [0 for _ in range(4)]eg[0] = list(exampleSparkDataFrame.toPandas()[&#39;Number&#39;])eg[1] = exampleSparkDataFrame.select(&#39;Number&#39;).rdd.flatMap(lambda x: x).collect()eg[2] = exampleSparkDataFrame.select(&#39;Number&#39;).rdd.map(lambda x: x[0]).collect()eg[3] = [x[0] for x in exampleSparkDataFrame.select(&#39;Number&#39;).collect()]" }, { "title": "Test Drive Spark/Hadoop using Chicago Crime Data", "url": "/posts/Test-Drive-Spark-Hadoop-using-Chicago-Crime-Data/", "categories": "Data, Spark", "tags": "data, hadoop, spark, java", "date": "2020-11-25 17:04:39 +0800", "snippet": "Data Practicing-EP0Before You Read This该文只是一门课程数据挖掘的大作业记录文, 并不是教程式文章.Why This Topic?看了很多人写的技术博客, 发现许多都是基于Pseudo-Distributed Mode或者Fully-Distributed Mode. 这两种模式因为资源问题我也只成功搭建并使用过前者, 当时在Windows10上拖着一个CentOS8的虚拟机, 因为数据集就快2GB大小, 一套下来发现虚拟机吃的内存快接近7G.又想试试YARN又不想做多机分布式, 那就做单机YARN 关于YARN:官方的也很简明易懂 Apache Hadoop YARN毕竟数据集只是几G, python直接上pandas套装估计会更方便.抱着入门一下这个技术栈, 完成一下大作业和想拥抱一下Arch社区的心态, 在使用过LinuxMint和Fedora的我换到了Manjaro Linux 20. mijazz@lenovo  ~  screenfetch ██████████████████ ████████ mijazz@lenovo ██████████████████ ████████ OS: Manjaro 20.2 Nibia ██████████████████ ████████ Kernel: x86_64 Linux 5.8.18-1-MANJARO ██████████████████ ████████ Uptime: 16m ████████ ████████ Packages: 1239 ████████ ████████ ████████ Shell: zsh 5.8 ████████ ████████ ████████ Resolution: 1920x1080 ████████ ████████ ████████ DE: KDE 5.76.0 / Plasma 5.20.3 ████████ ████████ ████████ WM: KWin ████████ ████████ ████████ GTK Theme: Breath [GTK2/3] ████████ ████████ ████████ Icon Theme: breath2-dark ████████ ████████ ████████ Disk: 18G / 102G (19%) ████████ ████████ ████████ CPU: Intel Core i5-7300HQ @ 4x 3.5GHz [45.0°C] ████████ ████████ ████████ GPU: GeForce GTX 1050 RAM: 2880MiB / 15904MiB mijazz@lenovo  ~  uname -a Linux lenovo 5.8.18-1-MANJARO #1 SMP PREEMPT Sun Nov 1 14:10:04 UTC 2020 x86_64 GNU/LinuxPrerequisite Java - OpenJDK8 为了避免奇怪的兼容性问题, 根据Hadoop官方的wiki, 其推荐版本是8, 并且更高版本的Hadoop支持11. 同时明确指明了OpenJDK8是官方用于其编译的版本. 此处参考: cwiki-Apache 这里给出AdoptOpenJDK8的地址 Redhat的, OpenJDK自编译的, 又或者来自Pacman包管理的都应该不成问题. 但是我在CentOS8上首次尝试Hadoop的时候, conf里也要多配置一次$JAVA_HOME有点奇怪 Official URL 清华tuna镜像 Hadoop - Hadoop 3.1.4 Tarball Official URL Aliyun Mirrors Spark - Spark 2.4.7 Tarball 选 Pre-built with user-provided Apache Hadoop 因采用YARN部署 Official URL Aliyun Mirrors Prepare EnvironmentExtract JDKmijazz@lenovo  ~/devEnvs  ll -atotal 100Mdrwxr-xr-x 6 mijazz mijazz 4.0K Nov 25 15:23 .drwx------ 36 mijazz mijazz 4.0K Nov 25 15:24 ..-rw-r--r-- 1 mijazz mijazz 100M Nov 24 15:20 OpenJDK8U-jdk_x64_linux_hotspot_8u275b01.tar.gzmijazz@lenovo  ~/devEnvs  tar -xf ./OpenJDK8U*.tar.gz mijazz@lenovo  ~/devEnvs  mv ./jdk8u275-b01 ./OpenJDK8mijazz@lenovo  ~/devEnvs  rm ./OpenJDK8U*.tar.gz mijazz@lenovo  ~/devEnvs  ll -atotal 28Kdrwxr-xr-x 7 mijazz mijazz 4.0K Nov 25 15:28 .drwx------ 36 mijazz mijazz 4.0K Nov 25 15:28 ..drwxr-xr-x 8 mijazz mijazz 4.0K Nov 9 20:23 OpenJDK8Configure $PATH 我使用的是zsh, 并且不索引bashrc. 虽然到时候我会在主shell里启动, 但是不保证他会不会在某些部件里调用bash. 我就把环境变量同时加进~/.bashrc和~/.zshrc里了. 如果你是使用bash, 只需加进~/.bashrc, 记得source或重启shell生效.echo &#39;# Java Environment Variable&#39; &amp;gt;&amp;gt; ~/.bashrcecho &#39;export JAVA_HOME=&quot;/home/mijazz/devEnvs/OpenJDK8&quot;&#39; &amp;gt;&amp;gt; ~/.bashrcecho &#39;export JRE_HOME=&quot;${JAVA_HOME}/jre&quot;&#39; &amp;gt;&amp;gt; ~/.bashrcecho &#39;export CLASSPATH=&quot;.:${JAVA_HOME}/lib:${JRE_HOME}/lib&quot;&#39; &amp;gt;&amp;gt; ~/.bashrcecho &#39;export PATH=&quot;${JAVA_HOME}/bin:$PATH&quot;&#39; &amp;gt;&amp;gt; ~/.bashrc# Only if u r using zshecho &#39;# Java Environment Variable&#39; &amp;gt;&amp;gt; ~/.zshrcecho &#39;export JAVA_HOME=&quot;/home/mijazz/devEnvs/OpenJDK8&quot;&#39; &amp;gt;&amp;gt; ~/.zshrcecho &#39;export JRE_HOME=&quot;${JAVA_HOME}/jre&quot;&#39; &amp;gt;&amp;gt; ~/.zshrcecho &#39;export CLASSPATH=&quot;.:${JAVA_HOME}/lib:${JRE_HOME}/lib&quot;&#39; &amp;gt;&amp;gt; ~/.zshrcecho &#39;export PATH=&quot;${JAVA_HOME}/bin:$PATH&quot;&#39; &amp;gt;&amp;gt; ~/.zshrcVerify Configuration mijazz@lenovo  ~  echo $JAVA_HOME/home/mijazz/devEnvs/OpenJDK8 mijazz@lenovo  ~  java -version openjdk version &quot;1.8.0_275&quot;OpenJDK Runtime Environment (AdoptOpenJDK)(build 1.8.0_275-b01)OpenJDK 64-Bit Server VM (AdoptOpenJDK)(build 25.275-b01, mixed mode)SSH, sshd.service Hadoop: Setting up a Single Node Cluster. ssh must be installed and sshd must be running to use the Hadoop scripts that manage remote Hadoop daemons if the optional start and stop scripts are to be used. mijazz@lenovo  ~  ssh localhostssh: connect to host localhost port 22: Connection refused mijazz@lenovo  ~  systemctl status sshd● sshd.service - OpenSSH Daemon Loaded: loaded (/usr/lib/systemd/system/sshd.service; disabled; vendor preset: disabled) Active: inactive (dead)明显没开, 配置一下sshd和限制root远程登录什么的, 还有只允许公匙登录什么的.sshd_configPasswordAuthentication noPermitEmptyPasswords noPubkeyAuthentication yesPermitRootLogin no权限记得设0600, Unix/Linux对这类权限要求很严格.ssh-keygen -t rsa -b 4096cat ~/.ssh/id_rsa.pub &amp;gt;&amp;gt; ~/.ssh/authorized_keyschmod 600 ~/.ssh/authorized_keys mijazz@lenovo  ~  sudo systemctl start sshd [sudo] password for mijazz: mijazz@lenovo  ~  sudo systemctl enable sshdCreated symlink /etc/systemd/system/multi-user.target.wants/sshd.service → /usr/lib/systemd/system/sshd.service. mijazz@lenovo  ~  ssh localhost The authenticity of host &#39;localhost (::1)&#39; can&#39;t be established.ECDSA key fingerprint is SHA256:OvL0/qmZWaRDL66+wbprrEiK4XhNgo1FAU/jRoWIsc0.Are you sure you want to continue connecting (yes/no/[fingerprint])? yesWarning: Permanently added &#39;localhost&#39; (ECDSA) to the list of known hosts. mijazz@lenovo  ~  exitConnection to localhost closed.公匙免密登录就配置好了Data Practicing-EP1Testing HadoopEP0中给出的hadoop-3.1.4.tar.gz mijazz@lenovo  ~/devEnvs  tar -xf ./hadoop-3.1.4.tar.gz # 文件结构 mijazz@lenovo  ~/devEnvs  tree -L 1 ./hadoop-3.1.4 ./hadoop-3.1.4├── LICENSE.txt├── NOTICE.txt├── README.txt├── bin # 可执行├── etc # 配置├── include ├── lib├── libexec├── sbin # 可执行└── share7 directories, 3 files严格按着官网给出的doc配置 Unpack the downloaded Hadoop distribution. In the distribution, edit the file etc/hadoop/hadoop-env.sh to define some parameters as follows: export JAVA_HOME=/usr/java/latest # DO NOT ADD THIS LINE Docs也就是让我们在hadoop下的配置路径下, 在加上一次JAVA_HOME的路径.严格来讲~/.bashrc ~/.zshrc环境下有就行echo &#39;export JAVA_HOME=/home/mijazz/devEnvs/OpenJDK8&#39; &amp;gt;&amp;gt; ./hadoop-3.1.4/etc/hadoop/hadoop-env.sh然后启动一次hadoop mijazz@lenovo  ~/devEnvs/hadoop-3.1.4  ./bin/hadoopUsage: hadoop [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS] or hadoop [OPTIONS] CLASSNAME [CLASSNAME OPTIONS] where CLASSNAME is a user-provided Java class OPTIONS is none or any of:--config dir Hadoop config directory--debug turn on shell script debug mode...............Configure Hadoop 官方文档推荐在etc/hadoop/core-site.xml和etc/hadoop/hdfs-site.xml下各添加一个字段. 此处不表, 直接贴出上述两个文件.core-site.xml&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;&amp;lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&amp;gt;&amp;lt;configuration&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;hadoop.tmp.dir&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;/home/mijazz/devEnvs/hadoop-3.1.4/tmp&amp;lt;/value&amp;gt; &amp;lt;description&amp;gt;Abase for other temporary directories.&amp;lt;/description&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;hdfs://localhost:9000&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt;&amp;lt;/configuration&amp;gt;hdfs-site.xml&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;&amp;lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&amp;gt;&amp;lt;configuration&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.replication&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;1&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.namenode.rpc-bind-host&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;0.0.0.0&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.namenode.servicerpc-bind-host&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;0.0.0.0&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.namenode.http-bind-host&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;0.0.0.0&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.namenode.https-bind-host&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;0.0.0.0&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt;&amp;lt;/configuration&amp;gt; 比官网多出来的这几个字段, 方便interface bind. 也就是都监听. 因为之前我尝试在虚拟机(CentOS8: 192.168.123.5/24, 172.16.0.2/24)和宿主机(Windows10: 192.168.123.2/24, 172.16.0.1/24)和另一台PC(CentOS7: 192.168.123.4/24, 10.100.0.2/16(OpenVPN-NAT))之间尝试做分布式. 这几个选项对我的恶心人的网络拓扑(又是host-only, bridge, openvpn-nat)很有帮助. 官方文档HDFS Support for Multihomed Networks避坑环境变量 老样子, 追加~/.zshrc和~/.bashrcexport HADOOP_HOME=&quot;/home/mijazz/devEnvs/hadoop-3.1.4&quot;export HADOOP_MAPRED_HOME=$HADOOP_HOME export HADOOP_COMMON_HOME=$HADOOP_HOME export HADOOP_HDFS_HOME=$HADOOP_HOME export YARN_HOME=$HADOOP_HOME export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin export HADOOP_INSTALL=$HADOOP_HOME Configure HDFS引用一下官方文档两张简洁明了的图HDFS Docs官方文档叙述的架构非常易懂.这里只提其对用户暴露出的Shell Commands接口, 其命令与Unix/Linux默认文件系统的接口非常类似. mijazz@lenovo  ~/devEnvs/hadoop-3.1.4  ./bin/hdfs dfs -help即可获得其描述.继续配置HDFS先对NameNode也就是hdfs的master做一下格式化 mijazz@lenovo  ~/devEnvs/hadoop-3.1.4  ./bin/hdfs namenode -formatWARNING: /home/mijazz/devEnvs/hadoop-3.1.4/logs does not exist. Creating.2020-11-25 23:49:55,071 INFO namenode.NameNode: STARTUP_MSG: /************************************************************STARTUP_MSG: Starting NameNodeSTARTUP_MSG: host = lenovo/127.0.1.1STARTUP_MSG: args = [-format]STARTUP_MSG: version = 3.1.4.............然后同时启动DataNode和NameNode mijazz@lenovo  ~/devEnvs/hadoop-3.1.4  ./sbin/start-dfs.sh Starting namenodes on [localhost]Starting datanodesStarting secondary namenodes [lenovo] 这里可以看到NameNode和DataNode的主机名并不一致, 因为之前配置的ssh登录就是在这里起作用的. 因为HDFS主要面向集群, 也就是NameNode-Master和DataNodes-Slaves大多配置在不同的机器上, 其之间的通信都是通过ssh的. 尽管当前部署是本地单机部署, 他还是会用ssh和本地的sshd来进行沟通.然后访问http://localhost:9870如果看到hadoop就行.可能会遇到的坑启动了上述命令, 但是访问localhost:9870没反映那就尝试做一下问题定位 先看看其是不是成功启动了, 用jps看看就行.发现没有Java进程在运行.到%HADOOP_HOME/logs下找日志就行2020-11-25 23:59:57,076 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 1: SIGHUP2020-11-25 23:59:57,076 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM2020-11-25 23:59:57,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: /************************************************************SHUTDOWN_MSG: Shutting down DataNode at lenovo/127.0.1.1************************************************************/看到了一些奇怪的东西127.0.1.1. 该指向在默认的hosts中存在, 具体原因不表, 具体情况因Linux发行版而异.看到这里, 很明显是主机名被解析到了一个不能访问到本机的ip上, 导致是DataNode和NameNode之前没心跳. mijazz@lenovo  ~/devEnvs/hadoop-3.1.4/logs  cat /etc/hosts# Host addresses127.0.0.1 localhost127.0.1.1 lenovo # Here.............简单修改就行. mijazz@lenovo  ~  cat /etc/hosts# Host addresses127.0.0.1 localhost127.0.0.1 lenovo # 127.0.0.1............ 还是启动不了, 尝试一下用hadoop namenode和hadoop datanode放在shell里启动. 如果此时jps能看到活的进程, 并且curl localhost:9870有返回, 后续可以尝试: 有条件可以去读一下几个启动脚本的源码, 如果启动脚本拉起不了, 既有可能是目录权限问题或者用户权限问题. 因为官方本就推荐hadoop运行于一个独立的用户/用户组. stop-all.shhadoop-daemon.sh start namenodehadoop-daemon.sh start datanode Configure YARN YARN = Yet Another Resource Negotiator 什么是YARN -&amp;gt; YARN Architechture继续贴我自己使用的配置文件etc/hadoop/mapred-site.xml&amp;lt;?xml version=&quot;1.0&quot;?&amp;gt;&amp;lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&amp;gt;&amp;lt;configuration&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;mapreduce.framework.name&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;yarn&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;mapreduce.application.classpath&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt;&amp;lt;/configuration&amp;gt;etc/hadoop/yarn-site.xml&amp;lt;?xml version=&quot;1.0&quot;?&amp;gt;&amp;lt;configuration&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;yarn.nodemanager.aux-services&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;mapreduce_shuffle&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;yarn.nodemanager.env-whitelist&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;yarn.nodemanager.vmem-check-enabled&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;false&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;yarn.webapp.ui2.enable&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt;&amp;lt;/configuration&amp;gt;直接用命令拉起NodeManager和ResourceManager即可. mijazz@lenovo  ~/devEnvs/hadoop-3.1.4  start-yarn.sh可能的坑用该命令拉起尝试即可yarn-daemon.sh start resourcemanageryarn-daemon.sh start nodemanageryarn-daemon.sh start historyserver另外定位%HADOOP_HOME/logs下即可.有个隐藏的坑随后再更新Minor Adjustment如果报Unable to load native-hadoop library for your platform试着执行 mijazz@lenovo  ~/devEnvs  hadoop checknative如果下列是一堆no, 不要上百度找答案, 百度上还有说不带lib的.只需要在etc/hadoop/hadoop-env.sh追加一个JVM参数就行.export HADOOP_OPTS=&quot;${HADOOP_OPTS} -Djava.library.path=${HADOOP_HOME}/lib/native&quot;随后 mijazz@lenovo  ~/devEnvs/spark-2.4.7  hadoop checknative2020-11-27 17:49:39,805 INFO bzip2.Bzip2Factory: Successfully loaded &amp;amp; initialized native-bzip2 library system-native2020-11-27 17:49:39,809 INFO zlib.ZlibFactory: Successfully loaded &amp;amp; initialized native-zlib library2020-11-27 17:49:39,817 WARN erasurecode.ErasureCodeNative: ISA-L support is not available in your platform... using builtin-java codec where applicable2020-11-27 17:49:39,864 INFO nativeio.NativeIO: The native code was built without PMDK support.Native library checking:hadoop: true /home/mijazz/devEnvs/hadoop-3.1.4/lib/native/libhadoop.so.1.0.0zlib: true /usr/lib/libz.so.1zstd : true /usr/lib/libzstd.so.1snappy: true /usr/lib/libsnappy.so.1lz4: true revision:10301bzip2: true /usr/lib/libbz2.so.1openssl: false EVP_CIPHER_CTX_cleanupISA-L: false libhadoop was built without ISA-L supportPMDK: false The native code was built without PMDK support.再次运行时警告就会消失.验证 http://your.host.or.ip:9870 http://you.host.or.ip:8088 jps mijazz@lenovo  ~/devEnvs/hadoop-3.1.4  jps2160 NameNode7297 Jps6084 ApplicationHistoryServer2551 ResourceManager2413 NodeManager2237 DataNode Data Practicing-EP2Testing SparkEP0中的spark mijazz@lenovo  ~/devEnvs  ll -atotal 161Mdrwxr-xr-x 8 mijazz mijazz 4.0K Nov 27 17:27 .drwx------ 38 mijazz mijazz 4.0K Nov 27 17:27 ..drwxr-xr-x 8 mijazz mijazz 4.0K Nov 9 20:23 OpenJDK8drwxr-xr-x 11 mijazz mijazz 4.0K Nov 25 23:49 hadoop-3.1.4-rw-r--r-- 1 mijazz mijazz 161M Nov 24 16:49 spark-2.4.7-bin-without-hadoop.tgz mijazz@lenovo  ~/devEnvs  tar -xf spark-2.4.7-bin-without-hadoop.tgz mijazz@lenovo  ~/devEnvs  mv ./spark-2.4.7-bin-without-hadoop ./spark-2.4.7 mijazz@lenovo  ~/devEnvs  tree -L 1 ./spark-2.4.7 ./spark-2.4.7├── LICENSE├── NOTICE├── R├── README.md├── RELEASE├── bin├── conf├── data├── examples├── jars├── kubernetes├── licenses├── python├── sbin└── yarn11 directories, 4 filesConfigure Spark老样子, 变量~/.zshrc, ~/.bashrc# Spark Environment Variableexport SPARK_HOME=&quot;/home/mijazz/devEnvs/spark-2.4.7&quot;export PATH=&quot;$PATH:${SPARK_HOME}/bin:${SPARK_HOME}/sbin&quot;export HADOOP_CONF_DIR=&quot;${HADOOP_HOME}/etc/hadoop&quot;export SPARK_DIST_CLASSPATH=&quot;$(hadoop classpath)&quot;spark下也有conf, 看一眼 mijazz@lenovo  ~/devEnvs/spark-2.4.7  tree ./conf./conf├── docker.properties.template├── fairscheduler.xml.template├── log4j.properties.template├── metrics.properties.template├── slaves.template├── spark-defaults.conf.template└── spark-env.sh.template0 directories, 7 files这些template里面都写着spark的默认配置. mijazz@lenovo  ~/devEnvs/spark-2.4.7  cat ./conf/spark-defaults.conf.template # ...# Example:# spark.master spark://master:7077# spark.eventLog.enabled true# spark.eventLog.dir hdfs://namenode:8021/directory# spark.serializer org.apache.spark.serializer.KryoSerializer# spark.driver.memory 5g# spark.executor.extraJavaOptions -XX:+PrintGCDetails -Dkey=value -Dnumbers=&quot;one two three&quot;但是默认的spark是有pre-built with hadoop的. 这次我是采用的自己的hadoop分离开搭建, 以便spark的RDD和hadoop的MapReduce我都能分开用.所以这次的spark的运行模式是yarn -&amp;gt; YARN on Hadoop, 所以spark.master字段要改yarn直接上配置spark-defaults.confspark.master yarnspark.eventLog.enabled true# 如果你在定义hadoop的hdfs时采用了自定义端口, 在这里更改spark.eventLog.dir hdfs://localhost:9000/tmp/spark-logsspark.history.provider org.apache.spark.deploy.history.FsHistoryProviderspark.history.fs.logDirectory hdfs://localhost:9000/tmp/spark-logsspark.history.fs.update.interval 10sspark.history.ui.port 18080历史记录应该是可以记录在本地的, 但是为了方便, 此处将其一共上传至hdfs, 方便追溯job history.start-history-server.sh可能遇到的坑 hdfs里的/tmp权限默认应该是可写的, 但是有可能在拉起记录进程的时候, 他访问文件夹的时候, 空的时候它不去创建. ✘ mijazz@lenovo  ~/devEnvs/spark-2.4.7  start-history-server.shstarting org.apache.spark.deploy.history.HistoryServer, logging to /home/mijazz/devEnvs/spark-2.4.7/logs/spark-mijazz-org.apache.spark.deploy.history.HistoryServer-1-lenovo.outfailed to launch: nice -n 0 /home/mijazz/devEnvs/spark-2.4.7/bin/spark-class org.apache.spark.deploy.history.HistoryServer at org.apache.spark.deploy.history.FsHistoryProvider.&amp;lt;init&amp;gt;(FsHistoryProvider.scala:207) at org.apache.spark.deploy.history.FsHistoryProvider.&amp;lt;init&amp;gt;(FsHistoryProvider.scala:86) ... 6 more Caused by: java.io.FileNotFoundException: File does not exist: hdfs://localhost:9000/spark-logs at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1586) at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1579) at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1594) at org.apache.spark.deploy.history.FsHistoryProvider.org$apache$spark$deploy$history$FsHistoryProvider$$startPolling(FsHistoryProvider.scala:257) ... 9 more用hdfs dfs -mkdir /tmp/spark-logs再hdfs dfs -ls /tmp确认一下7xx权限即可.Running Spark spark-submit cluster client模式, cluster模式.先理解一下spark的运行结构测试一下http://your.ip.or.host:18080查看history server能不能拉起. 因为其也是作为yarn运行hadoop上的, 所以此处的ip应该是master的.按照包里给出的example jar, 用spark-submit来提交jar包运行.spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode client $SPARK_HOME/examples/jars/spark-examples_2.11-2.4.7.jar 10至于cluster模式和client模式, 主要看spark driver运行在哪一侧. 如果是cluster模式, 在该次作业中, spark会把driver也交给yarn master来运行.2020-11-29 15:56:51,224 INFO scheduler.DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 0.920297 sPi is roughly 3.1402631402631402如果成功, 可以看到有该行输出. 记得| grep &quot;Pi is roughly&quot;.同时也将会在Spark History Server即18080端口, 和Yarn Cluster即8088端口看见yarn spark的运行记录以及event logs.参考运行 mijazz@lenovo  ~/devEnvs/spark-2.4.7  spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode client $SPARK_HOME/examples/jars/spark-examples_2.11-2.4.7.jar 10 2020-11-29 15:56:38,173 WARN util.Utils: Your hostname, lenovo resolves to a loopback address: 127.0.0.1; using 192.168.123.2 instead (on interface enp4s0)2020-11-29 15:56:38,173 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address2020-11-29 15:56:38,404 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable2020-11-29 15:56:38,541 INFO spark.SparkContext: Running Spark version 2.4.72020-11-29 15:56:38,556 INFO spark.SparkContext: Submitted application: Spark Pi2020-11-29 15:56:38,593 INFO spark.SecurityManager: Changing view acls to: mijazz2020-11-29 15:56:38,593 INFO spark.SecurityManager: Changing modify acls to: mijazz2020-11-29 15:56:38,593 INFO spark.SecurityManager: Changing view acls groups to: 2020-11-29 15:56:38,593 INFO spark.SecurityManager: Changing modify acls groups to: 2020-11-29 15:56:38,594 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(mijazz); groups with view permissions: Set(); users with modify permissions: Set(mijazz); groups with modify permissions: Set()2020-11-29 15:56:38,771 INFO util.Utils: Successfully started service &#39;sparkDriver&#39; on port 39113.2020-11-29 15:56:38,790 INFO spark.SparkEnv: Registering MapOutputTracker2020-11-29 15:56:38,802 INFO spark.SparkEnv: Registering BlockManagerMaster2020-11-29 15:56:38,804 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information2020-11-29 15:56:38,805 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up2020-11-29 15:56:38,811 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-01fd513c-7e08-401b-b6ea-46a0a268accf2020-11-29 15:56:38,823 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB2020-11-29 15:56:38,857 INFO spark.SparkEnv: Registering OutputCommitCoordinator2020-11-29 15:56:38,915 INFO util.log: Logging initialized @1320ms2020-11-29 15:56:38,955 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2019-08-14T05:28:18+08:00, git hash: 84700530e645e812b336747464d6fbbf370c9a202020-11-29 15:56:38,972 INFO server.Server: Started @1378ms2020-11-29 15:56:38,989 INFO server.AbstractConnector: Started ServerConnector@33aa93c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}2020-11-29 15:56:38,989 INFO util.Utils: Successfully started service &#39;SparkUI&#39; on port 4040.2020-11-29 15:56:39,006 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6bd51ed8{/jobs,null,AVAILABLE,@Spark}2020-11-29 15:56:39,007 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3b65e559{/jobs/json,null,AVAILABLE,@Spark}2020-11-29 15:56:39,007 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@bae47a0{/jobs/job,null,AVAILABLE,@Spark}2020-11-29 15:56:39,009 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1c05a54d{/jobs/job/json,null,AVAILABLE,@Spark}2020-11-29 15:56:39,010 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65ef722a{/stages,null,AVAILABLE,@Spark}2020-11-29 15:56:39,010 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fd9b663{/stages/json,null,AVAILABLE,@Spark}2020-11-29 15:56:39,011 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@214894fc{/stages/stage,null,AVAILABLE,@Spark}2020-11-29 15:56:39,012 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1c4ee95c{/stages/stage/json,null,AVAILABLE,@Spark}2020-11-29 15:56:39,012 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@79c4715d{/stages/pool,null,AVAILABLE,@Spark}2020-11-29 15:56:39,013 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5aa360ea{/stages/pool/json,null,AVAILABLE,@Spark}2020-11-29 15:56:39,013 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6548bb7d{/storage,null,AVAILABLE,@Spark}2020-11-29 15:56:39,013 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@e27ba81{/storage/json,null,AVAILABLE,@Spark}2020-11-29 15:56:39,014 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54336c81{/storage/rdd,null,AVAILABLE,@Spark}2020-11-29 15:56:39,014 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1556f2dd{/storage/rdd/json,null,AVAILABLE,@Spark}2020-11-29 15:56:39,015 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@35e52059{/environment,null,AVAILABLE,@Spark}2020-11-29 15:56:39,015 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62577d6{/environment/json,null,AVAILABLE,@Spark}2020-11-29 15:56:39,016 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@49bd54f7{/executors,null,AVAILABLE,@Spark}2020-11-29 15:56:39,016 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6b5f8707{/executors/json,null,AVAILABLE,@Spark}2020-11-29 15:56:39,017 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@772485dd{/executors/threadDump,null,AVAILABLE,@Spark}2020-11-29 15:56:39,017 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5a12c728{/executors/threadDump/json,null,AVAILABLE,@Spark}2020-11-29 15:56:39,022 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@79ab3a71{/static,null,AVAILABLE,@Spark}2020-11-29 15:56:39,023 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5a772895{/,null,AVAILABLE,@Spark}2020-11-29 15:56:39,024 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@39fc6b2c{/api,null,AVAILABLE,@Spark}2020-11-29 15:56:39,024 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7cc9ce8{/jobs/job/kill,null,AVAILABLE,@Spark}2020-11-29 15:56:39,025 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2e27d72f{/stages/stage/kill,null,AVAILABLE,@Spark}2020-11-29 15:56:39,026 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://lenovo.lan:40402020-11-29 15:56:39,035 INFO spark.SparkContext: Added JAR file:/home/mijazz/devEnvs/spark-2.4.7/examples/jars/spark-examples_2.11-2.4.7.jar at spark://lenovo.lan:39113/jars/spark-examples_2.11-2.4.7.jar with timestamp 16066365990352020-11-29 15:56:39,634 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:80322020-11-29 15:56:39,880 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers2020-11-29 15:56:39,933 INFO conf.Configuration: resource-types.xml not found2020-11-29 15:56:39,933 INFO resource.ResourceUtils: Unable to find &#39;resource-types.xml&#39;.2020-11-29 15:56:39,945 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)2020-11-29 15:56:39,946 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead2020-11-29 15:56:39,946 INFO yarn.Client: Setting up container launch context for our AM2020-11-29 15:56:39,948 INFO yarn.Client: Setting up the launch environment for our AM container2020-11-29 15:56:39,951 INFO yarn.Client: Preparing resources for our AM container2020-11-29 15:56:39,978 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.2020-11-29 15:56:40,534 INFO yarn.Client: Uploading resource file:/tmp/spark-49c12823-b6a4-4c2e-b397-d77a78188b8d/__spark_libs__1519230271046889967.zip -&amp;gt; hdfs://localhost:9000/user/mijazz/.sparkStaging/application_1606634326109_0003/__spark_libs__1519230271046889967.zip2020-11-29 15:56:41,208 INFO yarn.Client: Uploading resource file:/tmp/spark-49c12823-b6a4-4c2e-b397-d77a78188b8d/__spark_conf__3120810522893336741.zip -&amp;gt; hdfs://localhost:9000/user/mijazz/.sparkStaging/application_1606634326109_0003/__spark_conf__.zip2020-11-29 15:56:41,266 INFO spark.SecurityManager: Changing view acls to: mijazz2020-11-29 15:56:41,266 INFO spark.SecurityManager: Changing modify acls to: mijazz2020-11-29 15:56:41,266 INFO spark.SecurityManager: Changing view acls groups to: 2020-11-29 15:56:41,266 INFO spark.SecurityManager: Changing modify acls groups to: 2020-11-29 15:56:41,266 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(mijazz); groups with view permissions: Set(); users with modify permissions: Set(mijazz); groups with modify permissions: Set()2020-11-29 15:56:42,004 INFO yarn.Client: Submitting application application_1606634326109_0003 to ResourceManager2020-11-29 15:56:42,039 INFO impl.YarnClientImpl: Submitted application application_1606634326109_00032020-11-29 15:56:42,041 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1606634326109_0003 and attemptId None2020-11-29 15:56:43,046 INFO yarn.Client: Application report for application_1606634326109_0003 (state: ACCEPTED)2020-11-29 15:56:43,048 INFO yarn.Client: client token: N/A diagnostics: AM container is launched, waiting for AM container to Register with RM ApplicationMaster host: N/A ApplicationMaster RPC port: -1 queue: default start time: 1606636602015 final status: UNDEFINED tracking URL: http://localhost:8088/proxy/application_1606634326109_0003/ user: mijazz2020-11-29 15:56:44,050 INFO yarn.Client: Application report for application_1606634326109_0003 (state: ACCEPTED)2020-11-29 15:56:45,052 INFO yarn.Client: Application report for application_1606634326109_0003 (state: ACCEPTED)2020-11-29 15:56:45,963 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -&amp;gt; localhost, PROXY_URI_BASES -&amp;gt; http://localhost:8088/proxy/application_1606634326109_0003), /proxy/application_1606634326109_00032020-11-29 15:56:46,054 INFO yarn.Client: Application report for application_1606634326109_0003 (state: RUNNING)2020-11-29 15:56:46,054 INFO yarn.Client: client token: N/A diagnostics: N/A ApplicationMaster host: 192.168.123.2 ApplicationMaster RPC port: -1 queue: default start time: 1606636602015 final status: UNDEFINED tracking URL: http://localhost:8088/proxy/application_1606634326109_0003/ user: mijazz2020-11-29 15:56:46,055 INFO cluster.YarnClientSchedulerBackend: Application application_1606634326109_0003 has started running.2020-11-29 15:56:46,061 INFO util.Utils: Successfully started service &#39;org.apache.spark.network.netty.NettyBlockTransferService&#39; on port 37581.2020-11-29 15:56:46,061 INFO netty.NettyBlockTransferService: Server created on lenovo.lan:375812020-11-29 15:56:46,062 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy2020-11-29 15:56:46,079 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, lenovo.lan, 37581, None)2020-11-29 15:56:46,080 INFO storage.BlockManagerMasterEndpoint: Registering block manager lenovo.lan:37581 with 366.3 MB RAM, BlockManagerId(driver, lenovo.lan, 37581, None)2020-11-29 15:56:46,082 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, lenovo.lan, 37581, None)2020-11-29 15:56:46,083 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, lenovo.lan, 37581, None)2020-11-29 15:56:46,143 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)2020-11-29 15:56:46,205 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.2020-11-29 15:56:46,210 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9e02f84{/metrics/json,null,AVAILABLE,@Spark}2020-11-29 15:56:46,306 INFO scheduler.EventLoggingListener: Logging events to hdfs://localhost:9000/tmp/spark-logs/application_1606634326109_00032020-11-29 15:56:49,276 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.123.2:37340) with ID 12020-11-29 15:56:49,394 INFO storage.BlockManagerMasterEndpoint: Registering block manager localhost:35819 with 366.3 MB RAM, BlockManagerId(1, localhost, 35819, None)2020-11-29 15:56:49,976 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.123.2:37344) with ID 22020-11-29 15:56:50,034 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.82020-11-29 15:56:50,165 INFO storage.BlockManagerMasterEndpoint: Registering block manager localhost:40629 with 366.3 MB RAM, BlockManagerId(2, localhost, 40629, None)2020-11-29 15:56:50,304 INFO spark.SparkContext: Starting job: reduce at SparkPi.scala:382020-11-29 15:56:50,319 INFO scheduler.DAGScheduler: Got job 0 (reduce at SparkPi.scala:38) with 10 output partitions2020-11-29 15:56:50,320 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (reduce at SparkPi.scala:38)2020-11-29 15:56:50,321 INFO scheduler.DAGScheduler: Parents of final stage: List()2020-11-29 15:56:50,321 INFO scheduler.DAGScheduler: Missing parents: List()2020-11-29 15:56:50,325 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34), which has no missing parents2020-11-29 15:56:50,436 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.0 KB, free 366.3 MB)2020-11-29 15:56:50,454 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1381.0 B, free 366.3 MB)2020-11-29 15:56:50,456 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on lenovo.lan:37581 (size: 1381.0 B, free: 366.3 MB)2020-11-29 15:56:50,459 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:11842020-11-29 15:56:50,471 INFO scheduler.DAGScheduler: Submitting 10 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))2020-11-29 15:56:50,471 INFO cluster.YarnScheduler: Adding task set 0.0 with 10 tasks2020-11-29 15:56:50,496 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor 2, partition 0, PROCESS_LOCAL, 7877 bytes)2020-11-29 15:56:50,500 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor 1, partition 1, PROCESS_LOCAL, 7877 bytes)2020-11-29 15:56:50,783 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:35819 (size: 1381.0 B, free: 366.3 MB)2020-11-29 15:56:50,976 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:40629 (size: 1381.0 B, free: 366.3 MB)2020-11-29 15:56:51,003 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor 1, partition 2, PROCESS_LOCAL, 7877 bytes)2020-11-29 15:56:51,008 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 509 ms on localhost (executor 1) (1/10)2020-11-29 15:56:51,039 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, executor 1, partition 3, PROCESS_LOCAL, 7877 bytes)2020-11-29 15:56:51,042 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 40 ms on localhost (executor 1) (2/10)2020-11-29 15:56:51,075 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, localhost, executor 1, partition 4, PROCESS_LOCAL, 7877 bytes)2020-11-29 15:56:51,078 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 39 ms on localhost (executor 1) (3/10)2020-11-29 15:56:51,110 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, localhost, executor 2, partition 5, PROCESS_LOCAL, 7877 bytes)2020-11-29 15:56:51,112 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 627 ms on localhost (executor 2) (4/10)2020-11-29 15:56:51,122 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, localhost, executor 1, partition 6, PROCESS_LOCAL, 7877 bytes)2020-11-29 15:56:51,123 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 47 ms on localhost (executor 1) (5/10)2020-11-29 15:56:51,166 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, localhost, executor 1, partition 7, PROCESS_LOCAL, 7877 bytes)2020-11-29 15:56:51,171 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 49 ms on localhost (executor 1) (6/10)2020-11-29 15:56:51,171 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8, localhost, executor 2, partition 8, PROCESS_LOCAL, 7877 bytes)2020-11-29 15:56:51,172 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 62 ms on localhost (executor 2) (7/10)2020-11-29 15:56:51,187 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9, localhost, executor 1, partition 9, PROCESS_LOCAL, 7877 bytes)2020-11-29 15:56:51,188 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 22 ms on localhost (executor 1) (8/10)2020-11-29 15:56:51,213 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 42 ms on localhost (executor 2) (9/10)2020-11-29 15:56:51,219 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 33 ms on localhost (executor 1) (10/10)2020-11-29 15:56:51,220 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 2020-11-29 15:56:51,221 INFO scheduler.DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in 0.868 s2020-11-29 15:56:51,224 INFO scheduler.DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 0.920297 sPi is roughly 3.14026314026314022020-11-29 15:56:51,235 INFO server.AbstractConnector: Stopped Spark@33aa93c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}2020-11-29 15:56:51,237 INFO ui.SparkUI: Stopped Spark web UI at http://lenovo.lan:40402020-11-29 15:56:51,240 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread2020-11-29 15:56:51,263 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors2020-11-29 15:56:51,264 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down2020-11-29 15:56:51,268 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices(serviceOption=None, services=List(), started=false)2020-11-29 15:56:51,268 INFO cluster.YarnClientSchedulerBackend: Stopped2020-11-29 15:56:51,364 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!2020-11-29 15:56:51,371 INFO memory.MemoryStore: MemoryStore cleared2020-11-29 15:56:51,371 INFO storage.BlockManager: BlockManager stopped2020-11-29 15:56:51,374 INFO storage.BlockManagerMaster: BlockManagerMaster stopped2020-11-29 15:56:51,376 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!2020-11-29 15:56:51,398 INFO spark.SparkContext: Successfully stopped SparkContext2020-11-29 15:56:51,400 INFO util.ShutdownHookManager: Shutdown hook called2020-11-29 15:56:51,401 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-5c235e44-e3d3-4d12-923c-a635b9143c392020-11-29 15:56:51,403 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-49c12823-b6a4-4c2e-b397-d77a78188b8dData Practicing-EP3Introduce Spark 这里贴出几个官方文档 Spark Overview Java API Docs Scala API Docs Spark SQL Docs这里只记录一下SparkRDD, RDD -&amp;gt; Resilient Distributed Datasets.它是一种可扩展的弹性分布式数据集, 他是只读的, 分区的, 并且保持不变的数据集合, 直接与在内存层面的一个分布式实现. 可分区/片(默认好象是Hash分区?) 可自定义分片计算函数 互相依赖(下个分区由之前的分区通过转换生成) 可控制分片数量 可以使用列表方式进行块储存它支持两种类型的操作 Transformations map() flatMap() filter() union() intersection() …… Actions reduce() collect() count() …… RDD Operations Examples 以下Code Block均为在Spark-shell下执行的结果 Spark-&amp;gt; 2.4.7 Yarn on Hadoop 3.1.4 Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_275)scala&amp;gt; val data = Array(2, 3, 5, 7, 11)data: Array[Int] = Array(2, 3, 5, 7, 11)scala&amp;gt; val rdd1 = sc.parallelize(data)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &amp;lt;console&amp;gt;:26scala&amp;gt; val rdd2 = rdd1.map(element =&amp;gt; (element*2, element*element)).collect()rdd2: Array[(Int, Int)] = Array((4,4), (6,9), (10,25), (14,49), (22,121))scala&amp;gt; val rdd3 = rdd1.union(rdd1)rdd3: org.apache.spark.rdd.RDD[Int] = UnionRDD[3] at union at &amp;lt;console&amp;gt;:25scala&amp;gt; rdd3.collect()res4: Array[Int] = Array(2, 3, 5, 7, 11, 2, 3, 5, 7, 11)scala&amp;gt; rdd3.sortBy(x =&amp;gt; x%8, ascending=false).collect()res5: Array[Int] = Array(7, 7, 5, 5, 11, 3, 3, 11, 2, 2)scala&amp;gt; rdd3.count()res6: Long = 10scala&amp;gt; rdd3.take(3)res7: Array[Int] = Array(2, 3, 5)scala&amp;gt; rdd3.distinct().collect()res8: Array[Int] = Array(5, 2, 11, 3, 7)........加上之前我们在hadoop里运行的HDFS, Spark可以很方便的通过hdfs://ip.or.host:port/path/to/file来访问hdfs的文件.也可以使用spark sql在处理数据.Prepare to Code网上太多教材关于Spark + Scala + IntelliJ IDEA + sbt四大件的了贴几个教程链接 IntelliJ IDEA sbt IntelliJ IDEA Scala Scala Official - Dev with IDEA Tutorial 1 Tutorial-2包管理对于我来说, 还是更熟悉Java的那一套, 毕竟Spring用多了. 不是Maven就是Gradle.镜像设置过程不表, 见Aliyun Maven Mirror Manjaro Linux 20 IntelliJ IDEA Ultimate 2020.2.3 Maven(bundled with idea, 3.6.3) Install Scala Plugin in IDEA(!important)常规IDEA建立Maven的Project, 依赖如下pom.xml &amp;lt;dependencies&amp;gt; &amp;lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core --&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spark-core_2.11&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;2.4.7&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-sql --&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spark-sql_2.11&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;2.4.7&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;/dependencies&amp;gt;如果spark版本不同, 去mvnrepository搜索对应的依赖, 粘贴进依赖区即可.Sync一下, Maven即可解决依赖问题. 而后在工程下右键, Add Framework Support, 加入该工程对Scala的支持.(该步骤需要有Scala Plugin)常规建包建类即可, 选Scala Class -&amp;gt; Object 参考Naming Conventions我的步骤 $Project Root/src/main/java新建package -&amp;gt; edu.zstu.mijazz.sparklearn 包下建类Scala Object -&amp;gt; HelloWorld package edu.zstu.mijazz.sparklearn1 import org.apache.spark.sql.SparkSession import scala.math.random object HelloWorld { def main(args: Array[String]): Unit = { val spark = SparkSession.builder.appName(&quot;Spark Pi&quot;).master(&quot;local&quot;).getOrCreate() val count = spark.sparkContext.parallelize(1 until 50000000, 3).map {_ =&amp;gt; val x = random * 2 - 1 val y = random * 2 - 1 if (x*x + y*y &amp;lt;= 1) 1 else 0 }.reduce(_ + _) println(s&quot;Pi is roughly ${4.0 * count / (50000000 - 1)}&quot;) spark.stop() spark.close() }} 直接建object, 执行时对象初始化触发对象main(), 至于Scala的语法和资料, 见Scala Docs 如果输出没问题 Using Spark&#39;s default log4j profile: org/apache/spark/log4j-defaults.properties20/11/29 22:21:24 INFO SparkContext: Running Spark version 2.4.720/11/29 22:21:24 INFO SparkContext: Submitted application: Spark Pi20/11/29 22:21:24 INFO SecurityManager: Changing view acls to: mijazz20/11/29 22:21:24 INFO SecurityManager: Changing modify acls to: mijazz20/11/29 22:21:24 INFO SecurityManager: Changing view acls groups to: 20/11/29 22:21:24 INFO SecurityManager: Changing modify acls groups to: 20/11/29 22:21:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(mijazz); groups with view permissions: Set(); users with modify permissions: Set(mijazz); groups with modify permissions: Set()20/11/29 22:21:24 INFO Utils: Successfully started service &#39;sparkDriver&#39; on port 46007.20/11/29 22:21:24 INFO SparkEnv: Registering MapOutputTracker20/11/29 22:21:24 INFO SparkEnv: Registering BlockManagerMaster20/11/29 22:21:25 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information20/11/29 22:21:25 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up20/11/29 22:21:25 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-be049433-3f00-4037-a865-67cd6f445fba20/11/29 22:21:25 INFO MemoryStore: MemoryStore started with capacity 1941.6 MB20/11/29 22:21:25 INFO SparkEnv: Registering OutputCommitCoordinator20/11/29 22:21:25 INFO Utils: Successfully started service &#39;SparkUI&#39; on port 4040.20/11/29 22:21:25 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://lenovo.lan:404020/11/29 22:21:25 INFO Executor: Starting executor ID driver on host localhost20/11/29 22:21:25 INFO Utils: Successfully started service &#39;org.apache.spark.network.netty.NettyBlockTransferService&#39; on port 44811.20/11/29 22:21:25 INFO NettyBlockTransferService: Server created on lenovo.lan:4481120/11/29 22:21:25 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy20/11/29 22:21:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, lenovo.lan, 44811, None)20/11/29 22:21:25 INFO BlockManagerMasterEndpoint: Registering block manager lenovo.lan:44811 with 1941.6 MB RAM, BlockManagerId(driver, lenovo.lan, 44811, None)20/11/29 22:21:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, lenovo.lan, 44811, None)20/11/29 22:21:25 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, lenovo.lan, 44811, None)20/11/29 22:21:25 INFO SparkContext: Starting job: reduce at HelloWorld.scala:1520/11/29 22:21:26 INFO DAGScheduler: Got job 0 (reduce at HelloWorld.scala:15) with 3 output partitions20/11/29 22:21:26 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at HelloWorld.scala:15)20/11/29 22:21:26 INFO DAGScheduler: Parents of final stage: List()20/11/29 22:21:26 INFO DAGScheduler: Missing parents: List()20/11/29 22:21:26 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at HelloWorld.scala:11), which has no missing parents20/11/29 22:21:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.0 KB, free 1941.6 MB)20/11/29 22:21:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1401.0 B, free 1941.6 MB)20/11/29 22:21:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on lenovo.lan:44811 (size: 1401.0 B, free: 1941.6 MB)20/11/29 22:21:26 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:118420/11/29 22:21:26 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at HelloWorld.scala:11) (first 15 tasks are for partitions Vector(0, 1, 2))20/11/29 22:21:26 INFO TaskSchedulerImpl: Adding task set 0.0 with 3 tasks20/11/29 22:21:26 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7866 bytes)20/11/29 22:21:26 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)20/11/29 22:21:27 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 867 bytes result sent to driver20/11/29 22:21:27 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7866 bytes)20/11/29 22:21:27 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)20/11/29 22:21:27 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1038 ms on localhost (executor driver) (1/3)20/11/29 22:21:28 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 867 bytes result sent to driver20/11/29 22:21:28 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 7866 bytes)20/11/29 22:21:28 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)20/11/29 22:21:28 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 998 ms on localhost (executor driver) (2/3)20/11/29 22:21:29 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 867 bytes result sent to driver20/11/29 22:21:29 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 991 ms on localhost (executor driver) (3/3)20/11/29 22:21:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 20/11/29 22:21:29 INFO DAGScheduler: ResultStage 0 (reduce at HelloWorld.scala:15) finished in 3.169 s20/11/29 22:21:29 INFO DAGScheduler: Job 0 finished: reduce at HelloWorld.scala:15, took 3.204133 s20/11/29 22:21:29 INFO SparkUI: Stopped Spark web UI at http://lenovo.lan:4040Pi is roughly 3.14149166282983320/11/29 22:21:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!20/11/29 22:21:29 INFO MemoryStore: MemoryStore cleared20/11/29 22:21:29 INFO BlockManager: BlockManager stopped20/11/29 22:21:29 INFO BlockManagerMaster: BlockManagerMaster stopped20/11/29 22:21:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!20/11/29 22:21:29 INFO SparkContext: Successfully stopped SparkContext20/11/29 22:21:29 INFO SparkContext: SparkContext already stopped.20/11/29 22:21:29 INFO ShutdownHookManager: Shutdown hook called20/11/29 22:21:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-701b3922-5c91-4ada-80de-0319be2db7e3 能够跑出结果, 说明在IDEA中直接使用scala与spark交互已经没问题了. 现在开始找数据集试试DataFrameData Practicing-EP4Find Data Chicago Crime Data is from CHICAGO DATA PORTAL Visit Here这次使用的是Chicago的Crime Data. 从2001年至最近的. mijazz@lenovo  ~/devEnvs  wc -l chicagoCrimeData.csv7212274 chicagoCrimeData.csv mijazz@lenovo  ~/devEnvs  head -n 2 ./chicagoCrimeData.csv ID,Case Number,Date,Block,IUCR,Primary Type,Description,Location Description,Arrest,Domestic,Beat,District,Ward,Community Area,FBI Code,X Coordinate,Y Coordinate,Year,Updated On,Latitude,Longitude,Location11034701,JA366925,01/01/2001 11:00:00 AM,016XX E 86TH PL,1153,DECEPTIVE PRACTICE,FINANCIAL IDENTITY THEFT OVER $ 300,RESIDENCE,false,false,0412,004,8,45,11,,,2001,08/05/2017 03:50:08 PM,,, mijazz@lenovo  ~/devEnvs  tail -n 2 ./chicagoCrimeData.csv 11707239,JC287563,11/30/2017 09:00:00 AM,022XX S KOSTNER AVE,1153,DECEPTIVE PRACTICE,FINANCIAL IDENTITY THEFT OVER $ 300,RESIDENCE,false,false,1013,010,22,29,11,,,2017,06/02/2019 04:09:42 PM,,,24559,JC278908,05/26/2019 02:11:00 AM,013XX W HASTINGS ST,0110,HOMICIDE,FIRST DEGREE MURDER,STREET,false,false,1233,012,25,28,01A,1167746,1893853,2019,06/20/2020 03:48:45 PM,41.864278357,-87.659682244,&quot;(41.864278357, -87.659682244)&quot;共7,212,274行数据, 每行数据代表一次记录在案的犯罪信息.部分列描述如下 ID - Unique Row ID Case Number - Unique Chicago Police Department Records Division Number, Unique Date Block - Address IUCR - Illinois Uniform Crime Reporting CodeCode Referrence Primary Type - IUCR Code/Crime Description Description - Crime Description Location Description Arrest - Arrest made or not Community Area - Community Area Code Code Referrence Location - (Latitude, Longitude)Move to HDFS 前面说过hdfs的提供的交互shell很像Unix/Linux的文件系统交互. 文档如下: File System Shell 或者 hdfs dfs -help mijazz@lenovo  ~/devEnvs  ll -atotal 1.8Gdrwxr-xr-x 10 mijazz mijazz 4.0K Nov 29 22:45 .drwx------ 42 mijazz mijazz 4.0K Nov 30 15:42 ..drwxr-xr-x 8 mijazz mijazz 4.0K Nov 9 20:23 OpenJDK8-rwxrwxrwx 1 mijazz mijazz 1.6G Oct 19 18:05 chicagoCrimeData.csvdrwxr-xr-x 11 mijazz mijazz 4.0K Nov 25 23:49 hadoop-3.1.4drwxr-xr-x 14 mijazz mijazz 4.0K Nov 29 15:25 spark-2.4.7-rw-r--r-- 1 mijazz mijazz 161M Nov 24 16:49 spark-2.4.7-bin-without-hadoop.tgz mijazz@lenovo  ~/devEnvs  hdfs dfs -mkdir /user/mijazz/chicagoData mijazz@lenovo  ~/devEnvs  hdfs dfs -put ./chicagoCrimeData.csv /user/mijazz/chicagoData/originCrimeData.csv mijazz@lenovo  ~/devEnvs  hdfs dfs -ls /user/mijazz/chicagoData Found 1 items-rw-r--r-- 1 mijazz supergroup 1701238602 2020-11-30 15:43 /user/mijazz/chicagoData/originCrimeData.csvdfs -put把文件上传上hdfs, 如果需要多用户读写, dfs -chmod给个666之后, 检查一下权限即可.上传之后, 在spark中就可以通过hdfs://your.ip.or.host:port/path/to/file来访问了.在我这里就是hdfs://localhost:9000/user/mijazz/chicagoData/originCrimeData.csvPre-Processing在EP3中配置好的IntelliJ IDEA的project, 新建一个Scala Object即可.前面几行代码都是必备的了 用SparkSession拉起一个Spark会话 Context负责数据Take a Glance at the Data Object + main()方法或者Object + extends App 当脚本用DataPreProcess.scala - 1package edu.zstu.mijazz.sparklearn1import org.apache.spark.sql.SparkSessionobject DataPreProcess { val HDFS_PATH = &quot;hdfs://localhost:9000/user/mijazz/&quot; val DATA_PATH = HDFS_PATH + &quot;chicagoData/&quot; def main(args: Array[String]): Unit = { val spark = SparkSession.builder.appName(&quot;Data Pre-Processing&quot;).master(&quot;local&quot;).getOrCreate() val sContext = spark.sparkContext val data = sContext.textFile(DATA_PATH + &quot;originCrimeData.csv&quot;) data.take(3).foreach(println) }}Full Output 后面的输出block, 我会把spark的输出手工砍掉. 当然你也可以在spark中配置比info级别高一些的log level, 但是留着便于我知道内存使用量.Using Spark&#39;s default log4j profile: org/apache/spark/log4j-defaults.properties20/11/30 16:00:51 WARN Utils: Your hostname, lenovo resolves to a loopback address: 127.0.0.1; using 192.168.123.2 instead (on interface enp4s0)20/11/30 16:00:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address20/11/30 16:00:51 INFO SparkContext: Running Spark version 2.4.720/11/30 16:00:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable20/11/30 16:00:51 INFO SparkContext: Submitted application: Data Pre-Processing20/11/30 16:00:51 INFO SecurityManager: Changing view acls to: mijazz20/11/30 16:00:51 INFO SecurityManager: Changing modify acls to: mijazz20/11/30 16:00:51 INFO SecurityManager: Changing view acls groups to: 20/11/30 16:00:51 INFO SecurityManager: Changing modify acls groups to: 20/11/30 16:00:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(mijazz); groups with view permissions: Set(); users with modify permissions: Set(mijazz); groups with modify permissions: Set()20/11/30 16:00:52 INFO Utils: Successfully started service &#39;sparkDriver&#39; on port 35377.20/11/30 16:00:52 INFO SparkEnv: Registering MapOutputTracker20/11/30 16:00:52 INFO SparkEnv: Registering BlockManagerMaster20/11/30 16:00:52 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information20/11/30 16:00:52 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up20/11/30 16:00:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-283f5487-ed7e-41b8-92ae-20d56fb33ba520/11/30 16:00:52 INFO MemoryStore: MemoryStore started with capacity 1941.6 MB20/11/30 16:00:52 INFO SparkEnv: Registering OutputCommitCoordinator20/11/30 16:00:52 INFO Utils: Successfully started service &#39;SparkUI&#39; on port 4040.20/11/30 16:00:52 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://lenovo.lan:404020/11/30 16:00:52 INFO Executor: Starting executor ID driver on host localhost20/11/30 16:00:52 INFO Utils: Successfully started service &#39;org.apache.spark.network.netty.NettyBlockTransferService&#39; on port 37555.20/11/30 16:00:52 INFO NettyBlockTransferService: Server created on lenovo.lan:3755520/11/30 16:00:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy20/11/30 16:00:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, lenovo.lan, 37555, None)20/11/30 16:00:52 INFO BlockManagerMasterEndpoint: Registering block manager lenovo.lan:37555 with 1941.6 MB RAM, BlockManagerId(driver, lenovo.lan, 37555, None)20/11/30 16:00:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, lenovo.lan, 37555, None)20/11/30 16:00:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, lenovo.lan, 37555, None)20/11/30 16:00:53 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1941.4 MB)20/11/30 16:00:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1941.4 MB)20/11/30 16:00:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on lenovo.lan:37555 (size: 20.4 KB, free: 1941.6 MB)20/11/30 16:00:53 INFO SparkContext: Created broadcast 0 from textFile at DataPreProcess.scala:1220/11/30 16:00:53 INFO FileInputFormat: Total input paths to process : 120/11/30 16:00:53 INFO SparkContext: Starting job: take at DataPreProcess.scala:1320/11/30 16:00:53 INFO DAGScheduler: Got job 0 (take at DataPreProcess.scala:13) with 1 output partitions20/11/30 16:00:53 INFO DAGScheduler: Final stage: ResultStage 0 (take at DataPreProcess.scala:13)20/11/30 16:00:53 INFO DAGScheduler: Parents of final stage: List()20/11/30 16:00:53 INFO DAGScheduler: Missing parents: List()20/11/30 16:00:53 INFO DAGScheduler: Submitting ResultStage 0 (hdfs://localhost:9000/user/mijazz/chicagoData/originCrimeData.csv MapPartitionsRDD[1] at textFile at DataPreProcess.scala:12), which has no missing parents20/11/30 16:00:53 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.6 KB, free 1941.4 MB)20/11/30 16:00:53 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1941.4 MB)20/11/30 16:00:53 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on lenovo.lan:37555 (size: 2.2 KB, free: 1941.6 MB)20/11/30 16:00:53 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:118420/11/30 16:00:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (hdfs://localhost:9000/user/mijazz/chicagoData/originCrimeData.csv MapPartitionsRDD[1] at textFile at DataPreProcess.scala:12) (first 15 tasks are for partitions Vector(0))20/11/30 16:00:53 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks20/11/30 16:00:54 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7925 bytes)20/11/30 16:00:54 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)20/11/30 16:00:54 INFO HadoopRDD: Input split: hdfs://localhost:9000/user/mijazz/chicagoData/originCrimeData.csv:0+13421772820/11/30 16:00:54 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1371 bytes result sent to driver20/11/30 16:00:54 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 133 ms on localhost (executor driver) (1/1)20/11/30 16:00:54 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 20/11/30 16:00:54 INFO DAGScheduler: ResultStage 0 (take at DataPreProcess.scala:13) finished in 0.183 s20/11/30 16:00:54 INFO DAGScheduler: Job 0 finished: take at DataPreProcess.scala:13, took 0.217644 s20/11/30 16:00:54 INFO SparkContext: Invoking stop() from shutdown hookID,Case Number,Date,Block,IUCR,Primary Type,Description,Location Description,Arrest,Domestic,Beat,District,Ward,Community Area,FBI Code,X Coordinate,Y Coordinate,Year,Updated On,Latitude,Longitude,Location11034701,JA366925,01/01/2001 11:00:00 AM,016XX E 86TH PL,1153,DECEPTIVE PRACTICE,FINANCIAL IDENTITY THEFT OVER $ 300,RESIDENCE,false,false,0412,004,8,45,11,,,2001,08/05/2017 03:50:08 PM,,,11227287,JB147188,10/08/2017 03:00:00 AM,092XX S RACINE AVE,0281,CRIM SEXUAL ASSAULT,NON-AGGRAVATED,RESIDENCE,false,false,2222,022,21,73,02,,,2017,02/11/2018 03:57:41 PM,,,20/11/30 16:00:54 INFO SparkUI: Stopped Spark web UI at http://lenovo.lan:404020/11/30 16:00:54 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!20/11/30 16:00:54 INFO MemoryStore: MemoryStore cleared20/11/30 16:00:54 INFO BlockManager: BlockManager stopped20/11/30 16:00:54 INFO BlockManagerMaster: BlockManagerMaster stopped20/11/30 16:00:54 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!20/11/30 16:00:54 INFO SparkContext: Successfully stopped SparkContext20/11/30 16:00:54 INFO ShutdownHookManager: Shutdown hook called20/11/30 16:00:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-c6ba3bd3-3fc3-46b8-9e7e-3413981456ffProcess finished with exit code 0Useful OutputID,Case Number,Date,Block,IUCR,Primary Type,Description,Location Description,Arrest,Domestic,Beat,District,Ward,Community Area,FBI Code,X Coordinate,Y Coordinate,Year,Updated On,Latitude,Longitude,Location11034701,JA366925,01/01/2001 11:00:00 AM,016XX E 86TH PL,1153,DECEPTIVE PRACTICE,FINANCIAL IDENTITY THEFT OVER $ 300,RESIDENCE,false,false,0412,004,8,45,11,,,2001,08/05/2017 03:50:08 PM,,,11227287,JB147188,10/08/2017 03:00:00 AM,092XX S RACINE AVE,0281,CRIM SEXUAL ASSAULT,NON-AGGRAVATED,RESIDENCE,false,false,2222,022,21,73,02,,,2017,02/11/2018 03:57:41 PM,,,有不少空值.DataPreProcess.scala - 2package edu.zstu.mijazz.sparklearn1import org.apache.spark.sql.SparkSessionobject DataPreProcess extends App { val HDFS_PATH = &quot;hdfs://localhost:9000/user/mijazz/&quot; val DATA_PATH = HDFS_PATH + &quot;chicagoData/&quot; val spark = SparkSession.builder.appName(&quot;Data Pre-Processing&quot;).master(&quot;local&quot;).getOrCreate() val sContext = spark.sparkContext val crimeDataFrame = spark.read .option(&quot;header&quot;, true) .option(&quot;inferSchema&quot;, true) .csv(DATA_PATH + &quot;originCrimeData.csv&quot;) crimeDataFrame.show(3) crimeDataFrame.printSchema() spark.stop() spark.close()}+--------+-----------+--------------------+------------------+----+-------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+--------+---------+--------+| ID|Case Number| Date| Block|IUCR| Primary Type| Description|Location Description|Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|Year| Updated On|Latitude|Longitude|Location|+--------+-----------+--------------------+------------------+----+-------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+--------+---------+--------+|11034701| JA366925|01/01/2001 11:00:...| 016XX E 86TH PL|1153| DECEPTIVE PRACTICE|FINANCIAL IDENTIT...| RESIDENCE| false| false| 412| 4| 8| 45| 11| null| null|2001|08/05/2017 03:50:...| null| null| null||11227287| JB147188|10/08/2017 03:00:...|092XX S RACINE AVE|0281|CRIM SEXUAL ASSAULT| NON-AGGRAVATED| RESIDENCE| false| false|2222| 22| 21| 73| 02| null| null|2017|02/11/2018 03:57:...| null| null| null||11227583| JB147595|03/28/2017 02:00:...| 026XX W 79TH ST|0620| BURGLARY| UNLAWFUL ENTRY| OTHER| false| false| 835| 8| 18| 70| 05| null| null|2017|02/11/2018 03:57:...| null| null| null|+--------+-----------+--------------------+------------------+----+-------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+--------+---------+--------+only showing top 3 rowsroot |-- ID: integer (nullable = true) |-- Case Number: string (nullable = true) |-- Date: string (nullable = true) |-- Block: string (nullable = true) |-- IUCR: string (nullable = true) |-- Primary Type: string (nullable = true) |-- Description: string (nullable = true) |-- Location Description: string (nullable = true) |-- Arrest: boolean (nullable = true) |-- Domestic: boolean (nullable = true) |-- Beat: integer (nullable = true) |-- District: integer (nullable = true) |-- Ward: integer (nullable = true) |-- Community Area: integer (nullable = true) |-- FBI Code: string (nullable = true) |-- X Coordinate: integer (nullable = true) |-- Y Coordinate: integer (nullable = true) |-- Year: integer (nullable = true) |-- Updated On: string (nullable = true) |-- Latitude: double (nullable = true) |-- Longitude: double (nullable = true) |-- Location: string (nullable = true)Start with Data为了方便阅读, 多加了一个Java Class - StaticTool.java, 专门用来存静态数据.package edu.zstu.mijazz.sparklearn1;public class StaticTool { public static final String HDFS_PATH = &quot;hdfs://localhost:9000/user/mijazz/&quot;; public static final String DATA_PATH = HDFS_PATH + &quot;chicagoData/&quot;; public static final String ORIGIN_DATA = DATA_PATH + &quot;originCrimeData.csv&quot;; public static final String DATE_DATA = DATA_PATH + &quot;dateDF.csv&quot;;}Date Column万事就先从时间开始吧, 对Date字段先做个分析 val dateNullRowCount = crimeDataFrame.filter(&quot;Date is null&quot;).count() println(dateNullRowCount)0看到日期值并没有空列, 很好, 不用na.fill了.foucsDate.scala - 1package edu.zstu.mijazz.sparklearn1import org.apache.spark.sql.SparkSessionobject focusDate extends App { val spark = SparkSession.builder.appName(&quot;Data Pre-Processing&quot;).master(&quot;local&quot;).getOrCreate() val sContext = spark.sparkContext val crimeDataFrame = spark.read .option(&quot;header&quot;, true) .option(&quot;inferSchema&quot;, true) .csv(StaticTool.DATA_PATH + &quot;originCrimeData.csv&quot;) var dateNeedColumn = crimeDataFrame.select(&quot;Date&quot;, &quot;Primary Type&quot;, &quot;Year&quot;) dateNeedColumn.show(3) dateNeedColumn.printSchema()}+--------------------+-------------------+----+| Date| Primary Type|Year|+--------------------+-------------------+----+|01/01/2001 11:00:...| DECEPTIVE PRACTICE|2001||10/08/2017 03:00:...|CRIM SEXUAL ASSAULT|2017||03/28/2017 02:00:...| BURGLARY|2017|+--------------------+-------------------+----+only showing top 3 rowsroot |-- Date: string (nullable = true) |-- Primary Type: string (nullable = true) |-- Year: integer (nullable = true)看到Date字段居然是String类型. 做个castfoucsDate.scala - 2dateNeedColumn = dateNeedColumn .withColumn(&quot;TimeStamp&quot;, unix_timestamp( col(&quot;Date&quot;), &quot;MM/dd/yyyy HH:mm:ss&quot;).cast(&quot;timestamp&quot;)) .drop(&quot;Date&quot;) .withColumnRenamed(&quot;Primary Type&quot;, &quot;Crime&quot;) dateNeedColumn.show(3) dateNeedColumn.printSchema()+-------------------+----+-------------------+| Crime|Year| TimeStamp|+-------------------+----+-------------------+| DECEPTIVE PRACTICE|2001|2001-01-01 11:00:00||CRIM SEXUAL ASSAULT|2017|2017-10-08 03:00:00|| BURGLARY|2017|2017-03-28 02:00:00|+-------------------+----+-------------------+only showing top 3 rowsroot |-- Crime: string (nullable = true) |-- Year: integer (nullable = true) |-- TimeStamp: timestamp (nullable = true)把时间都砍出来, 日期或者月份拿来做汇总, 时间用来后期画图?focusDate.scala - 3 dateNeedColumn = dateNeedColumn .withColumn(&quot;Year&quot;, col(&quot;Year&quot;)) .withColumn(&quot;Month&quot;, col(&quot;TimeStamp&quot;).substr(0, 7)) .withColumn(&quot;Day&quot;, col(&quot;TimeStamp&quot;).substr(0, 10)) .withColumn(&quot;Hour&quot;, col(&quot;TimeStamp&quot;).substr(11, 3)) .withColumnRenamed(&quot;Location Description&quot;, &quot;Location&quot;) dateNeedColumn.show(5) dateNeedColumn.printSchema()+-------------------+----+-----------+-------------------+-------+----------+----+| Crime|Year| Location| TimeStamp| Month| Day|Hour|+-------------------+----+-----------+-------------------+-------+----------+----+| DECEPTIVE PRACTICE|2001| RESIDENCE|2001-01-01 11:00:00|2001-01|2001-01-01| 11||CRIM SEXUAL ASSAULT|2017| RESIDENCE|2017-10-08 03:00:00|2017-10|2017-10-08| 03|| BURGLARY|2017| OTHER|2017-03-28 02:00:00|2017-03|2017-03-28| 02|| THEFT|2017| RESIDENCE|2017-09-09 08:17:00|2017-09|2017-09-09| 08||CRIM SEXUAL ASSAULT|2017|HOTEL/MOTEL|2017-08-26 10:00:00|2017-08|2017-08-26| 10|+-------------------+----+-----------+-------------------+-------+----------+----+only showing top 5 rowsroot |-- Crime: string (nullable = true) |-- Year: integer (nullable = true) |-- Location: string (nullable = true) |-- TimeStamp: timestamp (nullable = true) |-- Month: string (nullable = true) |-- Day: string (nullable = true) |-- Hour: string (nullable = true)Write Data to CSVfocusDate.scala - 4 dateNeedColumn.write.option(&quot;header&quot;, true).csv(StaticTool.DATA_PATH + &quot;dateDF.csv&quot;)Crime Column提取完日期, 然后看看Crime列到底有多少种犯罪focusCrime.scala - 1package edu.zstu.mijazz.sparklearn1import org.apache.spark.sql.SparkSessionobject focusCrime extends App { val spark = SparkSession.builder.appName(&quot;Data Analysis&quot;).master(&quot;local&quot;).getOrCreate() val sContext = spark.sparkContext val data = spark.read .option(&quot;header&quot;, true) .option(&quot;inferSchema&quot;, true) .csv(StaticTool.DATE_DATA)// 取回focusDate.scala中转储在hdfs中的数据 var crimeColumnDataSet = data.select(&quot;Crime&quot;).distinct() crimeColumnDataSet.show(20) println(crimeColumnDataSet.count())}+--------------------+| Crime|+--------------------+|OFFENSE INVOLVING...||CRIMINAL SEXUAL A...|| STALKING||PUBLIC PEACE VIOL...|| OBSCENITY||NON-CRIMINAL (SUB...|| ARSON|| DOMESTIC VIOLENCE|| GAMBLING|| CRIMINAL TRESPASS|| ASSAULT|| NON - CRIMINAL||LIQUOR LAW VIOLATION|| MOTOR VEHICLE THEFT|| THEFT|| BATTERY|| ROBBERY|| HOMICIDE|| RITUALISM|| PUBLIC INDECENCY|+--------------------+only showing top 20 rows36总共36种不同的犯罪类型.Crime Summary(Spark SQL) DataFrame内操作也行, 抱着入门框架的心态, 硬上Spark SQL吧只看总的犯罪统计, 抓个靠前的十宗罪吧 这里留了点代码, 到时候往Hive里面写或者往MariaDB里面写, 换到pyspark画图方便些.focusCrime.scala - 2 注意的是: 要使用spark sql, dataframe或者rdd里面的东西要做成一个View, 就可以当成一个表做结构化查询了. data.createOrReplaceTempView(&quot;t_CrimeDate&quot;) val eachCrimeSummary = spark. sql(&quot;select Crime, count(1) Occurs &quot; + &quot;from t_CrimeDate &quot; + &quot;group by Crime&quot;) // For Writing in CSV or Hive DB in further PySpark Usage// eachCrimeSummary.write.option(&quot;header&quot;, true).csv(&quot;&quot;) eachCrimeSummary.orderBy(desc(&quot;Occurs&quot;)).show(10)+-------------------+-------+| Crime| Occurs|+-------------------+-------+| THEFT|1522618| # 偷窃| BATTERY|1321333| # 殴打| CRIMINAL DAMAGE| 821509| # 破坏(刑事)| NARCOTICS| 733993| # 毒品犯罪| ASSAULT| 456288| # 攻击| OTHER OFFENSE| 447617| # 其他侵犯| BURGLARY| 406317| # 非法入侵|MOTOR VEHICLE THEFT| 331980| # 盗窃车辆| DECEPTIVE PRACTICE| 297519| # 诈骗| ROBBERY| 270936| # 抢劫+-------------------+-------+Monthly Summary(Spark SQL)抓一下按月分类的, 看看数据是否有特征, 如果有特征就可以尝试后续做图.focusCrime.scala - 3 val groupByMonth = spark .sql(&quot;select month(Month) NaturalMonth, count(1) CrimePerMonth &quot; + &quot;from t_CrimeDate &quot; + &quot;group by NaturalMonth&quot;) groupByMonth.orderBy(desc(&quot;CrimePerMonth&quot;)).show(12)+------------+-------------+|NaturalMonth|CrimePerMonth|+------------+-------------+| 7| 675041|| 8| 668824|| 5| 644421|| 6| 641529|| 9| 625696|| 10| 620504|| 3| 594688|| 4| 593116|| 1| 568404|| 11| 553769|| 12| 525734|| 2| 500547|+------------+-------------+这里就有很明显的趋势了, 年中部分的犯罪数量明显比年尾年头高.Prepare External Data 天气数据见Weather Data Extraction上次抓天气数据, 把2001年到今年, 每年的数据都抓下来了, 数据格式是Date, High, Low mijazz@lenovo  ~/pyProjects/.../weatherDataCsv   master ±✚  ls2001.csv 2003.csv 2005.csv 2007.csv 2009.csv 2011.csv 2013.csv 2015.csv 2017.csv 2019.csv 2002.csv 2004.csv 2006.csv 2008.csv 2010.csv 2012.csv 2014.csv 2016.csv 2018.csv 2020.csv mijazz@lenovo  ~/pyProjects/.../weatherDataCsv   master ±✚  echo &quot;Date,High,Low&quot; &amp;gt; ./temperature.full.csv mijazz@lenovo  ~/pyProjects/.../weatherDataCsv   master ±✚  cat ./*.csv &amp;gt;&amp;gt; ./temperature.full.csv ✘ mijazz@lenovo  ~/pyProjects/.../weatherDataCsv   master ±✚  head -n 3 ./temperature.full.csv Date,High,Low2001-01-01,24,52001-01-02,19,5 mijazz@lenovo  ~/pyProjects/.../weatherDataCsv   master ±✚  tail -n 3 ./temperature.full.csv 2020-11-21,48,362020-11-22,47,412020-11-23,46,33现在可以将其放到hdfs里, 然后尝试在spark里交叉补充好气温信息. 为可视化做准备. mijazz@lenovo  ~/pyProjects/.../weatherDataCsv   master ±✚  hdfs dfs -put ./temperature.full.csv /user/mijazz/chicagoData/temperature.full.csv mijazz@lenovo  ~/pyProjects/.../weatherDataCsv   master ±✚  hdfs dfs -ls /user/mijazz/chicagoDataFound 3 itemsdrwxr-xr-x - mijazz supergroup 0 2020-11-30 21:16 /user/mijazz/chicagoData/dateDF.csv-rw-r--r-- 1 mijazz supergroup 1701238602 2020-11-30 15:43 /user/mijazz/chicagoData/originCrimeData.csv-rw-r--r-- 1 mijazz supergroup 123272 2020-11-30 23:37 /user/mijazz/chicagoData/temperature.full.csvData Practicing-EP5Get Weather DataStaticTool.java - +(Add Row)+ public static final String WEATHER_DATA = DATA_PATH + &quot;temperature.full.csv&quot;;MergeWeather.scala - 1package edu.zstu.mijazz.sparklearn1import org.apache.spark.sql.SparkSessionobject MergeWeather extends App{ val spark = SparkSession.builder.appName(&quot;Data Analysis&quot;).master(&quot;local&quot;).getOrCreate() val sContext = spark.sparkContext val data = spark.read .option(&quot;header&quot;, true) .option(&quot;inferSchema&quot;, false) .csv(StaticTool.WEATHER_DATA) data.printSchema() data.show(3)}root |-- Date: string (nullable = true) |-- High: string (nullable = true) |-- Low: string (nullable = true) +----------+----+---+| Date|High|Low|+----------+----+---+|2001-01-01| 24| 5||2001-01-02| 19| 5||2001-01-03| 28| 7|+----------+----+---+only showing top 3 rows inferSchema==false 因为在EP4dataFrame中有一列的Day的类型本就是string, 这里如果给了true, 就被内转成timestamp了, 对后续sql不算太方便.Merge them Together在EP4中我曾经通过createOrReplaceTempView()来创建了一张临时表, 但是这张表的生命周期是绑定在SparkSession上的, 现在我换了一个Session, 现在采用createGlobalTempView()详细见文章总结.focusDate.scala - (- means delete row, + means add row)- data.createOrReplaceTempView(&quot;t_CrimeDate&quot;)+ data.createOrReplaceGlobalTempView(&quot;t_CrimeDate&quot;) val eachCrimeSummary = spark. sql(&quot;select Crime, count(1) Occurs &quot; +- &quot;from t_CrimeDate &quot; + + &quot;from global_temp.t_CrimeDate &quot; + &quot;group by Crime&quot;) val groupByMonth = spark .sql(&quot;select month(Month) NaturalMonth, count(1) CrimePerMonth &quot; +- &quot;from t_CrimeDate &quot; + + &quot;from global_temp.t_CrimeDate &quot; + &quot;group by NaturalMonth&quot;)通过前面生成了globalTempView之后, 就可以在另一个Session中来通过global_temp.表名来访问了.这里采用spark sql来进行数据合并. 先把表做出来MergeWeather.scala - 2 data.createOrReplaceGlobalTempView(&quot;t_weatherData&quot;)先看一下两张表的Schema关联长什么样// global_temp.t_CrimeDateroot |-- Crime: string (nullable = true) |-- Year: integer (nullable = true) |-- TimeStamp: timestamp (nullable = true) |-- Month: string (nullable = true) |-- Day: timestamp (nullable = true) |-- Hour: integer (nullable = true) // global_temp.t_weatherData root |-- Date: string (nullable = true) |-- High: string (nullable = true) |-- Low: string (nullable = true)global_temp.t_CrimeDate.Day &amp;lt;==&amp;gt; global_temp.t_weatherData.DateHigh, Low Join上MergeWeather.scala - 3 val mergedData = spark.newSession() .sql(&quot;select C.Crime, C.Year, C.TimeStamp, C.Month, C.Day, W.High, W.Low C.Location &quot; + &quot;from global_temp.t_CrimeDate C, global_temp.t_weatherData W &quot; + &quot;where C.Day = W.Date&quot;) mergedData.printSchema() mergedData.show(3)+-------------------+----+-------------------+-------+-------------------+----+---+---------+| Crime|Year| TimeStamp| Month| Day|High|Low| Location|+-------------------+----+-------------------+-------+-------------------+----+---+---------+| DECEPTIVE PRACTICE|2001|2001-01-01 11:00:00|2001-01|2001-01-01 00:00:00| 24| 5|RESIDENCE||CRIM SEXUAL ASSAULT|2017|2017-10-08 03:00:00|2017-10|2017-10-08 00:00:00| 78| 54|RESIDENCE|| BURGLARY|2017|2017-03-28 02:00:00|2017-03|2017-03-28 00:00:00| 50| 36| OTHER|+-------------------+----+-------------------+-------+-------------------+----+---+---------+only showing top 3 rows所以现在每条犯罪记录都有了当天的天气信息了.但是温标是华氏温标, (F - 32) / 1.8 = C. 用DataFrame来做就行, 虽然当时SQL导入的时候也可以这样做.MergeWeather.scala - 4 mergedData = mergedData .withColumn(&quot;HighC&quot;, round(col(&quot;High&quot;).cast(&quot;float&quot;).-(32.0)./(1.8), 2)) .withColumn(&quot;LowC&quot;, round(col(&quot;Low&quot;).cast(&quot;float&quot;).-(32.0)./(1.8), 2)) .drop(&quot;High&quot;) .drop(&quot;Low&quot;) mergedData.printSchema() mergedData.show(3)+-------------------+----+-------------------+-------+-------------------+---------+-----+-----+| Crime|Year| TimeStamp| Month| Day| Location|HighC| LowC|+-------------------+----+-------------------+-------+-------------------+---------+-----+-----+| DECEPTIVE PRACTICE|2001|2001-01-01 11:00:00|2001-01|2001-01-01 00:00:00|RESIDENCE|-4.44|-15.0||CRIM SEXUAL ASSAULT|2017|2017-10-08 03:00:00|2017-10|2017-10-08 00:00:00|RESIDENCE|25.56|12.22|| BURGLARY|2017|2017-03-28 02:00:00|2017-03|2017-03-28 00:00:00| OTHER| 10.0| 2.22|+-------------------+----+-------------------+-------+-------------------+---------+-----+-----+only showing top 3 rowsMergeWeather.scala - 5 reparition(1) Returns a new Dataset partitioned by the given partitioning expressions into numPartitions. The resulting Dataset is hash partitioned. mergedData .repartition(1) .write .option(&quot;header&quot;, true) .csv(StaticTool.DATA_PATH + &quot;forPySpark.csv&quot;)Data Practicing-EP6Introduce pysparkScala和Python下对于Spark的操作还是有很多相似的地方的.迁移到PySpark下, 因为toPandas和collect() =&amp;gt; List这两个pyspark独有的特性, 使得可视化较Scala下方便.不过要注意的是Spark.DataFrame和Pandas.DataFrame是两个完全不同的东西. 不过也很好理解, 鉴于这一次实验我是故意避开不使用Pandas的东西的.假设有如下案例吧import randomdef rInt(): return random.randint(1, 100)def rStr(): return random.choice(&#39;I Just Dont Want To Use DataFrame From Pandas&#39;.split(&#39; &#39;))def rRow(): return [rInt(), rStr()]print(rRow())[66, &#39;Pandas&#39;][35, &#39;Just&#39;]每次调用rRow()都会返回一个List, 也就是sparkDataFrame中的一行数据.通过Scala中可以知道, SparkSession控制每次的Spark会话, 而他也提供一个方法来创建会话.parallelize()用于RDD, toDF()会把RDD数据转成Spark.DataFramefrom pyspark.sql import SparkSessionspark = SparkSession.builder\\ .master(&#39;local&#39;).appName(&#39;Learn Pyspark&#39;).getOrCreate()sc = spark.sparkContextexampleSparkDataFrame = \\ sc.parallelize([rRow() for _ in range(5)]).toDF((&quot;Number&quot;, &quot;Word&quot;))exampleSparkDataFrame.show()print(type(exampleSparkDataFrame))+------+---------+|Number| Word|+------+---------+| 60|DataFrame|| 43| Just|| 85| Want|| 64| Use|| 52|DataFrame|+------+---------+&amp;lt;class &#39;pyspark.sql.dataframe.DataFrame&#39;&amp;gt;也可以很方便的通过toPandas()方式转换.examplePandasDataFrame = exampleSparkDataFrame.toPandas()examplePandasDataFrame.info()print(type(examplePandasDataFrame))RangeIndex: 5 entries, 0 to 4Data columns (total 2 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Number 5 non-null int64 1 Word 5 non-null objectdtypes: int64(1), object(1)memory usage: 208.0+ bytes&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;当想取列时, select()选择列, collect()将其从远端的Spark.DataFrame拉回本地Python.print(exampleSparkDataFrame.select(&#39;Number&#39;).collect())print(exampleSparkDataFrame.select(&#39;Word&#39;).collect())[Row(Number=6), Row(Number=16), Row(Number=50), Row(Number=53), Row(Number=51)][Row(Word=&#39;Just&#39;), Row(Word=&#39;To&#39;), Row(Word=&#39;From&#39;), Row(Word=&#39;Just&#39;), Row(Word=&#39;Pandas&#39;)]假如你需要拿spark.DataFrame中的列来画图, 如下几种方法都是一样的.eg = [0 for _ in range(4)]eg[0] = list(exampleSparkDataFrame.toPandas()[&#39;Number&#39;])eg[1] = exampleSparkDataFrame.select(&#39;Number&#39;).rdd.flatMap(lambda x: x).collect()eg[2] = exampleSparkDataFrame.select(&#39;Number&#39;).rdd.map(lambda x: x[0]).collect()eg[3] = [x[0] for x in exampleSparkDataFrame.select(&#39;Number&#39;).collect()]for example in eg: print(example)[95, 56, 54, 61, 58][95, 56, 54, 61, 58][95, 56, 54, 61, 58][95, 56, 54, 61, 58]但是不推荐eg[0]对应的方法, 他是将整个spark.DataFrame从远端取回来, 如果使用的是集群, 或者数据量比较大的话, 交给本地的python将其转为Pandas.DataFrame. 而其余几种, 而是交给spark处理过后, 单独剥离一列值进行返回.rdd内实现的操作这里不详述.Start to Use PySparkEP5中拿出了两批数据, 分别是forPyspark.csv和temperature.full.csv先做以下导入# -*- coding: utf-8 -*-# @Author : MijazzChan# Python Version == 3.8.6import osimport pandas as pdimport numpy as npfrom matplotlib import pyplot as pltimport seaborn as snsimport pylab as plotfrom pyspark.sql import SparkSessionfrom pyspark.sql.functions import col, roundplt.rcParams[&#39;figure.dpi&#39;] = 150plt.rcParams[&#39;savefig.dpi&#39;] = 150sns.set(rc={&quot;figure.dpi&quot;: 150, &#39;savefig.dpi&#39;: 150})DATA_PATH = &quot;hdfs://localhost:9000/user/mijazz/chicagoData/&quot;Something irrelevantspark = SparkSession.builder.master(&#39;local&#39;).appName(&#39;Data Visualization&#39;).getOrCreate()weatherData = spark.read\\ .option(&#39;header&#39;, True)\\ .option(&#39;inferSchema&#39;, True)\\ .csv(DATA_PATH + &#39;temperature.full.csv&#39;)# 转摄氏度weatherData = weatherData\\ .withColumn(&#39;HighC&#39;, round((col(&#39;High&#39;).cast(&#39;float&#39;) - 32.0) / 1.8, 2))\\ .withColumn(&#39;LowC&#39;, round((col(&#39;Low&#39;).cast(&#39;float&#39;) - 32.0) / 1.8, 2))\\ .drop(&#39;High&#39;)\\ .drop(&#39;Low&#39;)weatherData.createOrReplaceGlobalTempView(&#39;v_Weather&#39;)weatherData.describe().show()+-------+----------+------------------+------------------+|summary| Date| HighC| LowC|+-------+----------+------------------+------------------+| count| 7267| 7267| 7267|| mean| null|15.352508600522908| 5.617067565708001|| stddev| null|11.811098684239695|10.534155955862133|| min|2001-01-01| -23.33| -30.56|| max|2020-11-23| 39.44| 27.78|+-------+----------+------------------+------------------+拿到的数据集, 2001-01-01年到2020-11-23总平均最高气温是15.35, 总平均最低气温是5.62Full Coverage对着整个天气数据集画个图呢?xDays = weatherData.select(&#39;Date&#39;).rdd.flatMap(lambda x: x).collect()yFullHigh = weatherData.select(&#39;HighC&#39;).rdd.flatMap(lambda x: x).collect()yFullLow = weatherData.select(&#39;LowC&#39;).rdd.flatMap(lambda x: x).collect()fig, axs = plt.subplots(2, 1)axs[0].plot(xDays, yFullHigh)axs[0].set_title(&#39;High Temp Full Coverage in Chicago City, 2001-2020&#39;)axs[0].set_xlabel(&#39;Year&#39;)axs[0].set_xticks([])axs[0].set_ylabel(&#39;Temperature Celsius&#39;)axs[1].plot(xDays, yFullLow)axs[1].set_title(&#39;High Temp Full Coverage in Chicago City, 2001-2020&#39;)axs[1].set_xlabel(&#39;Year&#39;)axs[1].set_xticks([])axs[1].set_ylabel(&#39;Temperature Celsius&#39;)plt.show()仿佛看不出来什么规律. 说好的全球变暖呢Annual Summary那就按年平均画个图吧annualData = \\ spark.sql(&#39;SELECT year(Date) Annual, round(avg(HighC), 2) avgHigh, round(avg(LowC), 2) avgLow &#39; &#39;FROM global_temp.v_Weather &#39; &#39;GROUP BY year(Date) &#39;)\\ .orderBy(asc(&#39;Annual&#39;))annualData.show(20)+------+-------+------+|Annual|avgHigh|avgLow|+------+-------+------+| 2001| 15.39| 5.49|| 2002| 15.37| 5.62|| 2003| 14.63| 4.24|| 2004| 14.98| 4.88|| 2005| 15.87| 5.53|| 2006| 15.9| 6.31|| 2007| 15.6| 5.84|| 2008| 14.25| 4.38|| 2009| 14.05| 4.58|| 2010| 15.66| 6.07|| 2011| 15.04| 5.85|| 2012| 17.73| 7.3|| 2013| 14.43| 4.68|| 2014| 13.66| 3.76|| 2015| 15.02| 5.26|| 2016| 15.97| 6.57|| 2017| 16.27| 6.59|| 2018| 15.12| 6.08|| 2019| 14.44| 5.31|| 2020| 17.91| 8.26|+------+-------+------+fig, axs = plt.subplots(2, 1)xYear = annualData.select(&#39;Annual&#39;).collect()yAvgHigh = annualData.select(&#39;avgHigh&#39;).collect()yAvgLow = annualData.select(&#39;avgLow&#39;).collect()axs[0].plot(xYear, yAvgHigh)axs[0].set_title(&#39;Average High Temp in Chicago City&#39;)axs[0].set_xlabel(&#39;Year&#39;)axs[0].set_ylabel(&#39;Temperature Celsius&#39;)axs[1].plot(xYear, yAvgLow)axs[1].set_title(&#39;Average Low Temp in Chicago City&#39;)axs[1].set_xlabel(&#39;Year&#39;)axs[1].set_ylabel(&#39;Temperature Celsius&#39;)plt.show()现在是能看出一些趋势了.Plot Some DataSome Acknowledgement该函数用于快速返回指定spark.DataFrame的列.def column2List(dataFrame, column): return dataFrame.select(column).rdd.flatMap(lambda x: x).collect()而且拿dataFrame中的数据, 有各种方法, 此处就以犯罪数据排名作为例子.root |-- Crime: string (nullable = true) |-- Year: integer (nullable = true) |-- TimeStamp: timestamp (nullable = true) |-- Month: string (nullable = true) |-- Day: timestamp (nullable = true) |-- Location: string (nullable = true) |-- HighC: double (nullable = true) |-- LowC: double (nullable = true)想摘取数据进行分析DataFrame ApproachcrimeRankPlotData = fullData.select(&#39;Crime&#39;)\\ .groupBy(&#39;Crime&#39;)\\ .count()\\ .orderBy(desc(&#39;count&#39;))\\ .limit(15)Spark SQL ApproachfullData.createGlobalTempView(&#39;v_Crime&#39;)crimeRankPlotData = spark.sql(&#39;SELECT Crime, count(1) crimeCount &#39; &#39;FROM global_temp.v_Crime &#39; &#39;GROUP BY Crime &#39; &#39;ORDER BY crimeCount DESC &#39; &#39;LIMIT 15&#39;)RDD AppraochfullData.rdd.countByKey().items() # -&amp;gt; dictCrime Rank Plot记得在EP4中, 拿出来看过犯罪数据的排名. 做个前15的BarPlotfullData = spark.read\\ .option(&#39;header&#39;, True)\\ .option(&#39;inferSchema&#39;, True)\\ .csv(DATA_PATH + &#39;forPySpark.csv&#39;).cache()crimeRankPlotData = fullData.select(&#39;Crime&#39;)\\ .groupBy(&#39;Crime&#39;)\\ .count()\\ .orderBy(desc(&#39;count&#39;))\\ .limit(15)plt.figure()plt.barh(column2List(crimeRankPlotData, &#39;Crime&#39;), column2List(crimeRankPlotData, &#39;count&#39;))plt.xlabel(&#39;Crime Count&#39;)plt.ylabel(&#39;Crime Type&#39;)plt.title(&#39;TOP 15 Crime Count&#39;)plt.show()Location Distribution PlotlocationRankPlotData = fullData.select(&#39;Location&#39;)\\ .groupBy(&#39;Location&#39;)\\ .count()\\ .orderBy(desc(&#39;count&#39;))locationRankPlotData.show(20)plt.figure()tmp1 = column2List(locationRankPlotData, &#39;Location&#39;)tmp2 = column2List(locationRankPlotData, &#39;count&#39;)plt.barh(tmp1[:15], tmp2[:15])plt.xlabel(&#39;Crime Count&#39;)plt.ylabel(&#39;Crime Type&#39;)plt.title(&#39;Location Distribution of Crimes&#39;)plt.show()Annual Crime Count PlotcrimePerYear = spark.sql(&#39;SELECT year(C.TimeStamp) Annual, count(1) CrimePerYear &#39; &#39;FROM global_temp.v_Crime C &#39; &#39;GROUP BY year(C.TimeStamp) &#39; &#39;ORDER BY Annual ASC&#39;)crimePerYear.show(20)plt.figure()# 2020年的数据不齐, 去掉plt.plot(column2List(crimePerYear, &#39;Annual&#39;)[:19], column2List(crimePerYear, &#39;CrimePerYear&#39;)[:19])plt.title(&#39;Crime Count Per Year in Chicago City&#39;)plt.xlabel(&#39;Year&#39;)plt.ylabel(&#39;Crime Count&#39;)plt.show()+------+------------+|Annual|CrimePerYear|+------+------------+| 2001| 485783|| 2002| 486764|| 2003| 475962|| 2004| 469395|| 2005| 453735|| 2006| 448138|| 2007| 437041|| 2008| 427099|| 2009| 392770|| 2010| 370395|| 2011| 351878|| 2012| 336137|| 2013| 307299|| 2014| 275545|| 2015| 264449|| 2016| 269443|| 2017| 268675|| 2018| 268222|| 2019| 260318|| 2020| 163225|+------+------------+Data Practicing-EP7Visualization in PythonPandas和notebook一起用, 在这个先被Spark处理过的几百万行的数据集上做可视化还是感觉方便些.先做个依赖导入和数据清洗吧# -*- coding: utf-8 -*-# Python Version == 3.8.6import osimport pandas as pdimport numpy as npfrom matplotlib import pyplot as pltimport seaborn as snsplt.rcParams[&#39;figure.dpi&#39;] = 150plt.rcParams[&#39;savefig.dpi&#39;] = 150sns.set(rc={&quot;figure.dpi&quot;: 150, &#39;savefig.dpi&#39;: 150})from jupyterthemes import jtplotjtplot.style(theme=&#39;monokai&#39;, context=&#39;notebook&#39;, ticks=True, grid=False)fullData = pd.read_csv(&#39;~/devEnvs/chicagoCrimeData.csv&#39;, encoding=&#39;utf-8&#39;)fullData.info()/home/mijazz/devEnvs/pyvenv/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3062: DtypeWarning: Columns (21) have mixed types.Specify dtype option on import or set low_memory=False. has_raised = await self.run_ast_nodes(code_ast.body, cell_name,&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;RangeIndex: 7212273 entries, 0 to 7212272Data columns (total 22 columns): # Column Dtype --- ------ ----- 0 ID int64 1 Case Number object 2 Date object 3 Block object 4 IUCR object 5 Primary Type object 6 Description object 7 Location Description object 8 Arrest bool 9 Domestic bool 10 Beat int64 11 District float64 12 Ward float64 13 Community Area float64 14 FBI Code object 15 X Coordinate float64 16 Y Coordinate float64 17 Year int64 18 Updated On object 19 Latitude float64 20 Longitude float64 21 Location object dtypes: bool(2), float64(7), int64(3), object(10)memory usage: 1.1+ GBfullData.head(5) ID Case Number Date Block IUCR Primary Type Description Location Description Arrest Domestic ... Ward Community Area FBI Code X Coordinate Y Coordinate Year Updated On Latitude Longitude Location 0 11034701 JA366925 01/01/2001 11:00:00 AM 016XX E 86TH PL 1153 DECEPTIVE PRACTICE FINANCIAL IDENTITY THEFT OVER $ 300 RESIDENCE False False ... 8.0 45.0 11 NaN NaN 2001 08/05/2017 03:50:08 PM NaN NaN NaN 1 11227287 JB147188 10/08/2017 03:00:00 AM 092XX S RACINE AVE 0281 CRIM SEXUAL ASSAULT NON-AGGRAVATED RESIDENCE False False ... 21.0 73.0 02 NaN NaN 2017 02/11/2018 03:57:41 PM NaN NaN NaN 2 11227583 JB147595 03/28/2017 02:00:00 PM 026XX W 79TH ST 0620 BURGLARY UNLAWFUL ENTRY OTHER False False ... 18.0 70.0 05 NaN NaN 2017 02/11/2018 03:57:41 PM NaN NaN NaN 3 11227293 JB147230 09/09/2017 08:17:00 PM 060XX S EBERHART AVE 0810 THEFT OVER $500 RESIDENCE False False ... 20.0 42.0 06 NaN NaN 2017 02/11/2018 03:57:41 PM NaN NaN NaN 4 11227634 JB147599 08/26/2017 10:00:00 AM 001XX W RANDOLPH ST 0281 CRIM SEXUAL ASSAULT NON-AGGRAVATED HOTEL/MOTEL False False ... 42.0 32.0 02 NaN NaN 2017 02/11/2018 03:57:41 PM NaN NaN NaN 5 rows × 22 columnsfullData.drop_duplicates(subset=[&#39;ID&#39;, &#39;Case Number&#39;], inplace=True)fullData.drop([&#39;Case Number&#39;, &#39;IUCR&#39;,&#39;Updated On&#39;,&#39;Year&#39;, &#39;FBI Code&#39;, &#39;Beat&#39;,&#39;Ward&#39;,&#39;Community Area&#39;, &#39;Location&#39;], inplace=True, axis=1)fullData[&#39;Location Description&#39;].describe()count 7204883unique 214top STREETfreq 1874164Name: Location Description, dtype: objectfullData[&#39;Description&#39;].describe()count 7212273unique 532top SIMPLEfreq 849119Name: Description, dtype: objectfullData[&#39;Primary Type&#39;].describe()count 7212273unique 36top THEFTfreq 1522618Name: Primary Type, dtype: object可以看到这三列的其中两列, Location Description和Description有许多Unique值, 我们只取数量多的, 这里只取计数为前20的作为大类以做特征分析, 其他的归为杂类.locationDescription20Except = list(fullData[&#39;Location Description&#39;].value_counts()[20:].index)# 用loc把数据砍掉fullData.loc[fullData[&#39;Location Description&#39;].isin(locationDescription20Except), fullData.columns==&#39;Location Description&#39;] = &#39;OTHER&#39;description20Except = list(fullData[&#39;Description&#39;].value_counts()[20:].index)# 用loc把数据砍掉fullData.loc[fullData[&#39;Description&#39;].isin(description20Except) , fullData.columns==&#39;Description&#39;] = &#39;OTHER&#39;之前在spark中已经看到犯罪数量是36种, 并且数量从2001年到现在是逐年减少的. 但是只有每年的统计, 这里尝试作做rolling sum. 也就是每个取样点的横坐标对应一个日期, 纵坐标对应(当前日期-364天 ~ 当天)的犯罪数量和.先把Date换成DatetimefullData.Date = pd.to_datetime(fullData.Date, format=&#39;%m/%d/%Y %I:%M:%S %p&#39;)做Resample要有Index, 日期做了cast之后就行.fullData.index = pd.DatetimeIndex(fullData.Date)fullData.resample(&#39;D&#39;).size().rolling(365).sum().plot()plt.xlabel(&#39;Days&#39;)plt.ylabel(&#39;Crimes Count&#39;)plt.show()​可以看到rolling sum是在稳步减少的.现在分犯罪种类Primary Type来作图.eachCrime = fullData.pivot_table(&#39;ID&#39;, aggfunc=np.size, columns=&#39;Primary Type&#39;, index=fullData.index.date, fill_value=0)eachCrime.index = pd.DatetimeIndex(eachCrime.index)tmp = eachCrime.rolling(365).sum().plot(figsize=(12, 60), subplots=True, layout=(-1, 2), sharex=False, sharey=False)​ ​这里看到了一些无用的数据, 有些犯罪种类甚至近20年来发生不超过千次, 砍掉犯罪数量非前20的犯罪种类, 只留下前20的种类再做一个rolling sum.并且留意到NON-CRIMINAL和NON - CRIMINAL两个类重复, 砍掉. 并也将其变为OTHERcrime20Except = list(fullData[&#39;Primary Type&#39;].value_counts()[20:].index)fullData.loc[fullData[&#39;Primary Type&#39;].isin(crime20Except), fullData.columns==&#39;Primary Type&#39;] = &#39;OTHER&#39;fullData.loc[fullData[&#39;Primary Type&#39;] == &#39;NON-CRIMINAL&#39;, fullData.columns==&#39;Primary Type&#39;] = &#39;OTHER&#39;fullData.loc[fullData[&#39;Primary Type&#39;] == &#39;NON - CRIMINAL&#39;, fullData.columns==&#39;Primary Type&#39;] = &#39;OTHER&#39;eachCrime = fullData.pivot_table(&#39;ID&#39;, aggfunc=np.size, columns=&#39;Primary Type&#39;, index=fullData.index.date, fill_value=0)eachCrime.index = pd.DatetimeIndex(eachCrime.index)tmp = eachCrime.rolling(365).sum().plot(figsize=(12, 60), subplots=True, layout=(-1, 2), sharex=False, sharey=False)数据处理完之后, 明显能够看出来, 基本的犯罪种类的数量的确是在下降的, 但是有两个WEAPONS VIOLATION和INTERFERENCE WITH PUBLIC OFFICER在逆势上涨.Data Practicing-EP8基于日期的话, 因为有index的缘故, 按日分类和按月分类都较为方便.days = [&#39;Mon&#39;,&#39;Tue&#39;,&#39;Wed&#39;, &#39;Thur&#39;, &#39;Fri&#39;, &#39;Sat&#39;, &#39;Sun&#39;]fullData.groupby([fullData.index.dayofweek]).size().plot(kind=&#39;barh&#39;)plt.yticks(np.arange(7), days)plt.xlabel(&#39;Crime Counts&#39;)plt.show()​ ​周五的贡献由为突出fullData.groupby([fullData.index.month]).size().plot(kind=&#39;barh&#39;)plt.ylabel(&#39;Month&#39;)plt.xlabel(&#39;Crime Counts&#39;)plt.show()​ ​按月分类可以看到主要集中于夏季. 在EP7里, Location Description已经被减少至20个, 排名靠后的被修改成OTHERS了对犯罪发生地点的归类.fullData.groupby([fullData[&#39;Location Description&#39;]]).size().sort_values(ascending=True).plot(kind=&#39;barh&#39;)plt.ylabel(&#39;Crime Location&#39;)plt.xlabel(&#39;Crimes Count&#39;)plt.show()​排除OTHER的话, 可以看到一下几个地点的犯罪发生率明显高于其它. 街道 居民住宅区 公寓 人行道引入依赖包以及参考的缩放函数, 作多元的数据透视图以寻找数据联系.Colormaps - matplotlib docsfrom sklearn.cluster import AgglomerativeClustering as ACdef scale_df(df,axis=0): &#39;&#39;&#39; A utility function to scale numerical values (z-scale) to have a mean of zero and a unit variance. &#39;&#39;&#39; return (df - df.mean(axis=axis)) / df.std(axis=axis)def plot_hmap(df, ix=None, cmap=&#39;seismic&#39;, xColumn=False): &#39;&#39;&#39; A function to plot heatmaps that show temporal patterns &#39;&#39;&#39; if ix is None: ix = np.arange(df.shape[0]) plt.imshow(df.iloc[ix,:], cmap=cmap) plt.colorbar(fraction=0.03) plt.yticks(np.arange(df.shape[0]), df.index[ix]) if(xColumn): plt.xticks(np.arange(df.shape[1]), df.columns, rotation=&#39;vertical&#39;) else: plt.xticks(np.arange(df.shape[1])) plt.grid(False) plt.show() def scale_and_plot(df, ix = None, xCol=False): &#39;&#39;&#39; A wrapper function to calculate the scaled values within each row of df and plot_hmap &#39;&#39;&#39; df_marginal_scaled = scale_df(df.T).T if ix is None: ix = AC(4).fit(df_marginal_scaled).labels_.argsort() # a trick to make better heatmaps cap = np.min([np.max(df_marginal_scaled.to_numpy()), np.abs(np.min(df_marginal_scaled.to_numpy()))]) df_marginal_scaled = np.clip(df_marginal_scaled, -1*cap, cap) plot_hmap(df_marginal_scaled, ix=ix, xColumn=xCol) 犯罪发生具体时间 与 位置 犯罪发生具体时间 与 犯罪类型 工作日/周末 与 位置 工作日/周末 与 犯罪类型 位置 与 犯罪类型hour_by_location = fullData.pivot_table(values=&#39;ID&#39;, index=&#39;Location Description&#39;, columns=fullData.index.hour, aggfunc=np.size).fillna(0)hour_by_type = fullData.pivot_table(values=&#39;ID&#39;, index=&#39;Primary Type&#39;, columns=fullData.index.hour, aggfunc=np.size).fillna(0)dayofweek_by_location = fullData.pivot_table(values=&#39;ID&#39;, index=&#39;Location Description&#39;, columns=fullData.index.dayofweek, aggfunc=np.size).fillna(0)dayofweek_by_type = fullData.pivot_table(values=&#39;ID&#39;, index=&#39;Primary Type&#39;, columns=fullData.index.dayofweek, aggfunc=np.size).fillna(0)location_by_type = fullData.pivot_table(values=&#39;ID&#39;, index=&#39;Location Description&#39;, columns=&#39;Primary Type&#39;, aggfunc=np.size).fillna(0)scale_and_plot(hour_by_location)​ ​观察到有几块热区 小巷(ALLEY), 人行道(SIDEWALK), 街道(STREET), 私家车(VEHICLE NON-COM..), 加油站(GAS STATION), 停车场/区(..PARKING LOT..)区域, 都于17点过后至午夜1点犯罪活跃. 停车场(PARKING LOT..),写字楼/商业区(COMMERCIAL/BUSINESS OFFICE),学校/公共类楼宇(SCHOOL, PUBLIC BUILDING)均于早上8点至下午3点犯罪活跃. 几类商店(.. STORE)均于整个中午和下午犯罪活跃. 居民住宅区和公寓型住宅均于 正午12点与午夜0点有热区 凌晨2-6时均为冷区. 这些结论也基本与常识理解较为贴近.scale_and_plot(hour_by_type)​ ​ 人身侵犯, 性侵犯在中午过后几小时有热区 诈骗, 欺凌类型犯罪在上午和中午有热区 其余犯罪均在18点过后, 即非工作时间存在热区.scale_and_plot(dayofweek_by_location)​ ​工作日热区: 停车场, 写字楼, 商店, 学校周五/周六热区: 小巷, 人行道, 餐厅, 商业区停车场, 街道, 私家车, 住宅区, 住宅区走廊/门廊.周六周日热区: 公寓, 油站, 住宅前/后院scale_and_plot(dayofweek_by_type)​ ​工作日热区: 人身侵犯 性骚扰 欺凌 非法入侵 卖淫 节假日热区: 妨碍公务 破坏(刑事破坏) 性侵犯 殴打/斗殴 值得注意的是, 周五当天有几个明显的热区 赌博 妨碍公共安全秩序 偷窃机动车 青少年犯罪 酒水买卖犯罪 枪械犯罪 scale_and_plot(location_by_type, xCol=True)​ ​这个犯罪地点X犯罪类型的图, 每个交叉的热点都是某种特定的犯罪形式最可能发生的地点, 这里不做赘述.只提两个点. 斗殴和盗窃在所有地点都几乎是热区 加油站地点, 几乎无赌博, 性侵, 卖淫, 酒水买卖犯罪 " }, { "title": "Weather Data Extraction", "url": "/posts/Weather-Data-Extraction/", "categories": "Data, Python", "tags": "data, python, crawl", "date": "2020-11-25 15:11:32 +0800", "snippet": "天气数据抓取 数据来自于National Weather Service Forecast Office 数据服务于Hadoop+Spark Data Practicing需求需要分析的主数据来自2001-2020年Chicago Area, 故对以下年份数据进行抓取 Data Copyright Notice:National Weather Service DisclaimerApproach从chrome拿到异步请求数据的cURLcurl &#39;https://data.rcc-acis.org/StnData&#39; \\ -H &#39;Connection: keep-alive&#39; \\ -H &#39;Accept: application/json, text/javascript, */*; q=0.01&#39; \\ -H &#39;DNT: 1&#39; \\ -H &#39;User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36&#39; \\ -H &#39;Content-Type: application/x-www-form-urlencoded; charset=UTF-8&#39; \\ -H &#39;Origin: https://nowdata.rcc-acis.org&#39; \\ -H &#39;Sec-Fetch-Site: same-site&#39; \\ -H &#39;Sec-Fetch-Mode: cors&#39; \\ -H &#39;Sec-Fetch-Dest: empty&#39; \\ -H &#39;Referer: https://nowdata.rcc-acis.org/&#39; \\ -H &#39;Accept-Language: en,zh-CN;q=0.9,zh;q=0.8,en-XA;q=0.7&#39; \\ --data-raw &#39;params=%7B%22elems%22%3A%5B%7B%22name%22%3A%22maxt%22%7D%2C%7B%22name%22%3A%22mint%22%7D%2C%7B%22name%22%3A%22maxt%22%2C%22duration%22%3A%22dly%22%2C%22normal%22%3A%221%22%2C%22prec%22%3A1%7D%2C%7B%22name%22%3A%22mint%22%2C%22duration%22%3A%22dly%22%2C%22normal%22%3A%221%22%2C%22prec%22%3A1%7D%5D%2C%22sid%22%3A%22ORDthr+9%22%2C%22sDate%22%3A%222001-01-01%22%2C%22eDate%22%3A%222001-12-31%22%7D&amp;amp;output=json&#39; \\ --compressed做一下处理变成python的request, 并且去掉一些请求参数, 只保留实际测量出的温度, 去除历史记录最高温等不相关的数据.import requestsheaders = { &#39;Connection&#39;: &#39;keep-alive&#39;, &#39;Accept&#39;: &#39;application/json, text/javascript, */*; q=0.01&#39;, &#39;DNT&#39;: &#39;1&#39;, &#39;User-Agent&#39;: &#39;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36&#39;, &#39;Content-Type&#39;: &#39;application/x-www-form-urlencoded; charset=UTF-8&#39;, &#39;Origin&#39;: &#39;https://nowdata.rcc-acis.org&#39;, &#39;Sec-Fetch-Site&#39;: &#39;same-site&#39;, &#39;Sec-Fetch-Mode&#39;: &#39;cors&#39;, &#39;Sec-Fetch-Dest&#39;: &#39;empty&#39;, &#39;Referer&#39;: &#39;https://nowdata.rcc-acis.org/&#39;, &#39;Accept-Language&#39;: &#39;en,zh-CN;q=0.9,zh;q=0.8,en-XA;q=0.7&#39;,}data = { &#39;params&#39;: &#39;{&quot;elems&quot;:[{&quot;name&quot;:&quot;maxt&quot;},{&quot;name&quot;:&quot;mint&quot;}],&quot;sid&quot;:&quot;ORDthr 9&quot;,&quot;sDate&quot;:&quot;2001-01-01&quot;,&quot;eDate&quot;:&quot;2001-12-31&quot;}&#39;, &#39;output&#39;: &#39;json&#39;}response = requests.post(&#39;https://data.rcc-acis.org/StnData&#39;, headers=headers, data=data)print(response.content)节选一点response.contentb&#39;{&quot;meta&quot;:{&quot;state&quot;: &quot;IL&quot;, &quot;sids&quot;: [&quot;ORDthr 9&quot;], &quot;uid&quot;: 32819, &quot;name&quot;: &quot;Chicago Area&quot;},\\n&quot;data&quot;:[[&quot;2001-01-01&quot;,&quot;24&quot;,&quot;5&quot;],\\n[&quot;2001-01-02&quot;,&quot;19&quot;,&quot;5&quot;],\\n[&quot;2001-01-03&quot;,&quot;28&quot;,&quot;7&quot;],\\n[&quot;2001-01-04&quot;,&quot;30&quot;,&quot;19&quot;],\\n[&quot;2001-01-05&quot;,&quot;36&quot;,&quot;21&quot;],\\n[&quot;2001-01-06&quot;,&quot;33&quot;,&quot;17&quot;],\\n[&quot;2001-01-07&quot;,&quot;34&quot;,&quot;21&quot;],\\n[&quot;2001-01-08&quot;,&quot;26&quot;,&quot;12&quot;],可以看到data类下包含需要的数据针对post data修改参数, 从而获得2001-2020年的数据并转成csv Github: weatherfetch.pyimport osimport requestsheaders = { &#39;Connection&#39;: &#39;keep-alive&#39;, &#39;Accept&#39;: &#39;application/json, text/javascript, */*; q=0.01&#39;, &#39;DNT&#39;: &#39;1&#39;, &#39;User-Agent&#39;: &#39;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36&#39;, &#39;Content-Type&#39;: &#39;application/x-www-form-urlencoded; charset=UTF-8&#39;, &#39;Origin&#39;: &#39;https://nowdata.rcc-acis.org&#39;, &#39;Sec-Fetch-Site&#39;: &#39;same-site&#39;, &#39;Sec-Fetch-Mode&#39;: &#39;cors&#39;, &#39;Sec-Fetch-Dest&#39;: &#39;empty&#39;, &#39;Referer&#39;: &#39;https://nowdata.rcc-acis.org/&#39;, &#39;Accept-Language&#39;: &#39;en,zh-CN;q=0.9,zh;q=0.8,en-XA;q=0.7&#39;,}def buildPostData(year: int): year = str(year) data = { &#39;params&#39;: &#39;{&quot;elems&quot;:[{&quot;name&quot;:&quot;maxt&quot;},{&quot;name&quot;:&quot;mint&quot;}],&quot;sid&quot;:&quot;ORDthr 9&quot;,&quot;sDate&quot;:&quot;&#39; + year + &#39;-01-01&quot;,&quot;eDate&quot;:&quot;&#39; + year + &#39;-12-31&quot;}&#39;, &#39;output&#39;: &#39;json&#39; } return datafor year in range(2001, 2021): path = &#39;./weatherDataCsv&#39; response = requests.post(&#39;https://data.rcc-acis.org/StnData&#39;, headers=headers, data=buildPostData(year)) # print(response.json()[&quot;data&quot;]) dataList = list(response.json()[&quot;data&quot;]) if not os.path.isdir(path): os.mkdir(path) file = open(&#39;{}/{}.csv&#39;.format(path, year), &#39;w+&#39;, encoding=&#39;utf-8&#39;) for eachDay in dataList: # Avoid Annoying &#39;&#39; in str output file.write(&#39;,&#39;.join(eachDay)) file.write(&#39;\\n&#39;) file.flush() file.close()Result数据位于./weatherDataCsv/*.csv下节选2001.csv数据形式为Date, Highest Temp(F), Lowest Temp(F)2001-01-01,24,52001-01-02,19,52001-01-03,28,72001-01-04,30,192001-01-05,36,212001-01-06,33,17" }, { "title": "SSH ProxyCommand NC ERROR", "url": "/posts/ssh-proxycommand-nc-error/", "categories": "Linux, Proxy", "tags": "linux, manjaro, notes, error, trace", "date": "2020-11-24 21:51:37 +0800", "snippet": "SSH ProxyCommand NC ERRORDescription Platform: Manjaro 20 Configuration reference: using-proxy-for-git-or-github.mdError: Couldn&#39;t resolve host &quot;x.x.x.x:8888&quot;kex_exchange_identification: Connection closed by remote hostConnection closed by UNKNOWN port 65535When I was configuring ssh proxy tunnel,ProxyCommand nc ... , then I try to ssh -T git@github.com to validate my proxy configuration. I got those errors described below.It seems like nc didn’t recognized the option -X , thus failed to use the proxy.I tried to locate nc by using which nc, then I got /usr/bin/nc.Locate the problemsSo I try to find to trace the version of nc, to see whether its version is too old or something with its dependencies causing it to be outdated. This kind of problem is not likely to happen on a rolling base system like Manjaro, since it always receives the latest softwares/dependencies update.nc --versionnetcat (The GNU Netcat) 0.7.1Copyright (C) 2002 - 2003 Giovanni GiacobbiThis program comes with NO WARRANTY, to the extent permitted by law.You may redistribute copies of this program under the terms ofthe GNU General Public License.For more information about these matters, see the file named COPYING.Original idea and design by Avian Research &amp;lt;hobbit@avian.org&amp;gt;,Written by Giovanni Giacobbi &amp;lt;giovanni@giacobbi.net&amp;gt;.nc --helpGNU netcat 0.7.1, a rewrite of the famous networking tool.Basic usages:connect to somewhere: nc [options] hostname port [port] ...listen for inbound: nc -l -p port [options] [hostname] [port] ...tunnel to somewhere: nc -L hostname:port -p port [options]This is what i got when exec nc --version &amp;amp; nc --helpThen i try using pamac search to see whether the /usr/bin/nc in my path/system have some kind of dependency problems.I gotpamac search netcat # Outputopenbsd-netcat 1.217_2-1 community TCP/IP swiss army knife. OpenBSD variant.gnu-netcat [Installed] 0.7.1-8 extra GNU rewrite of netcat, the network piping applicationTurns out that there are two different versions of nc, one is under openbsd, the other one is under gnu.Solutionattempt installing the right version of nc It may prompt “To remove conflict”, just type y, let pacman handle for you.sudo pacman -S openbsd-netcatnc -helpOpenBSD netcat (Debian patchlevel 1)usage: nc [-46CDdFhklNnrStUuvZz] [-I length] [-i interval] [-M ttl] [-m minttl] [-O length] [-P proxy_username] [-p source_port] [-q seconds] [-s sourceaddr] [-T keyword] [-V rtable] [-W recvlimit] [-w timeout] [-X proxy_protocol] [-x proxy_address[:port]] [destination] [port]It lists -X as proxy_protocol, just like using-proxy-for-git-or-github.md and nc docs.here is my ~/.ssh/configHost github.com User git IdentityFile &quot;/path/to/key&quot; ProxyCommand nc -X 5 -x 192.168.123.4:8888 %h %pAnd it worksssh -T git@github.comEnter passphrase for key &#39;/path/to/key*****&#39;: Hi ****! You&#39;ve successfully authenticated, but GitHub does not provide shell access." }, { "title": "Import SSH Key", "url": "/posts/import-ssh-key/", "categories": "Linux, other", "tags": "notes", "date": "2020-11-24 18:35:34 +0800", "snippet": "Import SSH KeyKey backupgithub.key for ssh login/authentication on github.com for generating key, visit Github DocsRe-import keys# Start key agent in backgroundeval &quot;$(ssh-agent -s)&quot; ## Output Agent pid 28022ssh-add ./github.keyYou may encounter problem likessh-add ./github.key @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: UNPROTECTED PRIVATE KEY FILE! @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@Permissions 0777 for &#39;./github.key&#39; are too open.It is required that your private key files are NOT accessible by others.This private key will be ignored. Key file in my scenario, is copied from a NTFS file system(Windows Partition). Since the permission of key file in Linux/Unix System is pretty strict, so you will be unable to add it into your system when the permission is not 0600.chmod 600 ./github.keyssh-add ./github.key# OutputEnter passphrase for ./github.key: Identity added: ./github.key (mijazz@qq.com)ssh -T git@github.comThe authenticity of host &#39;github.com (192.30.255.113)&#39; can&#39;t be established.RSA key fingerprint is SHA256:nThbg6kXUpJWGl7E1IGOCspRomTxdCARLviKw6E5SY8.Are you sure you want to continue connecting (yes/no/[fingerprint])? yesWarning: Permanently added &#39;github.com,192.30.255.113&#39; (RSA) to the list of known hosts.Hi ******! You&#39;ve successfully authenticated, but GitHub does not provide shell access.Configure SSH through proxyAssume you have a local proxy tunnel Socks5 proxy : socks5://x.x.x.x:x HTTPS proxy : https://x.x.x.x:xYou can force you ssh client to send all traffic via given proxy.By editing ~/.ssh/config This is a host-wise setting. Only github.com will use this proxy# File in ~/.ssh/configHost github.com User git IdentityFile &quot;/home/mijazz/.securedkeys/github.key&quot; # Socks5 ProxyCommand nc -X 5 -x 192.168.123.4:8888 %h %p # Https # ProxyCommand nc -X connect -x 192.168.123.4:8889 %h %pIn nc docs , you can find detailed usage of nc. nc -X 5 # For socks5 protocol nc -X connect # For https protocol From nc docs: -X proxy_versionRequests that nc should use the specified protocol when talking to the proxy server. Supported protocols are ‘‘4’’ (SOCKS v.4), ‘‘5’’ (SOCKS v.5) and ‘‘connect’’ (HTTPS proxy). If the protocol is not specified, SOCKS version 5 is used.Problem You May encounternc: invalid option -- &#39;X&#39;Try `nc --help&#39; for more information.kex_exchange_identification: Connection closed by remote hostConnection closed by UNKNOWN port 65535See SSH ProxyCommand NC Error" }, { "title": "Import GPG Key", "url": "/posts/import-gpg-key/", "categories": "Linux, other", "tags": "notes", "date": "2020-11-24 17:15:02 +0800", "snippet": "Import GPG KeyAfter re-installing the system, I lost all my git setting since I did not back up the gitconfig file. I have a 2 gpg keys, one for Github and one for my personal Gitea Server. ## To Find Keys available/active on systemgpg --list-keys# Using the github key as the global optiongit config --global user.signingkey $MY_GITHUB_GPGKEY$# Project-wise setting git config user.signingkey $MY_OWN_GPGKEY$ How to re-import locate the key first, in my case privategpg.key is the private gpg keygpg --import ./privategpg.keyIt will prompt for key password# Terminal Outputgpg: directory &#39;/home/mijazz/.gnupg&#39; createdgpg: keybox &#39;/home/mijazz/.gnupg/pubring.kbx&#39; createdgpg: /home/mijazz/.gnupg/trustdb.gpg: trustdb createdgpg: key AA***********D: public key &quot;MijazzChan &amp;lt;mijazz@vip.qq.com&amp;gt;&quot; importedgpg: key AA***********D: secret key importedgpg: Total number processed: 1gpg: imported: 1gpg: secret keys read: 1gpg: secret keys imported: 1Trust Ultimatelygpg --edit-key $GPG_KEY_EMAIL$gpg&amp;gt; trust5 -&amp;gt; Trust UltimatelySign the commitOnce the key has been imported, simply git commit -S ... will make git sign the commit. You may be asked for private key password if your key was generated with it." }, { "title": "Titanic Data Analysis", "url": "/posts/Titanic-Data-Analysis/", "categories": "Data, Python", "tags": "data, python, pandas, visualization", "date": "2020-11-17 03:19:37 +0800", "snippet": "Titanic Data Analysis Data Mining Course Assignment-3# -*- coding: utf-8 -*-# @Author : 陈浩骏, 2017326603075# Python Version == 3.8.5import osimport pandas as pdimport numpy as npfrom matplotlib import pyplot as pltimport seaborn as snsimport pylab as plotfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifierfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.svm import LinearSVC%matplotlib inlineplt.rcParams[&#39;figure.dpi&#39;] = 150plt.rcParams[&#39;savefig.dpi&#39;] = 150sns.set(rc={&quot;figure.dpi&quot;: 150, &#39;savefig.dpi&#39;: 150})from jupyterthemes import jtplotjtplot.style(theme=&#39;monokai&#39;, context=&#39;notebook&#39;, ticks=True, grid=False)首次数据是从kaggle上直接下载下来的.首次作业见Assignment3 Page因下列block所依赖数据为kaggle上的原始数据, 而实际课程上作业拿到的数据是经过特意修改后的, 含有重复列和特殊值的.故以下block专门对数据进行去重.corruptedData = pd.read_csv(&#39;./CorruptedTitanic/train.csv&#39;)corruptedData.describe() PassengerId Survived Pclass Age SibSp Parch Fare count 892.000000 892.000000 892.000000 715.000000 892.000000 892.000000 892.000000 mean 445.547085 0.383408 2.308296 30.249189 0.523543 0.381166 195.705100 std 257.564835 0.486489 0.835666 20.038824 1.102240 0.805706 4887.636304 min 1.000000 0.000000 1.000000 0.420000 0.000000 0.000000 0.000000 25% 222.750000 0.000000 2.000000 20.750000 0.000000 0.000000 7.917700 50% 445.500000 0.000000 3.000000 28.000000 0.000000 0.000000 14.454200 75% 668.250000 1.000000 3.000000 38.000000 1.000000 0.000000 31.000000 max 891.000000 1.000000 3.000000 400.000000 8.000000 6.000000 146000.520800 很明显有些奇怪的东西混了进来 891名乘客, 计数是892 年龄有个最大值是400corruptedData[corruptedData.duplicated()] PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Ethnicity Cabin Embarked 42 42 0 2 Turpin, Mrs. William John Robert (Dorothy Ann ... female 27.0 1 0 11668 21.0 white NaN S 即位于PassengerId == 42的列是重复列.corruptedData.drop_duplicates(inplace=True)corruptedData[corruptedData.duplicated()] PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Ethnicity Cabin Embarked 重复的现在被去掉了.接下来处理年龄.corruptedData[corruptedData[&#39;Age&#39;] &amp;gt; 100] PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Ethnicity Cabin Embarked 10 11 1 3 Sandstrom, Miss. Marguerite Rut female 400.0 1 1 PP 9549 16.7 white G6 S PassengerId == 11出了一个400岁的.拿掉, 平均数填充.corruptedData.loc[corruptedData.PassengerId == 11] = corruptedData[&#39;Age&#39;].meancorruptedData.describe() PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Ethnicity Cabin Embarked count 891 891 891 891 891 714.0 891 891 891 891.00 891 204 889 unique 891 3 4 891 3 89.0 8 8 682 250.00 3 148 4 top 366 0 3 Gheorgheff, Mr. Stanio male 24.0 0 0 347082 8.05 white B96 B98 S freq 1 549 490 1 577 30.0 608 678 7 43.00 888 4 643 异常值处理完毕trainData = pd.read_csv(&#39;./titanic/train.csv&#39;)print(trainData.shape)trainData.head(5)(891, 12) PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S 完成依赖引入与数据读入. 如无特殊说明, 图例中绿色代表存活Survived, 红色代表不幸罹难Perished.trainData.describe() PassengerId Survived Pclass Age SibSp Parch Fare count 891.000000 891.000000 891.000000 714.000000 891.000000 891.000000 891.000000 mean 446.000000 0.383838 2.308642 29.699118 0.523008 0.381594 32.204208 std 257.353842 0.486592 0.836071 14.526497 1.102743 0.806057 49.693429 min 1.000000 0.000000 1.000000 0.420000 0.000000 0.000000 0.000000 25% 223.500000 0.000000 2.000000 20.125000 0.000000 0.000000 7.910400 50% 446.000000 0.000000 3.000000 28.000000 0.000000 0.000000 14.454200 75% 668.500000 1.000000 3.000000 38.000000 1.000000 0.000000 31.000000 max 891.000000 1.000000 3.000000 80.000000 8.000000 6.000000 512.329200 注意到PassengerID.count == 891, 而Age.count == 714, 即年龄缺失177个数据.进行中位数/随机森林预测数据补充.# Median Data# trainData[&#39;AgeM&#39;] = trainData[&#39;Age&#39;].fillna(trainData[&#39;Age&#39;].median)# Random Forest Approachage_df = trainData[[&#39;Age&#39;, &#39;Fare&#39;, &#39;Parch&#39;, &#39;SibSp&#39;, &#39;Pclass&#39;]]age_df_notnull = age_df.loc[(trainData[&#39;Age&#39;].notnull())]age_df_isnull = age_df.loc[(trainData[&#39;Age&#39;].isnull())]X = age_df_notnull.values[:,1:]Y = age_df_notnull.values[:,0]RFR = RandomForestRegressor(n_estimators=1000, n_jobs=-1)RFR.fit(X,Y)predictAges = RFR.predict(age_df_isnull.values[:,1:])trainData.loc[trainData[&#39;Age&#39;].isnull(), [&#39;Age&#39;]]= predictAgestrainData[&#39;Age&#39;].count() # 为891即补充完整891关注性别 基于生存人数(计数)的性别分布# 加入新列: Perished -&amp;gt; 逝世(Boolean)trainData[&#39;Perished&#39;] = 1 - trainData[&#39;Survived&#39;]trainData.groupby(&#39;Sex&#39;).agg(&#39;sum&#39;)[[&#39;Survived&#39;, &#39;Perished&#39;]] Survived Perished Sex female 233 81 male 109 468 基于生存人数(按比例)的性别分布trainData.groupby(&#39;Sex&#39;).agg(&#39;mean&#39;)[[&#39;Survived&#39;, &#39;Perished&#39;]] Survived Perished Sex female 0.742038 0.257962 male 0.188908 0.811092 # 基于性别的死亡计数trainData.groupby(&#39;Sex&#39;).agg(&#39;sum&#39;)[[&#39;Survived&#39;, &#39;Perished&#39;]] \\ .plot(kind=&#39;bar&#39;, stacked=True, color=[&#39;g&#39;, &#39;r&#39;], title=&#39;Survival Count Based on Sex&#39;, figsize=(16, 12))# 基于性别的死亡率计算trainData.groupby(&#39;Sex&#39;).agg(&#39;mean&#39;)[[&#39;Survived&#39;, &#39;Perished&#39;]] \\ .plot(kind=&#39;bar&#39;, stacked=True, color=[&#39;g&#39;, &#39;r&#39;], title=&#39;Survival Rate/Percentage Based on Sex&#39;, figsize=(16, 12))&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x206291984f0&amp;gt;不难看出, 在数据集中, Age == Female即女性的死亡率较低. 因此加入年龄作为参考因素, 绘制violin graph.fig = plt.figure(figsize=(24, 12))# 基于性别分类的存活率与死亡率的年龄分布小提琴图sns.violinplot(x=&#39;Sex&#39;, y=&#39;Age&#39;, hue=&#39;Survived&#39;, data=trainData, split=True, palette={0: &quot;r&quot;, 1: &quot;g&quot;}, title=&#39;Violin Plot on Survival Rate and Death Rate Based on Sex&#39;)&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x2062a2f25b0&amp;gt;得到以下特征 青少年男性存活比例较高, 而中年(Age~=30)男性死亡率高 女性各年龄段存活比例相对平均关注客舱等级(Pclass)trainData.groupby(&#39;Pclass&#39;).agg(&#39;sum&#39;)[[&#39;Survived&#39;, &#39;Perished&#39;]] Survived Perished Pclass 1 136 80 2 87 97 3 119 372 trainData.groupby(&#39;Pclass&#39;).agg(&#39;mean&#39;)[[&#39;Survived&#39;, &#39;Perished&#39;]] Survived Perished Pclass 1 0.629630 0.370370 2 0.472826 0.527174 3 0.242363 0.757637 # 基于客舱等级的死亡计数trainData.groupby(&#39;Pclass&#39;).agg(&#39;sum&#39;)[[&#39;Survived&#39;, &#39;Perished&#39;]]\\ .plot(kind=&#39;bar&#39;, stacked=True, color=[&#39;g&#39;, &#39;r&#39;], title=&#39;Survival Count Based on Pclass&#39;, figsize=(16, 12))&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x2062929c6a0&amp;gt; 客舱等级为1的死亡率最低, 仅约37% 客舱等级为3的死亡率最高, 约为75%此时加入船票费用(Fare)验证客舱等级1是否为高价或低价舱位# 每个客舱等级对应的费用trainData.groupby(&#39;Pclass&#39;).mean()[&#39;Fare&#39;] \\ .plot(kind=&#39;bar&#39;, color=&#39;y&#39;, figsize=(16, 12), title=&#39;Fare for each Pclass&#39;)&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x206292a95b0&amp;gt;验证上述猜想, 1号Pclass等级的客舱售价最高, 约80+美元, 而2, 3等级的客舱售价较低结合船票费用与年龄将死亡率与分布可视化plt.figure(figsize=(24, 12))plt.xlabel(&#39;Age&#39;)plt.ylabel(&#39;Ticket Fare&#39;)plt.scatter(trainData[trainData[&#39;Survived&#39;] == 1][&#39;Age&#39;], trainData[trainData[&#39;Survived&#39;] == 1][&#39;Fare&#39;], c=&#39;green&#39;, s=trainData[trainData[&#39;Survived&#39;] == 1][&#39;Fare&#39;])plt.scatter(trainData[trainData[&#39;Survived&#39;] == 0][&#39;Age&#39;], trainData[trainData[&#39;Survived&#39;] == 0][&#39;Fare&#39;], c=&#39;red&#39;, s=trainData[trainData[&#39;Survived&#39;] == 0][&#39;Fare&#39;])&amp;lt;matplotlib.collections.PathCollection at 0x20629ecce20&amp;gt;上述图的散点大小代表船票费用(Fare), x轴代表年龄(Age), y轴亦代表船票费用.作以下说明 称位于上图顶端的, 30&amp;lt;=Age(x axis)&amp;lt;=40, 绿色的散点为聚类点1 称位于上图底端的, 20&amp;lt;=Age(x axis)&amp;lt;=40, 红色的散点为聚类点2 称位于上图中心的, 10&amp;lt;=Age(x axis)&amp;lt;=40, 绿色的散点的为聚类点3 称位于上图左下端, 0&amp;lt;=Age(x axis)&amp;lt;=10, 绿色的散点为聚类点4聚类点1的出现, 表明票价最高的存活率亦最高.聚类点2的出现, 表面票价最低的中年乘客存活率亦最低, 红点极其密集.聚类点3的出现, 表面票价适中部分的中年乘客存活率相当可观.聚类点4的出现, 是最有趣的, 他们属于拥有较低求生技能的一批乘客, 主要为婴幼儿与儿童, 但是存活率亦高.可以判断婴幼儿与儿童相较于其他乘客, 获得更好的求生/救助资源. 该结论反射的观点也的确是明显受社会认可的(妇女儿童优先).关注年龄trainData[&quot;AgeInt&quot;] = trainData[&quot;Age&quot;].astype(int)# 精确到每个年龄的成员成活率avgAge = trainData[[&quot;AgeInt&quot;, &quot;Survived&quot;]].groupby([&#39;AgeInt&#39;], as_index=False).mean()sns.barplot(x=&#39;AgeInt&#39;, y=&#39;Survived&#39;, data=avgAge)&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x20629efc2b0&amp;gt;separationPoint = [0, 6, 18, 40, 60, 100]trainData[&#39;AgeBatch&#39;] = pd.cut(trainData[&#39;AgeInt&#39;], separationPoint)batches = trainData.groupby(&#39;AgeBatch&#39;)[&#39;Survived&#39;].mean()# 按年龄段的存活率batchesAgeBatch(0, 6] 0.650000(6, 18] 0.366972(18, 40] 0.362522(40, 60] 0.404255(60, 100] 0.217391Name: Survived, dtype: float64batches.plot(kind=&#39;bar&#39;, color=&#39;g&#39;, figsize=(16, 12), title=&#39;Survival Rate on Age Batches&#39;)&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x2062a0f82e0&amp;gt;survivedtmp = trainData[trainData[&#39;Survived&#39;]==1][&#39;AgeBatch&#39;].value_counts()perishedtmp = trainData[trainData[&#39;Survived&#39;]==0][&#39;AgeBatch&#39;].value_counts()dftmp = pd.DataFrame([survivedtmp, perishedtmp])dftmp.index = [&#39;Survived&#39;,&#39;Perished&#39;]dftmp.plot(kind=&#39;bar&#39;, stacked=True, figsize=(16, 12))&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x2062a144070&amp;gt; 上一柱图颜色仅为区分年龄段上述年龄-存活率分布图更是验证了上面的说法, （0, 6]的年龄段可以获得65%的存活率.婴幼儿/儿童对应的某些年龄段, 获得了甚至接近100%的存活率.老人对应的年龄段, 考虑到他们的身体条件, 该存活率表现也足以表明社会救助的确有偏向性.考虑有无子女上船SibSp# 根据有无子女上船, 划分数据# OB-&amp;gt; On board, NOB-&amp;gt; NOT on boardsiblOB = trainData[trainData[&#39;SibSp&#39;] != 0]siblNOB = trainData[trainData[&#39;SibSp&#39;] == 0]plt.figure(figsize=(24, 12))plt.subplot(121)siblOB[&#39;Survived&#39;].value_counts().\\ plot(kind=&#39;pie&#39;, labels=[&#39;Perished&#39;, &#39;Survived&#39;], autopct=&#39;%.3f%%&#39;, colors=[&#39;r&#39;, &#39;g&#39;])plt.xlabel(&#39;Sibling onboard&#39;)plt.ylabel(&#39;Survival Rate&#39;)plt.subplot(122)siblNOB[&#39;Survived&#39;].value_counts().\\ plot(kind=&#39;pie&#39;, labels=[&#39;Perished&#39;, &#39;Survived&#39;], autopct=&#39;%.3f%%&#39;, colors=[&#39;r&#39;, &#39;g&#39;])plt.xlabel(&#39;Sibling NOT onboard&#39;)plt.ylabel(&#39;Survival Rate&#39;)Text(0, 0.5, &#39;Survival Rate&#39;)考虑有无父母上船Parch# 根据有无父母上船, 划分数据# OB-&amp;gt; On board, NOB-&amp;gt; NOT on boardparentOB = trainData[trainData[&#39;Parch&#39;] != 0]parentNOB = trainData[trainData[&#39;Parch&#39;] == 0]plt.figure(figsize=(24, 12))# plt.title(&#39;Survival Rate Based on Parents Onboard/Not Onboard&#39;)plt.subplot(121)siblOB[&#39;Survived&#39;].value_counts()\\ .plot(kind=&#39;pie&#39;, labels=[&#39;Perished&#39;, &#39;Survived&#39;], autopct=&#39;%.3f%%&#39;, colors=[&#39;r&#39;, &#39;g&#39;])plt.xlabel(&#39;Parent(s) onboard&#39;)plt.ylabel(&#39;Survival Rate&#39;)plt.subplot(122)siblNOB[&#39;Survived&#39;].value_counts()\\ .plot(kind=&#39;pie&#39;, labels=[&#39;Perished&#39;, &#39;Survived&#39;], autopct=&#39;%.3f%%&#39;, colors=[&#39;r&#39;, &#39;g&#39;])plt.xlabel(&#39;Parent NOT onboard&#39;)plt.ylabel(&#39;Survival Rate&#39;)Text(0, 0.5, &#39;Survival Rate&#39;)sns.pairplot(trainData, hue=&#39;Sex&#39;)&amp;lt;seaborn.axisgrid.PairGrid at 0x2062a2bd1c0&amp;gt;明显可以看出: 有父母或子女上船的乘客, 存活率都较比较组(父母或儿女未在船上)高.热力图将trainData中数据复制一份至heatMapData, 并去除相关系数较低的和上面新增的无用的字段, 如PassengerId类, 并将需要列化的数据进行ONE-HOT或BINARY编码.对某些数据做Scaling, 以增大其敏感度.并且将子女数量SibSp, 与父母数量Parch归为一个字段F(amily)M(embers)Count-&amp;gt;”家庭成员数” 家庭成员数 = 子女数+父母数+自己 FamilyMembersCount = SibSp + Parch + 1 heatMapData = trainData.copy(deep=True)heatMapData[&#39;FMCount&#39;] = heatMapData[&#39;Parch&#39;] + heatMapData[&#39;SibSp&#39;] + 1heatMapData.drop([&#39;Name&#39;,&#39;Ticket&#39;,&#39;Cabin&#39;,&#39;PassengerId&#39;,&#39;AgeBatch&#39;, &#39;AgeInt&#39;, &#39;Perished&#39;, &#39;SibSp&#39;, &#39;Parch&#39;], 1, inplace =True)heatMapData.Sex.replace((&#39;male&#39;,&#39;female&#39;), (0,1), inplace = True)heatMapData.Embarked.replace((&#39;S&#39;,&#39;C&#39;,&#39;Q&#39;), (1,2,3), inplace = True)# 有两行上船地点数据丢失, 用1Replace, 影响不大heatMapData.Embarked.fillna(1, inplace=True)heatMapData.head() Survived Pclass Sex Age Fare Embarked FMCount 0 0 3 0 22.0 7.2500 1.0 2 1 1 1 1 38.0 71.2833 2.0 2 2 1 3 1 26.0 7.9250 1.0 1 3 1 1 1 35.0 53.1000 1.0 2 4 0 3 0 35.0 8.0500 1.0 1 plt.figure(figsize=(16, 16))sns.heatmap(heatMapData.astype(float).corr(),linewidths=.4, square=True, linecolor=&#39;r&#39;, annot=True, cmap=&quot;RdPu&quot;)&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x206384bcaf0&amp;gt;尝试进行训练拟合xTrain = heatMapData.drop(&#39;Survived&#39;, axis=1)yTrain = heatMapData[&#39;Survived&#39;]testData = pd.read_csv(&#39;./titanic/test.csv&#39;)xTrain.info()testData.info()testData.head(5)&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;RangeIndex: 891 entries, 0 to 890Data columns (total 6 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Pclass 891 non-null int64 1 Sex 891 non-null int64 2 Age 891 non-null float64 3 Fare 891 non-null float64 4 Embarked 891 non-null float64 5 FMCount 891 non-null int64 dtypes: float64(3), int64(3)memory usage: 41.9 KB&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;RangeIndex: 418 entries, 0 to 417Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 PassengerId 418 non-null int64 1 Pclass 418 non-null int64 2 Name 418 non-null object 3 Sex 418 non-null object 4 Age 332 non-null float64 5 SibSp 418 non-null int64 6 Parch 418 non-null int64 7 Ticket 418 non-null object 8 Fare 417 non-null float64 9 Cabin 91 non-null object 10 Embarked 418 non-null object dtypes: float64(2), int64(4), object(5)memory usage: 36.0+ KB PassengerId Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 892 3 Kelly, Mr. James male 34.5 0 0 330911 7.8292 NaN Q 1 893 3 Wilkes, Mrs. James (Ellen Needs) female 47.0 1 0 363272 7.0000 NaN S 2 894 2 Myles, Mr. Thomas Francis male 62.0 0 0 240276 9.6875 NaN Q 3 895 3 Wirz, Mr. Albert male 27.0 0 0 315154 8.6625 NaN S 4 896 3 Hirvonen, Mrs. Alexander (Helga E Lindqvist) female 22.0 1 1 3101298 12.2875 NaN S 可以观察到, 测试数据并不是训练数据的子集, 测试数据来源有别于训练数据中的891位乘客, 而是另外418位乘客因为训练数据与测试数据有明显的字段差异(因在上文中, 对年龄的空缺值做了随机森林回归, 以及去除了无用字段).为保证训练能正常进行, xTrain要与testData即-&amp;gt;xTest进行同样的处理# 重复上文处理testData.Fare.fillna(testData[&#39;Fare&#39;].mean(), inplace=True)age_df = testData[[&#39;Age&#39;, &#39;Fare&#39;, &#39;Parch&#39;, &#39;SibSp&#39;, &#39;Pclass&#39;]]age_df_notnull = age_df.loc[(testData[&#39;Age&#39;].notnull())]age_df_isnull = age_df.loc[(testData[&#39;Age&#39;].isnull())]X = age_df_notnull.values[:,1:]Y = age_df_notnull.values[:,0]RFR = RandomForestRegressor(n_estimators=1000, n_jobs=-1)RFR.fit(X,Y)predictAges = RFR.predict(age_df_isnull.values[:,1:])testData.loc[testData[&#39;Age&#39;].isnull(), [&#39;Age&#39;]]= predictAgestestData[&#39;FMCount&#39;] = testData[&#39;Parch&#39;] + testData[&#39;SibSp&#39;] + 1testData.drop([&#39;Name&#39;,&#39;Ticket&#39;,&#39;Cabin&#39;,&#39;PassengerId&#39;, &#39;SibSp&#39;, &#39;Parch&#39;], 1, inplace=True)testData.Sex.replace((&#39;male&#39;,&#39;female&#39;), (0,1), inplace = True)testData.Embarked.replace((&#39;S&#39;,&#39;C&#39;,&#39;Q&#39;), (1,2,3), inplace = True)testData.Embarked.fillna(1, inplace=True)xTest = testData.copy()xTrain.info()xTest.info()&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;RangeIndex: 891 entries, 0 to 890Data columns (total 6 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Pclass 891 non-null int64 1 Sex 891 non-null int64 2 Age 891 non-null float64 3 Fare 891 non-null float64 4 Embarked 891 non-null float64 5 FMCount 891 non-null int64 dtypes: float64(3), int64(3)memory usage: 41.9 KB&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;RangeIndex: 418 entries, 0 to 417Data columns (total 6 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Pclass 418 non-null int64 1 Sex 418 non-null int64 2 Age 418 non-null float64 3 Fare 418 non-null float64 4 Embarked 418 non-null int64 5 FMCount 418 non-null int64 dtypes: float64(2), int64(4)memory usage: 19.7 KB可以看到训练数据与测试数据字段已经一致, 并且无空值.引入RandomForestClassifier进行数据拟合. 即根据前891名乘客的存活情况来预测余下418位乘客的存活情况# 训练数据头print(&#39;Training Data Head 5&#39;)xTrain.head(5)Training Data Head 5 Pclass Sex Age Fare Embarked FMCount 0 3 0 22.0 7.2500 1.0 2 1 1 1 38.0 71.2833 2.0 2 2 3 1 26.0 7.9250 1.0 1 3 1 1 35.0 53.1000 1.0 2 4 3 0 35.0 8.0500 1.0 1 # 测试数据头print(&#39;Testing Data Head 5&#39;)xTest.head(5)Testing Data Head 5 Pclass Sex Age Fare Embarked FMCount 0 3 0 34.5 7.8292 3 1 1 3 1 47.0 7.0000 1 2 2 2 0 62.0 9.6875 3 1 3 3 0 27.0 8.6625 1 1 4 3 1 22.0 12.2875 1 3 Linear Support Vector Classification 支持向量机分类SVC = LinearSVC()SVC.fit(xTrain, yTrain)yPredict = SVC.predict(xTest)predPercentage = SVC.score(xTrain, yTrain)print(&#39;Linear SVC Score&#39;)print(round(predPercentage*100, 4))Linear SVC Score68.9113c:\\dev\\env\\py38venv\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. warnings.warn(&quot;Liblinear failed to converge, increase &quot;Random Forest 随机森林预测randomForest = RandomForestClassifier(n_estimators=300)randomForest.fit(xTrain, yTrain)yPredict = randomForest.predict(xTest)predPercentage = randomForest.score(xTrain, yTrain)print(&#39;Random Forest Score&#39;)print(round(predPercentage*100, 4))Random Forest Score98.2043Decision Tree 决策树预测decisionTree = DecisionTreeClassifier()decisionTree.fit(xTrain, yTrain) yPredict = decisionTree.predict(xTest) predPercentage = decisionTree.score(xTrain, yTrain)print(&#39;Decision Tree Score&#39;)print(round(predPercentage*100, 4))Decision Tree Score98.2043" } ]
